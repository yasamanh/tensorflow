{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "matplotlib.rcParams['savefig.dpi'] = 144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import expectexception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = None\n",
    "\n",
    "def reset_tf():\n",
    "    global sess\n",
    "    if sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()\n",
    "    sess = tf.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- requirement: images/sq-circ.svg -->\n",
    "<!-- requirement: images/oct-circ.svg -->\n",
    "<!-- requirement: images/newtons.svg -->\n",
    "\n",
    "# Iterative Algorithms\n",
    "\n",
    "While it is always nice to be able to write down a single expression to do a calculation, very few of the things we wish to calculate can be calculated exactly.  Instead, we must use approximative schemes. Most work in an iterative manner: Starting with a simple approximation of the order, a calculation is made to improve the approximation.  This calculation can then be iterated to continually improve the approximation.\n",
    "\n",
    "This is what TensorFlow is really built for.  All of the work of setting up a computation graph first is not really worth it if the calculation needs to run only once.  But when the same calculation is repeated over and over, that extra initial work pays off again and again.\n",
    "\n",
    "Before we look at how to implement an iterative algorithm in TensorFlow, let's remind ourselves of one of the oldest such algorithms out there:\n",
    "\n",
    "## Archimedes' algorithm in Python\n",
    "\n",
    "By definition a circle with radius $1$ has a circumference of $2\\pi$.  Archimedes realized that you could get an estimate for the circumference by taking the perimeter of polygon (which he could calculate) that either circumscribed the unit circle or was inscribed within it.  Here are circumscribed and inscribed squares:\n",
    "\n",
    "![squares](images/sq-circ.svg)\n",
    "\n",
    "We'll let $P_n$ be the perimeter of the circumscribed $n$-gon and $p_n$ be the perimeter of the inscribed $n$-gon.  With just a little bit of geometry, you can work out that\n",
    "\n",
    "$$ P_4 = 8 $$\n",
    "$$ p_4 = 4\\sqrt2 \\approx 5.66 $$\n",
    "\n",
    "The actual value of $2\\pi \\approx 6.28$ is between them; in fact $(P_4 + p_4)/2 \\approx 6.82$ is an even better approximation.\n",
    "\n",
    "As we increase the number of sides, the polygons get closer to the circles, and the inscribing and circumscribing polygons will continue to bracket the circumference of the circle.  You can easily see this visually with octagons:\n",
    "\n",
    "![octagons](images/oct-circ.svg)\n",
    "\n",
    "Working out the circumference of each $n$-gon on its own would be quite tedious.  (Archemedes made it up to $n=96$!)  Luckily for us, there is a relatively simple recursive formula that gives us the perimeters of the $2n$-gons in terms of the perimeters of the $n$-gons:\n",
    "\n",
    "$$ P_{2n} = \\frac{2 p_n P_n}{p_n + P_n} $$\n",
    "$$ p_{2n} = \\sqrt{p_n P_{2n}} $$\n",
    "\n",
    "Thus, starting with $n = 4$, we can easily calculate $n = 8$, $16$, $32$, ....\n",
    "\n",
    "We'll illustrate this first with basic Python.  We start with the perimeters for the squares:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_insc = 4 * 2**0.5\n",
    "p_circ = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we'll double $n$ ten times, reporting the average of the two perimeters each time.  Note that $P_{2n}$ depends on $P_n$ and $p_n$, while $p_{2n}$ depends on $p_n$ and $P_{2n}$.  Therefore, we can update the circumscribed circumference before the the inscribed circumference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in xrange(10):\n",
    "    p_circ = 2 * p_insc * p_circ / (p_insc + p_circ)\n",
    "    p_insc = np.sqrt(p_insc * p_circ)\n",
    "    \n",
    "    print (p_circ + p_insc) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the end, we are dealing with a $4096$-gon.  Look how close our estimate is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(p_circ + p_insc) / 2 - 2*np.pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Archemedes' algorithm in TensorFlow\n",
    "\n",
    "These calculations are simple enough to do in TensorFlow.  They involve only simple mathematical operations, so setting up the graph should be no problem.  There is one important step that we haven't considered: updating the values each step.\n",
    "\n",
    "We could take the approach of using placeholders, and updating them with the new value each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_tf()\n",
    "\n",
    "p_insc = tf.placeholder(np.float64, name=\"Inscribed_Permimeter\")\n",
    "p_circ = tf.placeholder(np.float64, name=\"Circumscribed_Perimeter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we also define the updated values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "update_circ = 2 * p_circ * p_insc / (p_circ + p_insc)\n",
    "update_insc = tf.sqrt(p_insc * p_circ)\n",
    "estimate_2pi = (p_circ + p_insc) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run these iteratively, storing the updated values, so we can feed in the updated values at the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p_insc_val = 4 * np.sqrt(2)\n",
    "p_circ_val = 8\n",
    "for i in xrange(10):\n",
    "    p_circ_val = sess.run(update_circ,\n",
    "                          feed_dict={p_circ: p_circ_val,\n",
    "                                     p_insc: p_insc_val})\n",
    "    p_insc_val = sess.run(update_insc,\n",
    "                          feed_dict={p_circ: p_circ_val,\n",
    "                                     p_insc: p_insc_val})\n",
    "    print sess.run(estimate_2pi, feed_dict={p_circ: p_circ_val,\n",
    "                                            p_insc: p_insc_val})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works, but it is obviously inconvenient.  It gives us two objects for each value, and we need to make sure we're always feeding in the correct value to the correct placeholder.  This is clearly inviting errors.\n",
    "\n",
    "Fortunately, there's a better way.\n",
    "\n",
    "## Variables\n",
    "\n",
    "In addition to constants and placeholders, TensorFlow provides **variables**.  These tensors hold values much like others, but these values can be updated during a computation.  We will use variables again and again to hold parameters of models that need to be updated during training.\n",
    "\n",
    "Variables are created with an initial value.  Like other tensors, they can be given a name and a dtype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "var = tf.Variable(3, name=\"my_var\", dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, if you try to run a variable, you'll run into a problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%expect_exception tf.errors.FailedPreconditionError\n",
    "\n",
    "sess.run(var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables must be initialized before they are run.  The simplest way to do that is with `global_variables_initializer()`.  This TensorFlow operation will initialize all variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "sess.run(var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many other ways to [initialize variables](https://www.tensorflow.org/programmers_guide/variables#initialization).  Values can be stored on disk and restored later.  You can also initialize only the subset of variables that concern you.  We will use the global initializer so much that we'll make a simple function to run it in our global session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reset_vars():\n",
    "    sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables don't have to be initialized with a known value. In some cases, we'll want to initialize values randomly.  Here we create a random tensor of shape `[2, 2]` with values drawn from a normal distribution with $\\sigma = 1$ and $\\mu=2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rand_var = tf.Variable(tf.random_normal([2, 2], stddev=1, mean=2))\n",
    "reset_vars()\n",
    "print sess.run(rand_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables can be updated with the *assign* operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "change_var = var.assign(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this is an operation.  Nothing changes until the assignment is actually run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print sess.run(var)\n",
    "print sess.run(change_var)\n",
    "print sess.run(var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More typically, the value of a variable will be updated based on current values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "double_var = var.assign(var * 2)\n",
    "print sess.run(double_var)\n",
    "print sess.run(double_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of each variable exists only within a given session.  If you start with another session, variables will have a separate value in that one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess2 = tf.Session()\n",
    "sess2.run(tf.global_variables_initializer())\n",
    "print \"New session:\", sess2.run(var)\n",
    "print \"Old session:\", sess.run(var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Archemedes' algorithm done idomatically\n",
    "\n",
    "Now we can re-run our estimation for $2\\pi$ using TensorFlow variables.  We'll start them off with the values for $n = 4$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p_insc = tf.Variable(4 * np.sqrt(2), name=\"Inscribed_Perimeter\", dtype=np.float64)\n",
    "p_circ = tf.Variable(8.0, name=\"Circumscribed_Perimeter\", dtype=np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The update operations will now assign the value for $2n$ back to the variable.  Our estimate is unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "update_circ = p_circ.assign(2 * p_circ * p_insc / (p_circ + p_insc))\n",
    "update_insc = p_insc.assign(tf.sqrt(p_insc * p_circ))\n",
    "estimate_2pi = (p_circ + p_insc) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, our iterative step just runs the two updates and then prints the current estimate.  Notice how much cleaner this is than the previous approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reset_vars()\n",
    "\n",
    "for i in xrange(10):\n",
    "    sess.run(update_circ)\n",
    "    sess.run(update_insc)\n",
    "    print sess.run(estimate_2pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A collection of tensors can be passed to the `run()` method.  What happens if we tell TensorFlow to run all three steps at once?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reset_vars()\n",
    "\n",
    "for i in xrange(10):\n",
    "    print sess.run([update_circ, update_insc, estimate_2pi])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you probably notice, we no longer get the right answer!  When used in this way, TensorFlow gives no guarantees about the order in which operations get run.  Because our algorithm requires the circumscribed perimeter to be updated before the inscribed, it is broken by this lack of guarantee."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Fibonacci numbers\n",
    "\n",
    "Build an iterative TensorFlow graph that calculates the $n^{th}$ Fibonacci number $F_n$.  Recall that $F_n = F_{n-1} + F_{n-2}$, with $F_1 = F_2 = 1$.  Use this to find $F_{80}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newton's Method of Root Finding\n",
    "\n",
    "Newton's method is a technique for finding the roots of a function.  That is, given a function $f(x)$, it will find a root $x^*$ such that $f(x^*) = 0$.\n",
    "\n",
    "The algorithm works iteratively.  Given a starting position $x_0$, it approximates the function nearby as a straight line with slope $f'(x_0)$:\n",
    "\n",
    "$$ f(x) \\approx f(x_0) + f'(x_0) (x - x_0) $$\n",
    "\n",
    "![newton's method](images/newtons.svg)\n",
    "\n",
    "We use this approximation to get a better estimate of the root:\n",
    "\n",
    "$$ x_{1} = x_0 - \\frac{f(x_0)}{f'(x_0)} $$\n",
    "\n",
    "This process is then iterated.  At each step, we use the existing estimate of the root to make the next estimate.\n",
    "\n",
    "$$ x_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)} $$\n",
    "\n",
    "As a simple example, let's taket $f(x) = x^2 - 2$, which has roots $x = \\pm\\sqrt2$.  We'll keep track of our current estimate of the root in a variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_tf()\n",
    "\n",
    "x = tf.Variable(0.05, name=\"root\", dtype=np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have purposefully started the estimate far from the root so that we can observe the approach.\n",
    "\n",
    "Defining $f$ is easy enough.  Similarly, if we remember our calculus, we know that $f'(x) = 2x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = x * x - 2\n",
    "fp = 2 * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we don't make `f` or `fp` into functions; they are tensors just like `x`.  Because TensorFlow is lazy in its evaluation, their values are only calculated when needed.  When that calculation is done, the current value of `x` is used, so they will always be up to date with `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reset_vars()\n",
    "\n",
    "print sess.run([x, f])\n",
    "sess.run(x.assign(2))\n",
    "print sess.run([x, f])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The iterative step updates the value of `x` from the current position, function value, and function derivative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iterate = x.assign(x - f / fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run this for a number of steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reset_vars()\n",
    "\n",
    "for i in xrange(10):\n",
    "    print sess.run(iterate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how well we did:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess.run(x) - np.sqrt(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've managed to find the correct value to within floating point accuracy, in only 10 steps.  Not bad!\n",
    "\n",
    "We were able to write this easily because the function was easy to differentiate.  What if we're faced with some hideous function?\n",
    "\n",
    "Luckily for us, TensorFlow can take derivates for us!  Recall that when we define something like `f = x * x + 2`, we are not calculating a value for `f`.  Instead, we are recording the operations necessary to calculate `f`.  As long as each operation knows how derivatives should be taken, it is easy enough to use the chain rule to calculate the full derivative.\n",
    "\n",
    "This functionality is provided by the `gradients()` function.  The gradient is a generalization of the derivative to handle tensors.  If $\\mathbf{x} = \\left(x_1, x_2, ...\\right)$ is a tensor,\n",
    "\n",
    "$$ \\nabla f = \\left( \\frac{\\partial f}{\\partial x_1},\n",
    "\\frac{\\partial f}{\\partial x_2}, ... \\right) $$\n",
    "\n",
    "This function takes two arguments: the tensor to be differentiated and the tensor to do the differentiation.  In this case, we want the derivate of `f` with respect to `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fp_auto = tf.gradients(f, x)[0]  # Gives an array of length 1 in this case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that it gives the same values as the derivative we calculated by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess.run(x.assign(2))\n",
    "sess.run([fp, fp_auto])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use it in our Newton's method calculation just like we did with the previous derivative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iterate_auto = x.assign(x - f / fp_auto)\n",
    "\n",
    "reset_vars()\n",
    "\n",
    "for i in xrange(10):\n",
    "    print sess.run(iterate_auto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Minimizing functions\n",
    "\n",
    "We can use a similar algorithm to find minima of functions.  At the minimum (or maximum) of a function, the derivate is zero.  Therefore, we can use Newton's method to find roots of the derivative.  That is, we can approximate\n",
    "\n",
    "$$ f'(x) \\approx f'(x_0) + f''(x_0) (x - x_0) $$\n",
    "\n",
    "which leads to an update rule\n",
    "\n",
    "$$ x_{n+1} = x_n - \\frac{f'(x_n)}{f''(x_n)} $$\n",
    "\n",
    "The [gamma function](https://en.wikipedia.org/wiki/Gamma_function) is an extension of the factorial.  It is defined as\n",
    "\n",
    "$$ \\Gamma(x) = \\int_0^\\infty y^{x-1} e^{-y} dy $$\n",
    "\n",
    "For integers, $\\Gamma(n) = (n-1)!$.  TensorFlow doesn't provide the gamma function directly, but it does have the `lgamma()` function, which gives the logarithm of the gamma function.  (Actually, it's the log of the absolute value, but $\\Gamma(x) > 0 \\ \\forall x > 0$, which is good enough for us.)  We also have the exponential function, so we can recreate it easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xx = np.linspace(0.1,4)\n",
    "plt.plot(xx, sess.run(tf.exp(tf.lgamma(xx))))\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel(r'$\\Gamma(x)$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, it has a minimum around 1.5.  Write a algorithm to find the position of the minimum.\n",
    "\n",
    "## Exercise: Minimizing functions of two variables\n",
    "\n",
    "This scheme gets more complicated when we consider functions of multiple arguments.  The equivalent expansion in terms of gradients becomes\n",
    "\n",
    "$$ \\nabla f(\\mathbf x) \\approx \\nabla f'(\\mathbf {x_0}) + (\\mathbf{x} - \\mathbf{x_0})\\cdot \\nabla\\ \\nabla f(\\mathbf{x_0}) $$\n",
    "\n",
    "Setting all components of this equation to zero leads to $n$ equations for the $n$ components of $\\mathbf x$.\n",
    "\n",
    "If $\\mathbf{x} = (x, y)$, we can do a bit of math to find the update rules\n",
    "\n",
    "$$ x_{n+1} = x_n + \\left. \\left( \\frac{\\partial^2 f}{\\partial x \\partial y}\\frac{\\partial f}{\\partial y} - \\frac{\\partial^2 f}{\\partial y^2}\\frac{\\partial f}{\\partial x} \\right) \\middle/ \\left( \\frac{\\partial^2 f}{\\partial x^2} \\frac{\\partial^2 f}{\\partial y^2} - \\left(\\frac{\\partial^2 f}{\\partial x \\partial y}\\right)^2 \\right)\\right. $$\n",
    "\n",
    "$$ y_{n+1} = y_n + \\left. \\left( \\frac{\\partial^2 f}{\\partial x \\partial y}\\frac{\\partial f}{\\partial x} - \\frac{\\partial^2 f}{\\partial x^2}\\frac{\\partial f}{\\partial y} \\right) \\middle/ \\left( \\frac{\\partial^2 f}{\\partial x^2} \\frac{\\partial^2 f}{\\partial y^2} - \\left(\\frac{\\partial^2 f}{\\partial x \\partial y}\\right)^2 \\right)\\right. $$\n",
    "\n",
    "Use these to find the minimum of the function\n",
    "\n",
    "$$ f(x, y) = x\\,y - 0.5x \\log(y) - y \\log(x) - \\log(2x + y) $$\n",
    "\n",
    "(Consider only $x > 0$, $y > 0$.)  How sensitive is the algorithm to the initial condition?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright &copy; 2017 The Data Incubator.  All rights reserved.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
