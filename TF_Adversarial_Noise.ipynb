{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "matplotlib.rcParams['savefig.dpi'] = 144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sess = None\n",
    "\n",
    "def reset_vars():\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "def reset_tf():\n",
    "    global sess\n",
    "    if sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()\n",
    "    sess = tf.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<!-- requirement: images/noise_0.png -->\n",
    "<!-- requirement: images/noisy_image_0.png -->\n",
    "\n",
    "# Adversarial Noise\n",
    "\n",
    "### A potential problem with convolutional neural networks\n",
    "\n",
    "One problem with convolutional neural networks is that noise can fool a CNN into mis-classifying images, even if the image is clearly identifiable by the human eye. Consider the two images below. The first represents noise that, if added to any of the MNIST images, will fool a CNN into classifying that image as a zero. The red and blue pixels represent positive and negative changes to the pixel intensities that will fool the CNN. This type of noise is called adversarial noise. The second image is a seven with the adversarial noise superimposed.   \n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td> <img src=\"files/images/noise_0.png\" style=\"width: 400px;\"/> </td>\n",
    "        <td> <img src=\"files/images/noisy_image_0.png\" style=\"width: 400px;\"/> </td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## How do you find adversarial noise?\n",
    "\n",
    "Before attempting to correct for the noise, we first have to find it. We'll get different noise patterns for each class (0-9), so we'll have to calculate adversarial noise 10 times. We will leave this as an exercise for you, but will describe the process: \n",
    "\n",
    "1. Change all of the test class labels to a single class (the \"adversarial target class\"). \n",
    "2. Create a new loss function that is the sum of the original loss and the L2-norm loss (least squares error). \n",
    "3. Define an optimizer to minimize this loss (like gradient descent) where you change the adversarial noise to increase the number of images classified as the adversarial target class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Putting it all together\n",
    "\n",
    "There are two different optimization procedures in our neural net. The first is the typical procedure where we try to classify the digits and modify the weights and biases of our neural network. The second is trying to find the adversarial noise and is described above. In this procedure, we do not modify the variables of the neural network. \n",
    "\n",
    "To make the network immune to noise we have to train it twice. One time to find the noise and a second time to train network to correctly classify noisy images. In the example below, we do this for target class 0. In theory, we would like to do this for all classes, but there are several things to keep in mind:\n",
    "\n",
    "1. The model's accuracy decreases if we try to make it immune to all classes (0-9). \n",
    "2. If we have many classes, making the network immune to all adversarial noise is impractical. \n",
    "3. As the model becomes immune to noise, it does not classify clean images as accurately. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's first start off by repeating some of the steps we did in the previous notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "reset_tf()\n",
    "\n",
    "# Load data\n",
    "data = input_data.read_data_sets('/tmp/data/', one_hot=True)\n",
    "\n",
    "# Get class (number) for test data\n",
    "data.test.cls = np.argmax(data.test.labels, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We'll then set our model parameters and placeholder variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Model Parameters\n",
    "img_size = 28\n",
    "img_size_flat = img_size * img_size\n",
    "img_shape = (img_size, img_size)\n",
    "\n",
    "n_classes = 10\n",
    "n_channels = 1\n",
    "\n",
    "batch_size = 50\n",
    "display_step = 100\n",
    "\n",
    "n_steps = 70 # 700 # Change this before running\n",
    "\n",
    "# Placeholder variables\n",
    "x = tf.placeholder(tf.float32, shape=[None, img_size_flat], name='x')\n",
    "x_image = tf.reshape(x, [-1, img_size, img_size, n_channels])\n",
    "y_true = tf.placeholder(tf.float32, shape=[None, n_classes], name='y_true')\n",
    "y_true_cls = tf.argmax(y_true, dimension=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We will first initialize the adversarial noise (`x_noise`) to zero and assign it to the collection `adversary_variables`. During training, we will update these values and add them to our images. In order to ensure that our noisy images remain recognizable to the human eye, we will also limit the magnitude of this noise. Here, we choose this limit to be 0.35. Finally, we want to ensure that the values of our pixels remain between 0 and 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Noise\n",
    "noise_limit = 0.35\n",
    "noise_l2_weight = 0.02\n",
    "ADVERSARY_VARIABLES = 'adversary_variables'\n",
    "collections = [tf.GraphKeys.GLOBAL_VARIABLES, ADVERSARY_VARIABLES]\n",
    "\n",
    "x_noise = tf.Variable(tf.zeros([img_size, img_size, n_channels]),\n",
    "                      name='x_noise', trainable=False,\n",
    "                      collections=collections)\n",
    "\n",
    "x_noise_clip = tf.assign(x_noise, tf.clip_by_value(x_noise, -noise_limit, noise_limit))\n",
    "x_noisy_image = x_image + x_noise\n",
    "x_noisy_image = tf.clip_by_value(x_noisy_image, 0.0, 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We will adopt the same CNN architecture for our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Layers\n",
    "# Create network of layers\n",
    "def conv_net(x, img_size, n_classes, stride, filt_size, out_sizes):\n",
    "    # Reshape input picture\n",
    "    x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
    "\n",
    "    # Convolutional layers\n",
    "    for out_size in out_sizes[:-1]:\n",
    "        x = tf.layers.conv2d(x, out_size, filt_size, padding='same', activation=tf.nn.relu,\n",
    "                             kernel_initializer=tf.truncated_normal_initializer(stddev=x.shape.as_list()[-1]**-0.5))\n",
    "        x = tf.layers.max_pooling2d(x, (2, 2), (2, 2))\n",
    "        \n",
    "    # Fully connected layer\n",
    "    x = tf.reshape(x, [-1, x.shape[1:].num_elements()])\n",
    "    x = tf.layers.dense(x, out_size, activation=tf.nn.relu)\n",
    "\n",
    "    # Output, class prediction\n",
    "    y = tf.layers.dense(x, n_classes, activation=None,\n",
    "                        kernel_initializer=tf.truncated_normal_initializer(stddev=x.shape.as_list()[-1]**-0.5))\n",
    "    return y\n",
    "\n",
    "stride = 1\n",
    "filt_size = [5, 5]\n",
    "out_sizes = [32, 64, 1024]\n",
    "\n",
    "# for MNIST images\n",
    "y_pred = conv_net(x_noisy_image, img_size, n_classes, stride, filt_size, out_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In addition to having a loss function to optimize our weights and biases, we also need to define a new loss function to find the adversarial noise. You will notice that the new loss function is actually a combination of the original loss function and L2-loss for the the adversarial noise. Why do we combine the two? We want to the smallest values of noise which will result in the best (mis)classification. We prioritize classification accuracy, so we will weight the L2-loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_pred, labels=y_true))\n",
    "\n",
    "adversary_variables = tf.get_collection(ADVERSARY_VARIABLES)\n",
    "l2_loss_noise = noise_l2_weight * tf.nn.l2_loss(x_noise)\n",
    "loss_adversary = loss + l2_loss_noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Since we are optimizing both the noise and the network parameters, we will need to define two different optimizers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "optimizer_adversary = tf.train.AdamOptimizer().minimize(loss_adversary, var_list=adversary_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Our definition of accuracy, however, remains the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "y_pred_cls = tf.argmax(y_pred, dimension=1)\n",
    "correct_prediction = tf.equal(y_pred_cls, y_true_cls)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We will then then initialize our variables and launch the graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Initialize\n",
    "reset_vars()\n",
    "\n",
    "# Initialize noise\n",
    "sess.run(tf.variables_initializer([x_noise]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The model needs to be optimized twice -- first to find the adversarial noise and second to make the network immune to that noise. Since the noise is being trained at the same time as the classifier, we'll combine the training code into a single function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def optimize(num_iterations, adversary_target_cls=None):\n",
    "    # Start-time used for printing time-usage below.\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        # Get a batch of training examples.\n",
    "        x_batch, y_true_batch = data.train.next_batch(batch_size)\n",
    "\n",
    "        # ---------------------- TRAIN -------------------------\n",
    "\n",
    "        # Optimize model\n",
    "        feed_dict_train = {x: x_batch, y_true: y_true_batch}\n",
    "        \n",
    "        if adversary_target_cls is None:\n",
    "            sess.run(optimizer, feed_dict=feed_dict_train)\n",
    "        else:\n",
    "            # Hack class labels\n",
    "            y_true_batch = np.zeros_like(y_true_batch)\n",
    "            y_true_batch[:, adversary_target_cls] = 1.0\n",
    "            feed_dict_train = {x: x_batch, y_true: y_true_batch}\n",
    "            \n",
    "            # Optimize adversarial noise\n",
    "            sess.run(optimizer_adversary, feed_dict=feed_dict_train)\n",
    "            sess.run(x_noise_clip)\n",
    "\n",
    "        # Print status every 100 iterations.\n",
    "        if (i % display_step == 0) or (i == num_iterations - 1):\n",
    "            \n",
    "            # Calculate the accuracy\n",
    "            acc = sess.run(accuracy, feed_dict=feed_dict_train)\n",
    "\n",
    "            # Message for network evaluation\n",
    "            msg = \"Optimization Iteration: {0:>6}, Training Accuracy: {1:>6.1%}\"\n",
    "            print(msg.format(i, acc))\n",
    "\n",
    "    # Ending time.\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Difference between start and end-times.\n",
    "    time_dif = end_time - start_time\n",
    "\n",
    "    # Print the time-usage.\n",
    "    print(\"Time usage: \" + str(timedelta(seconds=int(round(time_dif)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by training the neural net for a while.  It achieves rather good performance pretty quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize(n_steps, adversary_target_cls=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The noise term was initialized to zero.  (This is why it didn't disrupt the training above.)  We'll get slightly better performance if we start it off with some random values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "sess.run(x_noise.assign(np.random.uniform(-noise_limit/2, noise_limit/2, size=x_noise.shape)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This random noise doesn't particularly bother the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "prediction = tf.argmax(y_pred, 1)\n",
    "noise = np.squeeze(sess.run(x_noise))\n",
    "\n",
    "def predict(idx):\n",
    "    image = data.test.images[idx]\n",
    "    return sess.run(prediction, feed_dict={x: [image]})\n",
    "\n",
    "idx = 0\n",
    "actual = np.argmax(data.test.labels[idx])\n",
    "print (\"Predicted: %d, Actual: %d\" % (predict(idx), actual))\n",
    "plt.imshow(data.test.images[idx].reshape((img_size,img_size)) + noise,\n",
    "           cmap=plt.cm.gray_r, interpolation='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But by optimizing with `adversary_target_cls=0`, we are tuning the noise to force classification of the images as zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimize(n_steps, adversary_target_cls=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The noise displays hints of a zero, but it is mostly random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "noise = np.squeeze(sess.run(x_noise))\n",
    "plt.imshow(noise, interpolation='nearest', cmap='seismic',\n",
    "           vmin=-1.0, vmax=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When it is combined with an image, our classifier is fooled.  But when we look at the image, it's still clearly a seven."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "idx = 0\n",
    "actual = np.argmax(data.test.labels[idx])\n",
    "print (\"Predicted: %d, Actual: %d\" % (predict(idx), actual))\n",
    "plt.imshow(data.test.images[idx].reshape((img_size,img_size)) + noise,\n",
    "           cmap=plt.cm.gray_r, interpolation='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By optimizing without `adversary_target_cls`, we immunize the classifier against this noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimize(n_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the classifier works on the noisy image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "idx = 0\n",
    "actual = np.argmax(data.test.labels[idx])\n",
    "print (\"Predicted: %d, Actual: %d\" % (predict(idx), actual))\n",
    "plt.imshow(data.test.images[idx].reshape((img_size,img_size)) + noise,\n",
    "           cmap=plt.cm.gray_r, interpolation='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Let's take a look at the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output, Image, display, HTML\n",
    "\n",
    "def strip_consts(graph_def, max_const_size=32):\n",
    "    \"\"\"Strip large constant values from graph_def.\"\"\"\n",
    "    strip_def = tf.GraphDef()\n",
    "    for n0 in graph_def.node:\n",
    "        n = strip_def.node.add() \n",
    "        n.MergeFrom(n0)\n",
    "        if n.op == 'Const':\n",
    "            tensor = n.attr['value'].tensor\n",
    "            size = len(tensor.tensor_content)\n",
    "            if size > max_const_size:\n",
    "                tensor.tensor_content = \"<stripped %d bytes>\"%size\n",
    "    return strip_def\n",
    "\n",
    "def show_graph(graph_def, max_const_size=32):\n",
    "    \"\"\"Visualize TensorFlow graph.\"\"\"\n",
    "    if hasattr(graph_def, 'as_graph_def'):\n",
    "        graph_def = graph_def.as_graph_def()\n",
    "    strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n",
    "    code = \"\"\"\n",
    "        <script>\n",
    "          function load() {{\n",
    "            document.getElementById(\"{id}\").pbtxt = {data};\n",
    "          }}\n",
    "        </script>\n",
    "        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n",
    "        <div style=\"height:600px\">\n",
    "          <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n",
    "        </div>\n",
    "    \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n",
    "\n",
    "    iframe = \"\"\"\n",
    "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"{}\"></iframe>\n",
    "    \"\"\".format(code.replace('\"', '&quot;'))\n",
    "    display(HTML(iframe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "show_graph(tf.get_default_graph().as_graph_def())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Exercise: Extending immunity\n",
    "\n",
    "Make the network immune to all target classes. How does the accuracy of the model change?\n",
    "\n",
    "Hint: Run `make_immune` in a for loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "*Copyright &copy; 2017 The Data Incubator.  All rights reserved.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
