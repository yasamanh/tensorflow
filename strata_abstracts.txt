SAMOA is an open-source platform for mining big data streams that runs on several distributed stream processing engines (such as S4 and Storm), and includes streaming algorithms for the most common machine learning tasks such as classification and clustering. More info at http://samoa-project.net
How to extend your toolbox to solve more big data problems with less effort. AWS provides a set of big data services that are elastic, scalable and highly available out of the box. Learning best practices and tips of how to integrate them together and with your architecture adds to your abilities to provide fast and reliable big data solutions.
Live demo of building an intelligent big data application from a web console. The tools and APIs behind are built on top of Spark, Shark, Tachyon, Mesos, Aurora, Cassandra, iPython and include: ELT pipeline (ingestion and transformation), data warehouse explorer, export to NoSql and generated APIs, predictive model building, training and publishing, dashboard UI, monitoring and instrumentation
Apache Spark: Streaming case studies based on interviews with the dev teams, compared and contrasted with alternative open source projects, plus an open source example that demonstrates integration of Spark Streaming, Spark SQL, and Tachyon within a single app.
An exploration of Apache Spark, an in-memory analytics framework that applies functional programming paradigms to provide ad-hoc analysis for distributed databases like Cassandra.
A practical exploration of anomaly detection (from credit card fraud to incorrectly tagged movies) through harnessing the power of the 'inverted index' - the foundation of information retrieval systems. Use Hadoop, Elasticsearch and Spark  to gain insights into your big data and discover 'what stands out' at scale.
The data philanthropy movement is growing in Europe. DataKind is actively working to expand it's presence, and DataKind UK is now in it's second year, running successful events and projects.  This is the story of the last two events - highlighting how charities have joined the data revolution.
The Web in itself forms a versatile dataset capable of powering most diverse applications. In our joint talk, we will present Common Crawl, an immense collection of Web data made freely available to anyone. We will then introduce MIA and show how this Cloud-based analysis platform and marketplace for data and algorithms enables users to perform analytical tasks on datasets at Web scale.
This session will examine the challenges and opportunities associated with Big Data in a regulated environment, and the use of a new generation of data management technology to address them. Several case studies will be presented based on real-life production deployments.
How can we change architecture to design more for the people and less for the architects? We present crowd-based solutions with which urban planners can get valuable information about what kind of urban design is attractive to the people. This leads to GPS systems that show you the "most beautiful" path to your destination and to indicators about the beauty of a city.
Histograms and heatmaps are often used to summarize large data sets. We provide guidelines for using them effectively and efficiently. We illustrate this using the complete Dutch income tax data by looking at distributions in wealth and income. Analysis of this data set is complicated by the large amount of variables. We use clustering techniques to automatically find relevant patterns.
It is 2:30 in the night, you are barely awake and racing through the city center of Amsterdam while you hear a 120db horn screaming overhead. You are in a fire truck. Within 4 minutes you will be facing a potential life threatening situation. How do you deal with all the data that can make your work safer in a environment like that? Learn about how we started solving these problems in a agile way.
SAMOA is an open-source platform for mining big data streams that runs on several distributed stream processing engines (such as S4 and Storm), and includes streaming algorithms for the most common machine learning tasks such as classification and clustering. More info at http://samoa-project.net
How to extend your toolbox to solve more big data problems with less effort. AWS provides a set of big data services that are elastic, scalable and highly available out of the box. Learning best practices and tips of how to integrate them together and with your architecture adds to your abilities to provide fast and reliable big data solutions.
Live demo of building an intelligent big data application from a web console. The tools and APIs behind are built on top of Spark, Shark, Tachyon, Mesos, Aurora, Cassandra, iPython and include: ELT pipeline (ingestion and transformation), data warehouse explorer, export to NoSql and generated APIs, predictive model building, training and publishing, dashboard UI, monitoring and instrumentation
Apache Spark: Streaming case studies based on interviews with the dev teams, compared and contrasted with alternative open source projects, plus an open source example that demonstrates integration of Spark Streaming, Spark SQL, and Tachyon within a single app.
An exploration of Apache Spark, an in-memory analytics framework that applies functional programming paradigms to provide ad-hoc analysis for distributed databases like Cassandra.
A practical exploration of anomaly detection (from credit card fraud to incorrectly tagged movies) through harnessing the power of the 'inverted index' - the foundation of information retrieval systems. Use Hadoop, Elasticsearch and Spark  to gain insights into your big data and discover 'what stands out' at scale.
The data philanthropy movement is growing in Europe. DataKind is actively working to expand it's presence, and DataKind UK is now in it's second year, running successful events and projects.  This is the story of the last two events - highlighting how charities have joined the data revolution.
The Web in itself forms a versatile dataset capable of powering most diverse applications. In our joint talk, we will present Common Crawl, an immense collection of Web data made freely available to anyone. We will then introduce MIA and show how this Cloud-based analysis platform and marketplace for data and algorithms enables users to perform analytical tasks on datasets at Web scale.
This session will examine the challenges and opportunities associated with Big Data in a regulated environment, and the use of a new generation of data management technology to address them. Several case studies will be presented based on real-life production deployments.
How can we change architecture to design more for the people and less for the architects? We present crowd-based solutions with which urban planners can get valuable information about what kind of urban design is attractive to the people. This leads to GPS systems that show you the "most beautiful" path to your destination and to indicators about the beauty of a city.
Histograms and heatmaps are often used to summarize large data sets. We provide guidelines for using them effectively and efficiently. We illustrate this using the complete Dutch income tax data by looking at distributions in wealth and income. Analysis of this data set is complicated by the large amount of variables. We use clustering techniques to automatically find relevant patterns.
It is 2:30 in the night, you are barely awake and racing through the city center of Amsterdam while you hear a 120db horn screaming overhead. You are in a fire truck. Within 4 minutes you will be facing a potential life threatening situation. How do you deal with all the data that can make your work safer in a environment like that? Learn about how we started solving these problems in a agile way.
Spark Camp: An Introduction to Apache Spark with Hands-on Tutorials.
Spark Camp: An Introduction to Apache Spark with Hands-on Tutorials.
This tutorial will teach you how to streamline your code and your thinking when doing data science. Analysts often spend over 80% of their time preparing and exploring data sets before they begin more formal analysis work. In this tutorial, I will introduce a set of principles -- and R packages -- that make this work easier and faster.
Are you looking for a deeper understanding of how to integrate components in  the Apache Hadoop ecosystem to implement data management and processing  solutions? Then this tutorial is for you. We'll provide a clickstream analytics example  illustrating how to architect solutions with Apache Hadoop along with providing best  practices and recommendations for using Hadoop and related tools.
This tutorial will teach you how to streamline your code and your thinking when doing data science. Analysts often spend over 80% of their time preparing and exploring data sets before they begin more formal analysis work. In this tutorial, I will introduce a set of principles -- and R packages -- that make this work easier and faster.
Are you looking for a deeper understanding of how to integrate components in  the Apache Hadoop ecosystem to implement data management and processing  solutions? Then this tutorial is for you. We'll provide a clickstream analytics example  illustrating how to architect solutions with Apache Hadoop along with providing best  practices and recommendations for using Hadoop and related tools.
In this presentation Daniel will discuss a process that can be used to go from data to stories. He will talk about ways to define the audience, create hypotheses, sketch data, analyze and build a story around it. The presentation includes the connecting dots game, Hulk, comics, architecture and other stories.
In this talk we will present practical cases on innovating with data in retail banking, a conservative industry. From initial idea to embracing open as a fundamental culture change, the talk will walk the audience through insights, lessons learned and practical examples on how to change the way value is delivered to customers.
Emotions are messy and complicated. That meant we had to develop new data science and research methods to understand emotional engagement with out TV shows. But it also meant we had to be careful about how we brought that data to bear in a creative business like BBC Worldwide. Hear about how data science is making a big difference to how we build brands around the world.
Everyone knows that creating value from big data requires the right skills, but what does this mean in practice? We present findings of a research project where we measure the skills needs of data-driven companies in 6 sectors, quantify the impact of data talent on company performance, and identify good practices to find, create value from and retain data talent.
Datafication is a new term used to describe the process of turning an existing business into a "data business".This is affecting many industry and services sectors.For this,data monetization strategies must be in place.  New data sources(open data..) have a key role as well as the need to protect data privacy.
We will detail the development of a bi-directional event stream recommendation system in RuneScape, a massively multiplayer online game. By capturing a feature rich relationship between player and content we were able to train different 'flavours' of recommendation. Delivered in real-time these 'flavours' balance engagement, monetisation and enjoyment according to shifting business needs.
Apache Spark enables interactive analysis of big data by reducing query latency to the range of human interactions through caching. Additionally, Sparks unified programming model and diverse programming interfaces enable smooth integration with popular visualization tools, such as ggplot and matplotlib. We can use these to perform visual exploratory big data analysis with Spark.
Apache Spark is a popular new paradigm for computation on Hadoop. It's particularly effective for iterative algorithms relevant to data science like clustering, which can be used to detect anomalies in data. Curious? Get a taste of Spark MLlib, Scala and k-means clustering in this walkthrough of anomaly detection as applied to network intrusion, using the KDD Cup '99 data set.
The Data Science Toolbox is a new, open source virtual environment for data science. Its mission is to: (1) get data scientists started in a matter minutes, (2) enable teachers and authors to offer a custom virtual environment for their students and readers, and (3) encourage researchers to set up reproducible experiments. We'll discuss its importance, its technology, and its future.
Apache Spark lets users build unified data analytic pipelines that combine diverse processing types. In this talk, we will leverage the versatility of Spark to combine SQL, machine learning, and realtime streaming processing to build a complete data pipeline in a single, short program which we will build up throughout the session.
The need to categorize short text strings arises in many domains: online advertising, search engines, social networking, etc. In this session, we will share strategies for categorizing large volumes of queries and keywords in the advertising space, our successes with open document collections (Wikipedia, DBPedia, Freebase), and details on our solution using Hadoop and Solr.
In this presentation Daniel will discuss a process that can be used to go from data to stories. He will talk about ways to define the audience, create hypotheses, sketch data, analyze and build a story around it. The presentation includes the connecting dots game, Hulk, comics, architecture and other stories.
In this talk we will present practical cases on innovating with data in retail banking, a conservative industry. From initial idea to embracing open as a fundamental culture change, the talk will walk the audience through insights, lessons learned and practical examples on how to change the way value is delivered to customers.
Emotions are messy and complicated. That meant we had to develop new data science and research methods to understand emotional engagement with out TV shows. But it also meant we had to be careful about how we brought that data to bear in a creative business like BBC Worldwide. Hear about how data science is making a big difference to how we build brands around the world.
Everyone knows that creating value from big data requires the right skills, but what does this mean in practice? We present findings of a research project where we measure the skills needs of data-driven companies in 6 sectors, quantify the impact of data talent on company performance, and identify good practices to find, create value from and retain data talent.
Datafication is a new term used to describe the process of turning an existing business into a "data business".This is affecting many industry and services sectors.For this,data monetization strategies must be in place.  New data sources(open data..) have a key role as well as the need to protect data privacy.
We will detail the development of a bi-directional event stream recommendation system in RuneScape, a massively multiplayer online game. By capturing a feature rich relationship between player and content we were able to train different 'flavours' of recommendation. Delivered in real-time these 'flavours' balance engagement, monetisation and enjoyment according to shifting business needs.
Apache Spark enables interactive analysis of big data by reducing query latency to the range of human interactions through caching. Additionally, Sparks unified programming model and diverse programming interfaces enable smooth integration with popular visualization tools, such as ggplot and matplotlib. We can use these to perform visual exploratory big data analysis with Spark.
Apache Spark is a popular new paradigm for computation on Hadoop. It's particularly effective for iterative algorithms relevant to data science like clustering, which can be used to detect anomalies in data. Curious? Get a taste of Spark MLlib, Scala and k-means clustering in this walkthrough of anomaly detection as applied to network intrusion, using the KDD Cup '99 data set.
The Data Science Toolbox is a new, open source virtual environment for data science. Its mission is to: (1) get data scientists started in a matter minutes, (2) enable teachers and authors to offer a custom virtual environment for their students and readers, and (3) encourage researchers to set up reproducible experiments. We'll discuss its importance, its technology, and its future.
Apache Spark lets users build unified data analytic pipelines that combine diverse processing types. In this talk, we will leverage the versatility of Spark to combine SQL, machine learning, and realtime streaming processing to build a complete data pipeline in a single, short program which we will build up throughout the session.
The need to categorize short text strings arises in many domains: online advertising, search engines, social networking, etc. In this session, we will share strategies for categorizing large volumes of queries and keywords in the advertising space, our successes with open document collections (Wikipedia, DBPedia, Freebase), and details on our solution using Hadoop and Solr.
This talk will show how HBase use-cases vary significantly from write-once, read many workloads storing events, to updatable entity workloads that use it as random read and write backing store. A discussion of how these use-cases can be classified, along with example, concludes the session.
The advent of next-generation DNA sequencing technologies is revolutionizing life sciences research by routinely generating extremely large data sets. Big data tools developed to handle large-scale internet data (like Hadoop) will help scientists effectively manage this new scale of data, and also enable addressing a host of questions that were previously out of reach.
Improve Digital is an ad tech company with large data volumes. This talk will explore our learnings from enhancing our established batch infrastructure with streaming near-realtime capabilities. In addition to discussing the impact on our architecture we will also describe how the work changed our approach to data lifecycle management.
Relevance and Personalization is crucial to building personalized local commerce experience at Groupon. Talk covers overview of the real time analytics infrastructure built using  open source technologies such as Kafka- Storm - HBase- Redis which handles over 1 million data points per second in real time.  Talk covers various solution choices, different techniques and strategies and more.
Find out how to run real-time analytics over raw data without requiring a manual ETL process targeted at an RDBMS. This talk describes Impalas approach to on-the-fly data transformation and its support for nested data; examples demonstrate how this can be used to query raw data feeds in formats such as text, JSON and XML, at a performance level commonly associated with specialized engines.
This session presents details on Ciscos enterprise Hadoop architecture including roadmap details, centralized funding model that helped it get deployed quickly as well as its logical and physical views. Prominent use cases already in use at Cisco will also be covered.
Solocal, the French company behind PagesJaunes.fr, recently put Big Data and Hadoop into action to replace its traditional BI infrastructure. In this session, you will learn why and how that was done.
Childrens Healthcare of Atlanta in the US implemented Hadoop to capture and analyze vital sign sensor data in the ICU. Its goal is to understand the impact of stressful procedures, to reduce pain, and to improve outcomes in their most fragile patients. This session will highlight the challenges of pediatric healthcare data management and the strategies used to make this project a success.
The talk will provide insight into how to achieve coordinated technological change in a highly agile IT organization; an organisational function that supports one of the UKs most recognisable brands. Discover valuable lessons learned and begin to understand how your organization may want to take first steps in its engagement proving and implementing Big Data technology.
With traditional revenue sources maturing and new entrants at the gate, data can be a powerful differentiator. This session will present the challenges involved in deploying the right technologies and the change management culture at the foundations of new info-led propositions.
Graph mining of large highly dynamic graphs is a challenging algorithmic and programming task requiring custom algorithms. Additionally, existing graph mining architectures are designed for batch workloads.  The RT-Giraph open source project simplifies online graph mining by maintaining the programming and algorithmic simplicity of Apache Giraph, while supporting dynamic graphs.
The world of data is inherently diverse and "messy". Wouldn't it be nice if your programming language was aware of the external data sources that you are accessing? In this talk, we look at doing data science with F#, which provides unique way of integrating external data sources and libraries. You can access data, but also Matlab scripts or R packages, all from a single environment.
This talk will show how HBase use-cases vary significantly from write-once, read many workloads storing events, to updatable entity workloads that use it as random read and write backing store. A discussion of how these use-cases can be classified, along with example, concludes the session.
The advent of next-generation DNA sequencing technologies is revolutionizing life sciences research by routinely generating extremely large data sets. Big data tools developed to handle large-scale internet data (like Hadoop) will help scientists effectively manage this new scale of data, and also enable addressing a host of questions that were previously out of reach.
Improve Digital is an ad tech company with large data volumes. This talk will explore our learnings from enhancing our established batch infrastructure with streaming near-realtime capabilities. In addition to discussing the impact on our architecture we will also describe how the work changed our approach to data lifecycle management.
Relevance and Personalization is crucial to building personalized local commerce experience at Groupon. Talk covers overview of the real time analytics infrastructure built using  open source technologies such as Kafka- Storm - HBase- Redis which handles over 1 million data points per second in real time.  Talk covers various solution choices, different techniques and strategies and more.
Find out how to run real-time analytics over raw data without requiring a manual ETL process targeted at an RDBMS. This talk describes Impalas approach to on-the-fly data transformation and its support for nested data; examples demonstrate how this can be used to query raw data feeds in formats such as text, JSON and XML, at a performance level commonly associated with specialized engines.
This session presents details on Ciscos enterprise Hadoop architecture including roadmap details, centralized funding model that helped it get deployed quickly as well as its logical and physical views. Prominent use cases already in use at Cisco will also be covered.
Solocal, the French company behind PagesJaunes.fr, recently put Big Data and Hadoop into action to replace its traditional BI infrastructure. In this session, you will learn why and how that was done.
Childrens Healthcare of Atlanta in the US implemented Hadoop to capture and analyze vital sign sensor data in the ICU. Its goal is to understand the impact of stressful procedures, to reduce pain, and to improve outcomes in their most fragile patients. This session will highlight the challenges of pediatric healthcare data management and the strategies used to make this project a success.
The talk will provide insight into how to achieve coordinated technological change in a highly agile IT organization; an organisational function that supports one of the UKs most recognisable brands. Discover valuable lessons learned and begin to understand how your organization may want to take first steps in its engagement proving and implementing Big Data technology.
With traditional revenue sources maturing and new entrants at the gate, data can be a powerful differentiator. This session will present the challenges involved in deploying the right technologies and the change management culture at the foundations of new info-led propositions.
Graph mining of large highly dynamic graphs is a challenging algorithmic and programming task requiring custom algorithms. Additionally, existing graph mining architectures are designed for batch workloads.  The RT-Giraph open source project simplifies online graph mining by maintaining the programming and algorithmic simplicity of Apache Giraph, while supporting dynamic graphs.
The world of data is inherently diverse and "messy". Wouldn't it be nice if your programming language was aware of the external data sources that you are accessing? In this talk, we look at doing data science with F#, which provides unique way of integrating external data sources and libraries. You can access data, but also Matlab scripts or R packages, all from a single environment.
Linking data to create broader data sets can dramatically improve analysis results, but what if the data sets lack common identifiers? Similarly, duplicates in data is very common, and can seriously skew analysis results. This talk covers common techniques from record linkage research for solving this, as well as an open source tool implementing those techniques, and real-world examples.
Processing huge volume event streams in realtime poses quite some challenges. Based on our experience with social media data and realtime user interaction data, we discuss our experience with building such systems starting with a single computer. We have distilled this experience in a number of realtime data analysis patterns, which solve key aspects of such systems.
One of the most exciting areas in Big Data is the development of new data products; predictive applications used to drive product recommendations, predict machine failures, forecast airfare, social match-make, identify fraud, predict disease outbreaks, and repurpose pharmaceuticals. In this talk, Ill share the trends were seeing in predictive application development, show how to....
Many people assume that researching/designing a predictive modeling algorithm is the hard part of building a predictive modeling system over Big Data. We will focus on the far less romantic infrastructure needed to support a system, by reviewing the necessary components and the common pitfalls encountered when trying to automate both horizontally and vertically scalable systems.
Computing various quantities such as medians or the number of unique elements requires a lot of time or a lot of memory or both. It is, however, possible to get really close to the right answer with much less time and much less memory. Such algorithms can be simpler than you might expect. I will describe these and show how they can be applied to applications like anomaly detection.
A lot of decisions are made for us based on data  but are we at risk of crossing over into the uncanny valley of over-familiar personalisation? Designers need to focus on human elements, rather than allowing tech to lead the way. Jesus Gorriti will discuss SMART, a collaboration with the Harvard Medical School where the pediatric growth chart was reinvented using big data and design thinking.
Making meaning and value from data is not only a job for data scientists. Ethnographic researchers, subject matter experts, visual communication designers, and behavioral scientists all play key roles in the data journey. In this talk, we'll explore the data value chain, and share opportunities for how all of us -- whether data scientists or not -- can create and use data for insight and impact.
We have the unfortunate tendency to fit our problems to the technology at hand. We should be looking for ways to bend technology to our problems...our big problems. Kim will take a long look into the future of data covering the controversial and hopeful areas of privacy, open data, hacking, ETL relief, latent machines, M2M, and mass crowdsourcing.
Experiences from development of contextual applications, especially on data, design and privacy issues
The ggvis package makes it easy to create interactive data graphics with R, with a declarative syntax similar to that of ggplot2. Like ggplot2, ggvis uses concepts from the grammar of graphics, but it also adds the ability to create interactive graphics and deliver them over the web.
Complex relationships in big data require involved graphical displays which can be intimidating to users.  This talk uses real world examples to identify confusing elements in online visualizations, and articulates a framework for using animation and story-telling to amplify their impact and usability.  Tangible and generalizable techniques applicable across fields will be presented.
Linking data to create broader data sets can dramatically improve analysis results, but what if the data sets lack common identifiers? Similarly, duplicates in data is very common, and can seriously skew analysis results. This talk covers common techniques from record linkage research for solving this, as well as an open source tool implementing those techniques, and real-world examples.
Processing huge volume event streams in realtime poses quite some challenges. Based on our experience with social media data and realtime user interaction data, we discuss our experience with building such systems starting with a single computer. We have distilled this experience in a number of realtime data analysis patterns, which solve key aspects of such systems.
One of the most exciting areas in Big Data is the development of new data products; predictive applications used to drive product recommendations, predict machine failures, forecast airfare, social match-make, identify fraud, predict disease outbreaks, and repurpose pharmaceuticals. In this talk, Ill share the trends were seeing in predictive application development, show how to....
Many people assume that researching/designing a predictive modeling algorithm is the hard part of building a predictive modeling system over Big Data. We will focus on the far less romantic infrastructure needed to support a system, by reviewing the necessary components and the common pitfalls encountered when trying to automate both horizontally and vertically scalable systems.
Computing various quantities such as medians or the number of unique elements requires a lot of time or a lot of memory or both. It is, however, possible to get really close to the right answer with much less time and much less memory. Such algorithms can be simpler than you might expect. I will describe these and show how they can be applied to applications like anomaly detection.
A lot of decisions are made for us based on data  but are we at risk of crossing over into the uncanny valley of over-familiar personalisation? Designers need to focus on human elements, rather than allowing tech to lead the way. Jesus Gorriti will discuss SMART, a collaboration with the Harvard Medical School where the pediatric growth chart was reinvented using big data and design thinking.
Making meaning and value from data is not only a job for data scientists. Ethnographic researchers, subject matter experts, visual communication designers, and behavioral scientists all play key roles in the data journey. In this talk, we'll explore the data value chain, and share opportunities for how all of us -- whether data scientists or not -- can create and use data for insight and impact.
We have the unfortunate tendency to fit our problems to the technology at hand. We should be looking for ways to bend technology to our problems...our big problems. Kim will take a long look into the future of data covering the controversial and hopeful areas of privacy, open data, hacking, ETL relief, latent machines, M2M, and mass crowdsourcing.
Experiences from development of contextual applications, especially on data, design and privacy issues
The ggvis package makes it easy to create interactive data graphics with R, with a declarative syntax similar to that of ggplot2. Like ggplot2, ggvis uses concepts from the grammar of graphics, but it also adds the ability to create interactive graphics and deliver them over the web.
Complex relationships in big data require involved graphical displays which can be intimidating to users.  This talk uses real world examples to identify confusing elements in online visualizations, and articulates a framework for using animation and story-telling to amplify their impact and usability.  Tangible and generalizable techniques applicable across fields will be presented.
While many companies are struggling to adopt big data and to unlock its potential, facing challenges of visualization and democratization of insight, a number of industry leaders are leapfrogging big data adoption and circumvent the analyst bottleneck by going straight to automation of core business processes. This requires overcoming a set of tough cultural, technical and scientific challenges.
By having understandable abstractions for important data objects, Etsy has enabled employees across the whole company to actively take part in the collection and analysis of data. Converting data to objects allows us to more naturally convert analysis questions into code, and enforce business rules and definitions consistently.
Demonstrating how to use Google Docs for a flexible, extensible, self-service front-end for your data warehouse. A simple, cheap, stable, flexible, user-friendly alternative to traditional tools.
In this session, Alistair Croll, author of the best-selling Lean Analytics and chair of OReilly Strata, will share what hes learned in a year of working with and interviewing intrapreneurs all over the world.
Outbrain serves 150 billion content recommendations to more than 500 million monthly users. Data tells us whats driving the mindset of the crowed. But how do you analyze if the individual user finds value in recommendations? Why being satisfied with click-focused-metrics is dangerous for growth? We outline a 3-layer framework for Data Scientists to analyze user engagement, facing such challenges.
A data strategy is only as good as its execution. In the world of Data Science it has become increasingly apparent that business leaders focus on the technical aspects for success in data projects, when in fact the quality of the data team is key. I will in this talk share my experiences training data scientists, and give some key insights into how to build a high-performing Data Science team.
Thanks to technologies like NoSQL and Hadoop, organizations can store massive amounts of data thats important to their business. Now the challenge is how to extract actionable insights from it. This session will explore why search is the foundation to gain value from big data across your business - from marketing, to product, to backend infrastructure - highlighting a few real-world examples.
The next generation of MapReduce, YARN, has widely touted job throughput and Apache Hadoop cluster utilization benefits. Less known are the pitfalls littering the migration path to YARN. Learn from our extensive field experience to avoid those pitfalls and get your YARN cluster configured right the first time.
Your application is out-growing its database, you've started shopping NoSQL options. Maybe you've adopted Hadoop into your Data Warehouse. You've heard HBase might be an appropriate technology, but you need to know more. This talk is for you. To understand its use, first understand how it works. This talk explores the design of HBase and its critical paths to ground an understanding of its use.
We will describe our experiences in implementing a full-scale, data-driven application applied to a large anonymised dataset from the mobile operator Telefonica using Map-Reduce  Our project was unusual in the breadth of techniques used and also in the diversity in our goals. We will provide our perspective based on our project and examine how upcoming technologies would have impacted our efforts
Creating a data architecture involves many moving parts. By examining the data value chain, from ingestion through to analytics, we will explain how the various parts of the Hadoop and big data ecosystem fit together to support batch, interactive and realtime analytical workloads.
Apache Mesos, Apache Hadoop, Apache Spark + Custom Enterprise Applications: This stack combined is greater than the sum of each of the pieces of this stack. Couple all of that with custom enterprise applications, and the data center turns into a well-oiled machine. When combined, this software stack delivers unlimited flexibility for the entire data center.
Communicating Data Clearly describes how to draw clear, concise, accurate graphs that are easier to understand than many of the graphs one sees today.  The tutorial emphasizes how to avoid common mistakes that produce confusing or even misleading graphs. Graphs for one, two, three, and many variables are covered as well as general principles for creating effective graphs.
D3.js has a very steep learning curve. However, there are three main concepts that, once you get your head around them, will make the climb much easier. Focusing on these three main concepts, we will walk through many examples to teach the fundamental building blocks of D3.js.
While many companies are struggling to adopt big data and to unlock its potential, facing challenges of visualization and democratization of insight, a number of industry leaders are leapfrogging big data adoption and circumvent the analyst bottleneck by going straight to automation of core business processes. This requires overcoming a set of tough cultural, technical and scientific challenges.
By having understandable abstractions for important data objects, Etsy has enabled employees across the whole company to actively take part in the collection and analysis of data. Converting data to objects allows us to more naturally convert analysis questions into code, and enforce business rules and definitions consistently.
Demonstrating how to use Google Docs for a flexible, extensible, self-service front-end for your data warehouse. A simple, cheap, stable, flexible, user-friendly alternative to traditional tools.
In this session, Alistair Croll, author of the best-selling Lean Analytics and chair of OReilly Strata, will share what hes learned in a year of working with and interviewing intrapreneurs all over the world.
Outbrain serves 150 billion content recommendations to more than 500 million monthly users. Data tells us whats driving the mindset of the crowed. But how do you analyze if the individual user finds value in recommendations? Why being satisfied with click-focused-metrics is dangerous for growth? We outline a 3-layer framework for Data Scientists to analyze user engagement, facing such challenges.
A data strategy is only as good as its execution. In the world of Data Science it has become increasingly apparent that business leaders focus on the technical aspects for success in data projects, when in fact the quality of the data team is key. I will in this talk share my experiences training data scientists, and give some key insights into how to build a high-performing Data Science team.
Thanks to technologies like NoSQL and Hadoop, organizations can store massive amounts of data thats important to their business. Now the challenge is how to extract actionable insights from it. This session will explore why search is the foundation to gain value from big data across your business - from marketing, to product, to backend infrastructure - highlighting a few real-world examples.
The next generation of MapReduce, YARN, has widely touted job throughput and Apache Hadoop cluster utilization benefits. Less known are the pitfalls littering the migration path to YARN. Learn from our extensive field experience to avoid those pitfalls and get your YARN cluster configured right the first time.
Your application is out-growing its database, you've started shopping NoSQL options. Maybe you've adopted Hadoop into your Data Warehouse. You've heard HBase might be an appropriate technology, but you need to know more. This talk is for you. To understand its use, first understand how it works. This talk explores the design of HBase and its critical paths to ground an understanding of its use.
We will describe our experiences in implementing a full-scale, data-driven application applied to a large anonymised dataset from the mobile operator Telefonica using Map-Reduce  Our project was unusual in the breadth of techniques used and also in the diversity in our goals. We will provide our perspective based on our project and examine how upcoming technologies would have impacted our efforts
Creating a data architecture involves many moving parts. By examining the data value chain, from ingestion through to analytics, we will explain how the various parts of the Hadoop and big data ecosystem fit together to support batch, interactive and realtime analytical workloads.
Apache Mesos, Apache Hadoop, Apache Spark + Custom Enterprise Applications: This stack combined is greater than the sum of each of the pieces of this stack. Couple all of that with custom enterprise applications, and the data center turns into a well-oiled machine. When combined, this software stack delivers unlimited flexibility for the entire data center.
Communicating Data Clearly describes how to draw clear, concise, accurate graphs that are easier to understand than many of the graphs one sees today.  The tutorial emphasizes how to avoid common mistakes that produce confusing or even misleading graphs. Graphs for one, two, three, and many variables are covered as well as general principles for creating effective graphs.
D3.js has a very steep learning curve. However, there are three main concepts that, once you get your head around them, will make the climb much easier. Focusing on these three main concepts, we will walk through many examples to teach the fundamental building blocks of D3.js.
We will discuss requirements for IoT data processing platforms incl. stream processing, dealing with raw device data, ensuring business continuity and to enforce security and privacy. We will dissect a number of IoT applications, such as a manufacturer offering pro-active maintenance, optimisations of waste management as well as streamlining a supply chain.
Enterprise IT Management tools play a key role in helping IT  organizations deliver a high level of service to their customers and manage the ongoing operation of production and mission critical systems according to regulatory requirements and to meet the goals of the business...
Analytics 3.0 is all about exploiting big data for just-in-time results to impact business outcomes.  But what's really changing?
This presentation addresses the geographic scalability of HDFS. It describes unique techniques implemented at WANdisco, which allow scaling HDFS over multiple geographically distributed data centers for continuous availability. . .
This session will show the evolution of big data at UniCredit, from troubleshooting and application monitoring to the real-time analytics of ATMs, mobile banking, transactions and card usage. It will go under the hood of technical decisions in setting up a scalable and reliable architecture and dealing with a heterogeneous, geographical distributed and multi-layered environment.
We will discuss requirements for IoT data processing platforms incl. stream processing, dealing with raw device data, ensuring business continuity and to enforce security and privacy. We will dissect a number of IoT applications, such as a manufacturer offering pro-active maintenance, optimisations of waste management as well as streamlining a supply chain.
Enterprise IT Management tools play a key role in helping IT  organizations deliver a high level of service to their customers and manage the ongoing operation of production and mission critical systems according to regulatory requirements and to meet the goals of the business...
Analytics 3.0 is all about exploiting big data for just-in-time results to impact business outcomes.  But what's really changing?
This presentation addresses the geographic scalability of HDFS. It describes unique techniques implemented at WANdisco, which allow scaling HDFS over multiple geographically distributed data centers for continuous availability. . .
This session will show the evolution of big data at UniCredit, from troubleshooting and application monitoring to the real-time analytics of ATMs, mobile banking, transactions and card usage. It will go under the hood of technical decisions in setting up a scalable and reliable architecture and dealing with a heterogeneous, geographical distributed and multi-layered environment.
How can big data make your journey to work better? In this case study well explore how! Trains today are complex systems consisting of many embedded subsystems, which operate together with the overall goal of delivering a high quality transportation service...
Lean how Pentaho's data integration and business analytics platform accelerates value from blended big data. *       Leverage analytics -from data access and integration, through visualisation and predictive analytics to deliver near real-time business insights. *       Empower users to architect big data blends at the source AND stream for more complete and accurate analytics...
Key takeaways: The business drivers and objectives Multi-tenancy concepts and architecture Multi-tenancy features in EDH Multi-tenancy configuration in EDH
Its been twenty years since Red Hat first launched Linux. Since then Red Hat has fueled the rapid adoption of open source technologies. As Big Data transitions into enterprise mode, Red Hat is again poised to facilitate the innovation and communities needed to empower multiple data stakeholders across your organization so you can truly open the possibilities of your data.
Understanding the balance between Availability, Risk and Trust when dealing with big data analytics.  As we approach the end of 2014 more people are talking big data than ever before, but what we are now calling big data is just a drop in the ocean. The danger we all face is that as we step back to consider just how beautifully BIG our data is getting, we start to lose control.
Join the Spark Team for an informal question and answer session.
How can big data make your journey to work better? In this case study well explore how! Trains today are complex systems consisting of many embedded subsystems, which operate together with the overall goal of delivering a high quality transportation service...
Lean how Pentaho's data integration and business analytics platform accelerates value from blended big data. *       Leverage analytics -from data access and integration, through visualisation and predictive analytics to deliver near real-time business insights. *       Empower users to architect big data blends at the source AND stream for more complete and accurate analytics...
Key takeaways: The business drivers and objectives Multi-tenancy concepts and architecture Multi-tenancy features in EDH Multi-tenancy configuration in EDH
Its been twenty years since Red Hat first launched Linux. Since then Red Hat has fueled the rapid adoption of open source technologies. As Big Data transitions into enterprise mode, Red Hat is again poised to facilitate the innovation and communities needed to empower multiple data stakeholders across your organization so you can truly open the possibilities of your data.
Understanding the balance between Availability, Risk and Trust when dealing with big data analytics.  As we approach the end of 2014 more people are talking big data than ever before, but what we are now calling big data is just a drop in the ocean. The danger we all face is that as we step back to consider just how beautifully BIG our data is getting, we start to lose control.
Join the Spark Team for an informal question and answer session.
We show how to use road sensor data for making reliable statistics about traffic intensities on the 3000 km long Dutch motorways. To use the data of 20.000 road sensors, dimension reduction is applied on the sensor data, which is highly redundant, for compensating the poor quality of the data.
IoT analytic brings an engineering and analytic complexity to the new market solutions.In this session we will share the learnings from the development of Intel's Cloud IoT Analytics Platform based on open source software.We will share learning from the product development and present use case in the Parkinson Disease research, leverages wearable sensors to monitor PD patients activities,24/7.
Creating a backend for data intensive apps requires gluing several technologies together, which isnt always simple, cheap or scalable. The world of sensor and IoT data, together with privacy concerns (mostly European), and the need to make contextual sense of it all, presents an opportunity to bring in the post-hadoop era and democratise data stores.
Too many big data sets live in walled gardens and thus limit innovation to a few players. Creating open data sets levels the playing field and allows open source hackers to participate.
We recently moved our entire data infrastructure to AWS: we now use Elastic MapReduce, Redshift and S3 for storage and processing. The talk describes the benefits and challenges of running in the cloud, how treating storage and processing as a utility allowed our small team to work on tools that democratized access to business analytics across the company and made us more happy in general.
Borrowing from Spanish information security best practices and in the light of increasing data breach regulations, the presentation examines how data flows should ideally be defined and secured in order to assure accountability through an entire data lifecycle.
We're living in an age of big data, a time when most of our movements and actions are collected and stored in real time. These data offer unprecedented insights on how we behave as a species. Mathematical analysis of location data however reveals how unique our individual behavior is and how this behavior puts fundamental constraints on our privacy.
We often face the need to analyze the count of discrete events which occur at a specific time and place whether they be crime events, taxi requests, or phone calls.  Forecasting these space-time events brings particular challenges: finding suitable tools for geographic processing and techniques for modeling the data.  The session will cover the lessons learned in building such a system.
Before Edward Snowden disclosed the US intelligence services digital surveillance, marketers had been collecting, aggregating and inferring behavioral profiles on consumers around the world.  This talk describes the chief technologies firms use to transform online activities into target audience segments, as well as the current and proposed regulations and public policies being considered.
Smartphones carry mighty sensors: GPS, wifi, acceleration, gyroscope, microphone, magnetic field, etc., tracking behavior and environment, giving answer to complex questions like "is the user driving in a car or riding on a train?" We will show cases from travel industry, sports retail, and health. We will propose, how to use such intrusive technology in an ethically correct way.
Breaking news from data that's already published, that's efficient Open Source Intelligence applied to journalism. The tools and methodologies  available today make it possible to go big on a budget.
We show how to use road sensor data for making reliable statistics about traffic intensities on the 3000 km long Dutch motorways. To use the data of 20.000 road sensors, dimension reduction is applied on the sensor data, which is highly redundant, for compensating the poor quality of the data.
IoT analytic brings an engineering and analytic complexity to the new market solutions.In this session we will share the learnings from the development of Intel's Cloud IoT Analytics Platform based on open source software.We will share learning from the product development and present use case in the Parkinson Disease research, leverages wearable sensors to monitor PD patients activities,24/7.
Creating a backend for data intensive apps requires gluing several technologies together, which isnt always simple, cheap or scalable. The world of sensor and IoT data, together with privacy concerns (mostly European), and the need to make contextual sense of it all, presents an opportunity to bring in the post-hadoop era and democratise data stores.
Too many big data sets live in walled gardens and thus limit innovation to a few players. Creating open data sets levels the playing field and allows open source hackers to participate.
We recently moved our entire data infrastructure to AWS: we now use Elastic MapReduce, Redshift and S3 for storage and processing. The talk describes the benefits and challenges of running in the cloud, how treating storage and processing as a utility allowed our small team to work on tools that democratized access to business analytics across the company and made us more happy in general.
Borrowing from Spanish information security best practices and in the light of increasing data breach regulations, the presentation examines how data flows should ideally be defined and secured in order to assure accountability through an entire data lifecycle.
We're living in an age of big data, a time when most of our movements and actions are collected and stored in real time. These data offer unprecedented insights on how we behave as a species. Mathematical analysis of location data however reveals how unique our individual behavior is and how this behavior puts fundamental constraints on our privacy.
We often face the need to analyze the count of discrete events which occur at a specific time and place whether they be crime events, taxi requests, or phone calls.  Forecasting these space-time events brings particular challenges: finding suitable tools for geographic processing and techniques for modeling the data.  The session will cover the lessons learned in building such a system.
Before Edward Snowden disclosed the US intelligence services digital surveillance, marketers had been collecting, aggregating and inferring behavioral profiles on consumers around the world.  This talk describes the chief technologies firms use to transform online activities into target audience segments, as well as the current and proposed regulations and public policies being considered.
Smartphones carry mighty sensors: GPS, wifi, acceleration, gyroscope, microphone, magnetic field, etc., tracking behavior and environment, giving answer to complex questions like "is the user driving in a car or riding on a train?" We will show cases from travel industry, sports retail, and health. We will propose, how to use such intrusive technology in an ethically correct way.
Breaking news from data that's already published, that's efficient Open Source Intelligence applied to journalism. The tools and methodologies  available today make it possible to go big on a budget.
In this presentation Doug Cutting, Cloudera's Chief Architect, will discuss how we might both reap the benefits of data while avoiding its perils.
Strata Barcelona Program Chairs, Roger Magoulas, Doug Cutting, and Edd Dumbill, welcome you to the second day of keynotes.
As we've moved from simple statistical analyses of big data to decision-making based on big data and data-science models, we face an ironic "dirty secret."  It is becoming increasingly difficult to understand why particular decisions have been made.  In many applications, data-driven models now take as input massive numbers of "signals", including words in text, locations frequented...
Drinking from the data lake is tempting, but what is it really? How did we get here, and what lessons can we learn from previous technologies? Its tempting to see this as the solution to data silos, but what are the costs? Martin Willcox provides a practical guide to help you understand the realities
Open data isn't just about waste pickup schedules and reporting pot holesit can hold real monetary value for everyday business. Whether it's supply chain enhancement or improved customer segmentation, open data holds unexpected value for everyone.
How can you turn raw data into predictions? How can you take advantage of both cloud scalability and state-of-the-art Open Source Software? This talk shows how we built a model that correctly predicted the outcome of 14 of 16 games in the World Cup using Google's Cloud Platform and tools like iPython and StatsModels.
Ever do something perfectly in practice, only to have it blow up as soon as you try it when it really counts? This little phenomenon sends skaters to the hospital on a regular basis, mainly because controlled environments usually cant evoke the depths of human responses.
Exploiting big data and analytics through the whole organisation is now business as usual for retail and online businesses. But cities and buildings also create a whole lot of data, which could change lives for better or for worse.  This talk explores whats happening right now in big data and analytics for cities and buildings, where it might head, and what we might want from it all.
This talk explores the critical importance of storytelling to science and what we can learn from that relationship.
Program Chairs, Roger Magoulas, Doug Cutting, and Edd Dumbill, welcome you to the first day of keynotes.
Mike Olson, CSO and Chairman, Cloudera
McLaren Applied Technologies capitalises on the convergence of real-time data management, predictive analytics and simulation to produce high performance design of products and processes.  In this talk we will describe how the approach of data-driven design can transform the way we go about creating and using products that are intrinsically intelligent and capable of adaptation
Big Data & Analytics continues to be a disruptive business force. Are we entering a new phase  Big Data & Analytics 3.0?
Welcome to the era of big, bad, open information. Analysts have predicted huge numbers of Internet-connected devices in our future for years now. We may dispute the number, but it is clear that the Internet of Things (IoT) will produce a colossal amount of data.
Camille Fournier, Head of Engineering, Rent the Runway
WANdisco CEO and Co-Founder David Richards will explore mission critical applications of Big Data across industry sectors, and highlight the importance of continuous availability, performance, and scalability in its application.
The network, new data capabilities, and mobile devices rich in sensors have created fresh and unconventional possibilities to rethink workflows and processes in the real world. To succeed in creating totally new services and rethinking old ones, we must first adopt fresh thinking about the design process, and how sensors and algorithms are driving significant changes in what is possible.
In this presentation Doug Cutting, Cloudera's Chief Architect, will discuss how we might both reap the benefits of data while avoiding its perils.
Strata Barcelona Program Chairs, Roger Magoulas, Doug Cutting, and Edd Dumbill, welcome you to the second day of keynotes.
As we've moved from simple statistical analyses of big data to decision-making based on big data and data-science models, we face an ironic "dirty secret."  It is becoming increasingly difficult to understand why particular decisions have been made.  In many applications, data-driven models now take as input massive numbers of "signals", including words in text, locations frequented...
Drinking from the data lake is tempting, but what is it really? How did we get here, and what lessons can we learn from previous technologies? Its tempting to see this as the solution to data silos, but what are the costs? Martin Willcox provides a practical guide to help you understand the realities
Open data isn't just about waste pickup schedules and reporting pot holesit can hold real monetary value for everyday business. Whether it's supply chain enhancement or improved customer segmentation, open data holds unexpected value for everyone.
How can you turn raw data into predictions? How can you take advantage of both cloud scalability and state-of-the-art Open Source Software? This talk shows how we built a model that correctly predicted the outcome of 14 of 16 games in the World Cup using Google's Cloud Platform and tools like iPython and StatsModels.
Ever do something perfectly in practice, only to have it blow up as soon as you try it when it really counts? This little phenomenon sends skaters to the hospital on a regular basis, mainly because controlled environments usually cant evoke the depths of human responses.
Exploiting big data and analytics through the whole organisation is now business as usual for retail and online businesses. But cities and buildings also create a whole lot of data, which could change lives for better or for worse.  This talk explores whats happening right now in big data and analytics for cities and buildings, where it might head, and what we might want from it all.
This talk explores the critical importance of storytelling to science and what we can learn from that relationship.
Program Chairs, Roger Magoulas, Doug Cutting, and Edd Dumbill, welcome you to the first day of keynotes.
Mike Olson, CSO and Chairman, Cloudera
McLaren Applied Technologies capitalises on the convergence of real-time data management, predictive analytics and simulation to produce high performance design of products and processes.  In this talk we will describe how the approach of data-driven design can transform the way we go about creating and using products that are intrinsically intelligent and capable of adaptation
Big Data & Analytics continues to be a disruptive business force. Are we entering a new phase  Big Data & Analytics 3.0?
Welcome to the era of big, bad, open information. Analysts have predicted huge numbers of Internet-connected devices in our future for years now. We may dispute the number, but it is clear that the Internet of Things (IoT) will produce a colossal amount of data.
Camille Fournier, Head of Engineering, Rent the Runway
WANdisco CEO and Co-Founder David Richards will explore mission critical applications of Big Data across industry sectors, and highlight the importance of continuous availability, performance, and scalability in its application.
The network, new data capabilities, and mobile devices rich in sensors have created fresh and unconventional possibilities to rethink workflows and processes in the real world. To succeed in creating totally new services and rethinking old ones, we must first adopt fresh thinking about the design process, and how sensors and algorithms are driving significant changes in what is possible.
Machine learning and paid crowdsourcing power several virtuous cycles in Locu's data processing pipeline.  To solve various problems, we interact with hundreds of long-term crowd workers on oDesk and tens of thousands of shorter-term workers on CrowdFlower.  Come learn about Locu's magic with examples based on problems we solve every day.
Creating value from big, messy data sets can be a daunting task. The session introduces the Sidekick Pattern: using small, curated data to increase the value of Big Data.  Drawing on lessons from data science for Jawbones UP fitness tracker, we will see how smart selection of data sidekicks can accelerate analysis, solve cold start problems, and simplify complicated data pipelines.
IPython and scikit-learn offer a nice environment for interactive data analytics in general and predictive modelling in particular. This presentation will give an overview on how to use both to perform tasks such as distributed model parameter tuning and parallel training of Random Forests on ad hoc compute clusters provisioned in the cloud.
Predictive models are popular for their ability to grapple with massive data and bring to light features which are non-obvious to even the best domain experts. Solving practical problems with real world data involves creating models that balance predictive accuracy with practical significance. This talk provides examples of this balance in optimizing Chicago area bars and extends to Bing search.
A new generation of data processing systems, including web search, Google's Knowledge Graph, IBM's Watson, and several different recommendation systems, combine rich databases with software driven by machine learning. This talk describes our recent thoughts on one crucial pain point in the construction of trained systems feature engineering.
SNA, social network analysis, is a powerful technique for making sense of a connected world.  But the skills needed to collect, analyze, visualize, and gain insights into collections of connections are hard to find.  Now, new tools make networks as easy to manage as a pie chart.  Using the familiar Excel spreadsheet, NodeXL enables end users to gain insights into Twitter, Facebook & more.
In this panel discussion, experts from four different industries will share their first-hand experiences building and deploying teams of data scientists.
Probabilistic programming is a new paradigm for modeling and inference that offers hope for a fundamental shift in our approach to understanding the stories behind our data. This talk will provide an overview of the systems currently available and their relative strengths, show examples of their usage, and offer a peak at the road ahead.
Recent years have seen an explosion of technologies for managing and analyzing graphs. While most people associate "graph" with "the social graph," there's a wide variety of non-social use cases for graph technologies. This session will explore graph adoption in finance, telecom, healthcare, HR & recruiting, gaming and beyond, using concrete case studies from actual graph production deployments.
Analytics and agility sometimes seem like natural enemies, but analytics suffer the same shifting requirements and uncertainty as other projects. This talk describe technique for incorporating analytics and data science into an agile rhythm.
Implementing and consuming Machine Learning techniques at scale are difficult tasks for ML Developers and End Users. MLbase (www.mlbase.org) is an open-source platform under active development addressing the issues of both groups.  In this talk we will describe the high-level functionality of MLbase and demonstrate its *scalability* and *ease-of-use* via real-world examples.
Data scientists know how hard it is to collect, categorize and label vast amounts of data.  But some smart data scientists are effectively leveraging the human intelligence of the crowd to solve these problems, resulting in better training of machine learning models and improved system performance.
Graph analytics promises to uncover new patterns in big data - but it's not easy to use commercially. Why is it so tough for data scientists to construct graphs and extract insight? This talk discusses Intel's efforts to deliver a graph cluster solution that is as easy to work with as it is powerful.
This talk will address some of the pressing problems in data preparation, analysis, visualization, and collaboration facing the modern data analyst. We will discuss the ways in which both programmatic and UI-driven tools are helping solve these problems and the areas in which more work and innovation are needed.
In a thrilling breakthrough at the intersection of neuroscience and statistics, penalized Least Squares methods have been used to construct a "mind-reading" algorithm that reconstructs movies from fMRI brain signals.
Machine learning and paid crowdsourcing power several virtuous cycles in Locu's data processing pipeline.  To solve various problems, we interact with hundreds of long-term crowd workers on oDesk and tens of thousands of shorter-term workers on CrowdFlower.  Come learn about Locu's magic with examples based on problems we solve every day.
Creating value from big, messy data sets can be a daunting task. The session introduces the Sidekick Pattern: using small, curated data to increase the value of Big Data.  Drawing on lessons from data science for Jawbones UP fitness tracker, we will see how smart selection of data sidekicks can accelerate analysis, solve cold start problems, and simplify complicated data pipelines.
IPython and scikit-learn offer a nice environment for interactive data analytics in general and predictive modelling in particular. This presentation will give an overview on how to use both to perform tasks such as distributed model parameter tuning and parallel training of Random Forests on ad hoc compute clusters provisioned in the cloud.
Predictive models are popular for their ability to grapple with massive data and bring to light features which are non-obvious to even the best domain experts. Solving practical problems with real world data involves creating models that balance predictive accuracy with practical significance. This talk provides examples of this balance in optimizing Chicago area bars and extends to Bing search.
A new generation of data processing systems, including web search, Google's Knowledge Graph, IBM's Watson, and several different recommendation systems, combine rich databases with software driven by machine learning. This talk describes our recent thoughts on one crucial pain point in the construction of trained systems feature engineering.
SNA, social network analysis, is a powerful technique for making sense of a connected world.  But the skills needed to collect, analyze, visualize, and gain insights into collections of connections are hard to find.  Now, new tools make networks as easy to manage as a pie chart.  Using the familiar Excel spreadsheet, NodeXL enables end users to gain insights into Twitter, Facebook & more.
In this panel discussion, experts from four different industries will share their first-hand experiences building and deploying teams of data scientists.
Probabilistic programming is a new paradigm for modeling and inference that offers hope for a fundamental shift in our approach to understanding the stories behind our data. This talk will provide an overview of the systems currently available and their relative strengths, show examples of their usage, and offer a peak at the road ahead.
Recent years have seen an explosion of technologies for managing and analyzing graphs. While most people associate "graph" with "the social graph," there's a wide variety of non-social use cases for graph technologies. This session will explore graph adoption in finance, telecom, healthcare, HR & recruiting, gaming and beyond, using concrete case studies from actual graph production deployments.
Analytics and agility sometimes seem like natural enemies, but analytics suffer the same shifting requirements and uncertainty as other projects. This talk describe technique for incorporating analytics and data science into an agile rhythm.
Implementing and consuming Machine Learning techniques at scale are difficult tasks for ML Developers and End Users. MLbase (www.mlbase.org) is an open-source platform under active development addressing the issues of both groups.  In this talk we will describe the high-level functionality of MLbase and demonstrate its *scalability* and *ease-of-use* via real-world examples.
Data scientists know how hard it is to collect, categorize and label vast amounts of data.  But some smart data scientists are effectively leveraging the human intelligence of the crowd to solve these problems, resulting in better training of machine learning models and improved system performance.
Graph analytics promises to uncover new patterns in big data - but it's not easy to use commercially. Why is it so tough for data scientists to construct graphs and extract insight? This talk discusses Intel's efforts to deliver a graph cluster solution that is as easy to work with as it is powerful.
This talk will address some of the pressing problems in data preparation, analysis, visualization, and collaboration facing the modern data analyst. We will discuss the ways in which both programmatic and UI-driven tools are helping solve these problems and the areas in which more work and innovation are needed.
In a thrilling breakthrough at the intersection of neuroscience and statistics, penalized Least Squares methods have been used to construct a "mind-reading" algorithm that reconstructs movies from fMRI brain signals.
Epstein explains the origins of the "magic number," how it should be used, and how it is often misused in a manner that often hinders performance science-and leads sports executives to overlook simple but important data-as well as the development of athletes.
The United States Patent and Trademark Office wanted a simple, lightweight, yet modern and rich discovery interface for Chinese patent data.  This is how we did it.
Twitter's Observability stack collects, processes, monitors and visualizes over 170 million real-time time series from all service and system components. This session covers how the stack is built and scales to enable developers and reliability engineers to build fault-tolerant distributed services.  In this talk, you will learn what works and what doesnt, from architecture to implementation.
Netflix is a data-driven company. While "data-driven" is often no more than a lofty buzzword, we'll discuss how we make it a reality. We'll dive into the technologies we use and the philosophies underpinning how we get things done. We'll cover our "cloud native" data infrastructure, our use and contributions to open source software, and our open and enabling data environment.
Data analytics is at the heart of product development at Facebook.  Facebooks data warehouse has grown rapidly over the years, and poses unique scalability challenges. This talk will briefly outline the evolution of the analytics software stack in the last year (both storage and query engines) and then delve deeper into the data management and compute challenges at this scale.
Yahoo! ingests hundreds of TB of advertising data into Hadoop each day. This talk describes how we are building our next-generation data architecture on top of Shark and Spark that is orders of magnitude faster than the previous. We will focus on the advanced streaming algorithms implemented in this new architecture, and how the new architecture have enabled deeper insights to our data scientists.
DARPA's XDATA program seeks to develop open source software to address government Big Data at all stages, from analysis to operations, in the areas of scalable analytics, processing, visualizations, and UIs. This new multi-year effort involves over 25 teams from academia, research labs, and small and large businesses, and includes efforts around Hadoop, Python, R, and other technologies.
What if students could be provided helpful feedback in real-time based on the notes they are  typing in class?  This talk presents a prototype that has been in use in multiple courses at the University of Michigan to both challenge students' understanding based on the words they type in class and offer further resources for further study.
Analyzing graphs can lead to useful insights that drive product and business decisions.  This talk describes our efforts at Facebook to scale Apache Giraph to very large graphs (up to one trillion edges) and how we run Apache Giraph in production.  We will also talk about how to build applications, some of the algorithms that we have implemented, and their use cases.
Social networks bring a new dimension to search. Instead of looking for web pages, users search a world of entities connected by a rich graph of relationships. Serving billions of deeply personalized searches creates unique infrastructure and relevance challenges for LinkedIn. We'll describe how we've addressed those challenges and discuss implications of social networks for the future of search.
The maturation and development of open source technologies has made it easier than ever for companies to derive insights from vast quantities of data. In this session, we will cover how to build a real-time analytics stack using Kafka, Storm, and Druid. This combination of technologies can power a robust data pipeline that supports real-time ingestion and flexible, low-latency queries.
Epstein explains the origins of the "magic number," how it should be used, and how it is often misused in a manner that often hinders performance science-and leads sports executives to overlook simple but important data-as well as the development of athletes.
The United States Patent and Trademark Office wanted a simple, lightweight, yet modern and rich discovery interface for Chinese patent data.  This is how we did it.
Twitter's Observability stack collects, processes, monitors and visualizes over 170 million real-time time series from all service and system components. This session covers how the stack is built and scales to enable developers and reliability engineers to build fault-tolerant distributed services.  In this talk, you will learn what works and what doesnt, from architecture to implementation.
Netflix is a data-driven company. While "data-driven" is often no more than a lofty buzzword, we'll discuss how we make it a reality. We'll dive into the technologies we use and the philosophies underpinning how we get things done. We'll cover our "cloud native" data infrastructure, our use and contributions to open source software, and our open and enabling data environment.
Data analytics is at the heart of product development at Facebook.  Facebooks data warehouse has grown rapidly over the years, and poses unique scalability challenges. This talk will briefly outline the evolution of the analytics software stack in the last year (both storage and query engines) and then delve deeper into the data management and compute challenges at this scale.
Yahoo! ingests hundreds of TB of advertising data into Hadoop each day. This talk describes how we are building our next-generation data architecture on top of Shark and Spark that is orders of magnitude faster than the previous. We will focus on the advanced streaming algorithms implemented in this new architecture, and how the new architecture have enabled deeper insights to our data scientists.
DARPA's XDATA program seeks to develop open source software to address government Big Data at all stages, from analysis to operations, in the areas of scalable analytics, processing, visualizations, and UIs. This new multi-year effort involves over 25 teams from academia, research labs, and small and large businesses, and includes efforts around Hadoop, Python, R, and other technologies.
What if students could be provided helpful feedback in real-time based on the notes they are  typing in class?  This talk presents a prototype that has been in use in multiple courses at the University of Michigan to both challenge students' understanding based on the words they type in class and offer further resources for further study.
Analyzing graphs can lead to useful insights that drive product and business decisions.  This talk describes our efforts at Facebook to scale Apache Giraph to very large graphs (up to one trillion edges) and how we run Apache Giraph in production.  We will also talk about how to build applications, some of the algorithms that we have implemented, and their use cases.
Social networks bring a new dimension to search. Instead of looking for web pages, users search a world of entities connected by a rich graph of relationships. Serving billions of deeply personalized searches creates unique infrastructure and relevance challenges for LinkedIn. We'll describe how we've addressed those challenges and discuss implications of social networks for the future of search.
The maturation and development of open source technologies has made it easier than ever for companies to derive insights from vast quantities of data. In this session, we will cover how to build a real-time analytics stack using Kafka, Storm, and Druid. This combination of technologies can power a robust data pipeline that supports real-time ingestion and flexible, low-latency queries.
3-Hours: IPython is an open source project that provides tools for interactive and parallel computing in Python. This includes the IPython Notebook, a web-based interactive computing environment that enables users to author documents that combine code, text, equations, figures and videos. This tutorial will provide a hands-on tour of the IPython Shell, Notebook and parallel computing architecture
d3.js is a powerful tool for creating interactive charts on the web with data. But digging into D3 from scratch can make your head spin.  This tutorial will take you from scattered to building your own working, interactive scatterplots in three hours.
Statistical methods tends to fail when there is someone on the other side of a problem actively evading detection. Here we look at three systems successfully used to fight adaptive adversaries engaged in fraud and cyber attacks. Using a combination of big data techniques and interactive human analysis, these systems are protecting commercial banks, pharmaceutical companies, and governments.
With more than 45 million users and over 40,000 petitions created every month, Change.org is the biggest online platform for social change around the world. This talk is about how both bleeding edge and simple machine learning algorithms are used at Change.org to connect users to petitions and social issues which are most relevant to them.
Earlier Data Governance generations (that support BI-DW or MDM) succeeded by aligning stakeholders and improving data interoperability.  But in the world of Big Data, interoperability is table-stakes, and next-gen Data Governance must provide contextual intelligence sufficient to reason out complex inquiries across diverse data.  How?  Would you believe a mash-up of building codes and game theory?
We are at the beginning of creating a generation of scientists & analysts who can relate to data in entirely new ways.  The feeble computational models weve created in Excel over the course of our lives are fundamentally different than what is just becoming possible.
Combine your best algorithms and smartest data architecture, and what do you get? Without humans, you have an expensive, high tech brick.  Humans generate data, which is used by and for humans to achieve human goals. If you want your data department to earn its keep by showing real value, you must build your social systems as meticulously as you build your pipeline.
What can an SQL query teach us about the gender gap? We'll dive into the 20 million Freebase entities to focus on people notable enough to be part of it. What percentage of them are women? How is the gender gap divided by profession? How is it changing throughout the years? How do all this variables this look mapped at a country, state, and neighborhood level?
Endemic organized crime, augmented by corrupt governments and business interests can threaten local and regional security throughout the world. In this session we'll show how journalists can use data and technology to ferret out, investigate and combat corruption.
Join Kiran Jain, the Senior Deputy Attorney for the City of Oakland, and Shannon Spanhake, the Deputy Innovation Officer for the City and County of San Francisco, to learn how governments are changing, and being changed, by the digital age.
    Everyone knows that massive, real-time data processing is behind many of the hottest new companies in technology. But whats really going on underneath the covers? In this session, investor and technology entrepreneur Michael Abbott unboxes three startups to look at the technology, architecture, and innovations theyve harnessed to deliver their products and services.
In this session, Edgeflip and Data Science for the Social Goods Rayid Ghani, IA Ventures Scientist-in-residence and Datakind co-founder Drew Conway, and Datakind co-founder and executive director Jake Porway look at where data is making a difference today, what it promises tomorrow, and whats holding it back.
The always-popular Great Debate series returns to Strata. In this Oxford-style debate, two opposing teams take opposing positions. We poll the audience, and the teams try to sway opinions. Itll be a fast-paced, sometimes irreverent look at some of the core challenges of putting data to work.
3-Hours: IPython is an open source project that provides tools for interactive and parallel computing in Python. This includes the IPython Notebook, a web-based interactive computing environment that enables users to author documents that combine code, text, equations, figures and videos. This tutorial will provide a hands-on tour of the IPython Shell, Notebook and parallel computing architecture
d3.js is a powerful tool for creating interactive charts on the web with data. But digging into D3 from scratch can make your head spin.  This tutorial will take you from scattered to building your own working, interactive scatterplots in three hours.
Statistical methods tends to fail when there is someone on the other side of a problem actively evading detection. Here we look at three systems successfully used to fight adaptive adversaries engaged in fraud and cyber attacks. Using a combination of big data techniques and interactive human analysis, these systems are protecting commercial banks, pharmaceutical companies, and governments.
With more than 45 million users and over 40,000 petitions created every month, Change.org is the biggest online platform for social change around the world. This talk is about how both bleeding edge and simple machine learning algorithms are used at Change.org to connect users to petitions and social issues which are most relevant to them.
Earlier Data Governance generations (that support BI-DW or MDM) succeeded by aligning stakeholders and improving data interoperability.  But in the world of Big Data, interoperability is table-stakes, and next-gen Data Governance must provide contextual intelligence sufficient to reason out complex inquiries across diverse data.  How?  Would you believe a mash-up of building codes and game theory?
We are at the beginning of creating a generation of scientists & analysts who can relate to data in entirely new ways.  The feeble computational models weve created in Excel over the course of our lives are fundamentally different than what is just becoming possible.
Combine your best algorithms and smartest data architecture, and what do you get? Without humans, you have an expensive, high tech brick.  Humans generate data, which is used by and for humans to achieve human goals. If you want your data department to earn its keep by showing real value, you must build your social systems as meticulously as you build your pipeline.
What can an SQL query teach us about the gender gap? We'll dive into the 20 million Freebase entities to focus on people notable enough to be part of it. What percentage of them are women? How is the gender gap divided by profession? How is it changing throughout the years? How do all this variables this look mapped at a country, state, and neighborhood level?
Endemic organized crime, augmented by corrupt governments and business interests can threaten local and regional security throughout the world. In this session we'll show how journalists can use data and technology to ferret out, investigate and combat corruption.
Join Kiran Jain, the Senior Deputy Attorney for the City of Oakland, and Shannon Spanhake, the Deputy Innovation Officer for the City and County of San Francisco, to learn how governments are changing, and being changed, by the digital age.
    Everyone knows that massive, real-time data processing is behind many of the hottest new companies in technology. But whats really going on underneath the covers? In this session, investor and technology entrepreneur Michael Abbott unboxes three startups to look at the technology, architecture, and innovations theyve harnessed to deliver their products and services.
In this session, Edgeflip and Data Science for the Social Goods Rayid Ghani, IA Ventures Scientist-in-residence and Datakind co-founder Drew Conway, and Datakind co-founder and executive director Jake Porway look at where data is making a difference today, what it promises tomorrow, and whats holding it back.
The always-popular Great Debate series returns to Strata. In this Oxford-style debate, two opposing teams take opposing positions. We poll the audience, and the teams try to sway opinions. Itll be a fast-paced, sometimes irreverent look at some of the core challenges of putting data to work.
Data science algorithms (think machine learning, clustering, outlier detection) often get conflated with the industry-standard tools and programming languages that run them. In this tutorial, John Foreman will use only spreadsheets to build models from his book Data Smart to demonstrate exactly how data science techniques work step-by-step.
3-Hours: Adviser is a new and unique statistics and machine learning application that provides a second opinion on the results of your analysis. It incorporates a full range of analytic methods plus an expert system that flags outliers, model miss-specifications, and other anomalies. This workshop will illustrate its use in real data analyses for both novices and experts.
A maze of twisty databases, all of which look the same, and each claim they're best for the job. Welcome to the world of choosing big data vendors. In this session we'll map out the data tool landscape, and lay out a framework to help you choose a solution, or elect to build one yourself.
This talk describes how LinkedIn's engineering, data science, and reporting teams work together to develop, test, and rank new insights, recommendations, and updates shown on our home page stream.
The growing popularity of Hadoop has led to an increasing number of clusters worldwide. Priming these clusters with data from existing client repositories is difficult due to a number of issues including data size, network constraints, security & lack of domain knowledge. In this talk, we present a number of techniques & best practices for uploading large amounts of data to remote Hadoop clusters.
Apache Accumulo has evolved from a niche government project to a key component of the Hadoop ecosystem with adopters across a variety of industries.  One important differentiator for Accumulo is the concept of "cell-level security."  Learn how to properly implement cell-level security concepts from the former technical director of the Accumulo project at NSA.
Design of Experiments (DOE) is a scientific approach to understanding causality using data collection and applied statistical techniques. Through a series of relevant case studies, this session will review the design and the experiment side of DOE, including systematic data collection and basic statistical applications, and discuss relevant applications beyond A/B testing websites.
Apache Cassandra has proven to be one of the best solutions for storing and retrieving time series data. This talk will give an overview of the many ways you can be successful.
Shrikanth Shankar, Quboles VP of Engineering, shares his best practices for building high-performance, scalable queries and deploying User Defined Functions (UDFs) to Big Data applications in Apache Hive. For data analysts and data scientists in the trenches, this is a key session to attend.
The Hadoop 2.0 revolution is in full force! Organizations, companies, users are gearing up for the move from 1.0 to 2.0. In this talk, we will discuss what Hadoop 2.0 is about, what YARN is, what features that HDFS2 unlocks and what it means to move to 2.0. We'll discuss this major migration from 1.0 to 2.0 from various perspectives - admins, frameworks, end users & data processing platforms.
Spreadsheets are used extensively in industry: they are the number one tool for financial analysis. But they are as easy to build, as they are difficult to analyze, maintain and check. Feliennes research aims at developing methods to support spreadsheet users to understand, update and improve spreadsheets.
Learn how and why it is now possible for Apache Hadoop to serve as a virtual Enterprise Data Warehouse (EDW) framework for native Big Data (stored in HDFS) - making it no longer necessary to move that data into the EDW at great expense simply for analysis. In this session, attendees will get an architect-level view of the solution and explore an example configuration and benchmark numbers.
BlinkDB is an approximate query engine that answers queries in seconds on extremely large datasets by leveraging data sampling. It exploits advances in machine learning and distributed query processing to allow trading off response times and accuracy. BlinkDB is being integrated into Shark and Presto. We will cover real world use case scenarios of BlinkDB at adopters such as Facebook.
Google "Omega" research: 80% cluster jobs are batch, 60% cluster resources go to services. Batch is simple, services are hard, mixing workloads is key to building efficient distributed apps. This talk examines case studies of Mesos workloads: ranging from Twitter (100% on prem) to Airbnb (100% cloud). How did they leverage "data center OS" building blocks for orders of magnitude gains at scale?
Learn how AWS thinks about big data and how we and our customers have approached managing large datasets using services such as Amazon S3, Amazon Elastic MapReduce, Amazon DynamoDB, and Amazon Redshift.
Big-data is evolving. The state of the art has gone from running large batch queries over static data sets updated rarely to handling high-velocity data with low processing latency. In this session we present a new data framework that is geared at processing data with a very high update frequency. The framework utilizes the Go language's advanced concurrency primitives and extensibility.
Data science algorithms (think machine learning, clustering, outlier detection) often get conflated with the industry-standard tools and programming languages that run them. In this tutorial, John Foreman will use only spreadsheets to build models from his book Data Smart to demonstrate exactly how data science techniques work step-by-step.
3-Hours: Adviser is a new and unique statistics and machine learning application that provides a second opinion on the results of your analysis. It incorporates a full range of analytic methods plus an expert system that flags outliers, model miss-specifications, and other anomalies. This workshop will illustrate its use in real data analyses for both novices and experts.
A maze of twisty databases, all of which look the same, and each claim they're best for the job. Welcome to the world of choosing big data vendors. In this session we'll map out the data tool landscape, and lay out a framework to help you choose a solution, or elect to build one yourself.
This talk describes how LinkedIn's engineering, data science, and reporting teams work together to develop, test, and rank new insights, recommendations, and updates shown on our home page stream.
The growing popularity of Hadoop has led to an increasing number of clusters worldwide. Priming these clusters with data from existing client repositories is difficult due to a number of issues including data size, network constraints, security & lack of domain knowledge. In this talk, we present a number of techniques & best practices for uploading large amounts of data to remote Hadoop clusters.
Apache Accumulo has evolved from a niche government project to a key component of the Hadoop ecosystem with adopters across a variety of industries.  One important differentiator for Accumulo is the concept of "cell-level security."  Learn how to properly implement cell-level security concepts from the former technical director of the Accumulo project at NSA.
Design of Experiments (DOE) is a scientific approach to understanding causality using data collection and applied statistical techniques. Through a series of relevant case studies, this session will review the design and the experiment side of DOE, including systematic data collection and basic statistical applications, and discuss relevant applications beyond A/B testing websites.
Apache Cassandra has proven to be one of the best solutions for storing and retrieving time series data. This talk will give an overview of the many ways you can be successful.
Shrikanth Shankar, Quboles VP of Engineering, shares his best practices for building high-performance, scalable queries and deploying User Defined Functions (UDFs) to Big Data applications in Apache Hive. For data analysts and data scientists in the trenches, this is a key session to attend.
The Hadoop 2.0 revolution is in full force! Organizations, companies, users are gearing up for the move from 1.0 to 2.0. In this talk, we will discuss what Hadoop 2.0 is about, what YARN is, what features that HDFS2 unlocks and what it means to move to 2.0. We'll discuss this major migration from 1.0 to 2.0 from various perspectives - admins, frameworks, end users & data processing platforms.
Spreadsheets are used extensively in industry: they are the number one tool for financial analysis. But they are as easy to build, as they are difficult to analyze, maintain and check. Feliennes research aims at developing methods to support spreadsheet users to understand, update and improve spreadsheets.
Learn how and why it is now possible for Apache Hadoop to serve as a virtual Enterprise Data Warehouse (EDW) framework for native Big Data (stored in HDFS) - making it no longer necessary to move that data into the EDW at great expense simply for analysis. In this session, attendees will get an architect-level view of the solution and explore an example configuration and benchmark numbers.
BlinkDB is an approximate query engine that answers queries in seconds on extremely large datasets by leveraging data sampling. It exploits advances in machine learning and distributed query processing to allow trading off response times and accuracy. BlinkDB is being integrated into Shark and Presto. We will cover real world use case scenarios of BlinkDB at adopters such as Facebook.
Google "Omega" research: 80% cluster jobs are batch, 60% cluster resources go to services. Batch is simple, services are hard, mixing workloads is key to building efficient distributed apps. This talk examines case studies of Mesos workloads: ranging from Twitter (100% on prem) to Airbnb (100% cloud). How did they leverage "data center OS" building blocks for orders of magnitude gains at scale?
Learn how AWS thinks about big data and how we and our customers have approached managing large datasets using services such as Amazon S3, Amazon Elastic MapReduce, Amazon DynamoDB, and Amazon Redshift.
Big-data is evolving. The state of the art has gone from running large batch queries over static data sets updated rarely to handling high-velocity data with low processing latency. In this session we present a new data framework that is geared at processing data with a very high update frequency. The framework utilizes the Go language's advanced concurrency primitives and extensibility.
3-Hours: An introduction to the newest components of the open-source Berkeley Data Analytics Stack (BDAS) in development at UC Berkeley (and an overview of existing ones). BlinkDB is a SQL engine that provides fast approximate distributed query results. MLbase includes a library to make machine learning at scale easy. Tachyon is a file system that provides memory speed sharing across frameworks..
3-Hours: Get hands-on training with the newest components of the open-source Berkeley Data Analytics Stack (BDAS). Lessons will cover BlinkDB, MLbase, Spark, Spark Streaming, and Shark. We will provide each audience member with an EC2 cluster and walk through hands-on exercises using these technologies to analyze real-world datasets.
A core element of product innovation and successful predictive modeling, information visualization plays a central role in effective data processing pipelines. In this talk, we will explore how the technologies and workflow patterns used by LinkedIn data scientists can be applied to analytical challenges found across a wide variety of problem domains.
Storing massive data is one challenge.  Making it useful throughout all levels of a company in real time is quite another.  The ability to intuitively sort, sift and analyze data through touch and gesture is here. We will review several case studies of how companies are creating an intuitive data driven cultures through Cloudera Search, leveraging Impala coupled with Zoomdata visualization.
The true power of big data will be realized when average people can use complex analytics to solve everyday problems. We will describe a future engagement model derived from work in the Intelligence Community, reviewing real-world use cases showing how user-centric design is transforming big data from a science requiring specialists to elegant visualizations that deliver insight to average users.
Visualization is a weak link in big data tools: shoving 1MM rows into standard charts breaks their visual design and kills interactivity. In our mission to scale charts, we built the Superconductor language. It automatically compiles declarative visualizations into GPU code (WebCL+WebGL). This talk will explore how we're redesigning and optimizing core charts like heat maps and line graphs.
Happy accidents can influence one's creative process. Ian Timourian will discuss his exploration of the algorithms and techniques utilized by the famous poet Gertrude Stein through visualization.
We have developed some open-source tools for building and scaling systems for realtime data analysis with data music videos and data gastronomification. We'll discuss the theory behind these two data analysis methods, and then we'll present case studies on how our tools are used to enable business analytics and instill a data-driven culture.
This talk by Shelley Evenson, Executive Director of Organizational Evolution at Fjord, will outline the key tenets of designing for big data: the difference between using personal or aggregate data, how to identify and utilize data patterns, how to build trust, and ways to deliver ongoing value at the right moments.
A well-designed domain specific language makes all parts of the data science process easier. In this talk I'll discuss two DSLs implemented in R that make it data manipulation and visualisation both easier to describe and faster to compute.
The IPython Notebook is an open-source, web-based interactive computing environment that enables users to create documents that combine live code and data with text, equations, plots and HTML. In this talk I will describe a new interactive widget architecture for the Notebook that allows the seamless integration of JavaScript (d3.js,...) and Python for data exploration and visualization purposes.
We're failing at big data, and bigger technology isn't helping. Complex infrastructure shouldn't justify complicated experiences. Let's apply the principles of consumer app culture to enterprise decision-making in a way that goes beyond dashboards. Let's use design thinking and metadata to connect people to information in a world where complexity is inevitable and technology alone is insufficient.
Ben Fry, Principal, Fathom
You might think that art has nothing to do with dashboarding - dealing with your data architecture is an engineering/operations problem, right? On the contrary, understanding how to deal with your data in a way that is consumable by humans is fundamentally a design problem. Learn how art and design influenced the process for developing a new dashboarding tool called StatusWolf.
3-Hours: An introduction to the newest components of the open-source Berkeley Data Analytics Stack (BDAS) in development at UC Berkeley (and an overview of existing ones). BlinkDB is a SQL engine that provides fast approximate distributed query results. MLbase includes a library to make machine learning at scale easy. Tachyon is a file system that provides memory speed sharing across frameworks..
3-Hours: Get hands-on training with the newest components of the open-source Berkeley Data Analytics Stack (BDAS). Lessons will cover BlinkDB, MLbase, Spark, Spark Streaming, and Shark. We will provide each audience member with an EC2 cluster and walk through hands-on exercises using these technologies to analyze real-world datasets.
A core element of product innovation and successful predictive modeling, information visualization plays a central role in effective data processing pipelines. In this talk, we will explore how the technologies and workflow patterns used by LinkedIn data scientists can be applied to analytical challenges found across a wide variety of problem domains.
Storing massive data is one challenge.  Making it useful throughout all levels of a company in real time is quite another.  The ability to intuitively sort, sift and analyze data through touch and gesture is here. We will review several case studies of how companies are creating an intuitive data driven cultures through Cloudera Search, leveraging Impala coupled with Zoomdata visualization.
The true power of big data will be realized when average people can use complex analytics to solve everyday problems. We will describe a future engagement model derived from work in the Intelligence Community, reviewing real-world use cases showing how user-centric design is transforming big data from a science requiring specialists to elegant visualizations that deliver insight to average users.
Visualization is a weak link in big data tools: shoving 1MM rows into standard charts breaks their visual design and kills interactivity. In our mission to scale charts, we built the Superconductor language. It automatically compiles declarative visualizations into GPU code (WebCL+WebGL). This talk will explore how we're redesigning and optimizing core charts like heat maps and line graphs.
Happy accidents can influence one's creative process. Ian Timourian will discuss his exploration of the algorithms and techniques utilized by the famous poet Gertrude Stein through visualization.
We have developed some open-source tools for building and scaling systems for realtime data analysis with data music videos and data gastronomification. We'll discuss the theory behind these two data analysis methods, and then we'll present case studies on how our tools are used to enable business analytics and instill a data-driven culture.
This talk by Shelley Evenson, Executive Director of Organizational Evolution at Fjord, will outline the key tenets of designing for big data: the difference between using personal or aggregate data, how to identify and utilize data patterns, how to build trust, and ways to deliver ongoing value at the right moments.
A well-designed domain specific language makes all parts of the data science process easier. In this talk I'll discuss two DSLs implemented in R that make it data manipulation and visualisation both easier to describe and faster to compute.
The IPython Notebook is an open-source, web-based interactive computing environment that enables users to create documents that combine live code and data with text, equations, plots and HTML. In this talk I will describe a new interactive widget architecture for the Notebook that allows the seamless integration of JavaScript (d3.js,...) and Python for data exploration and visualization purposes.
We're failing at big data, and bigger technology isn't helping. Complex infrastructure shouldn't justify complicated experiences. Let's apply the principles of consumer app culture to enterprise decision-making in a way that goes beyond dashboards. Let's use design thinking and metadata to connect people to information in a world where complexity is inevitable and technology alone is insufficient.
Ben Fry, Principal, Fathom
You might think that art has nothing to do with dashboarding - dealing with your data architecture is an engineering/operations problem, right? On the contrary, understanding how to deal with your data in a way that is consumable by humans is fundamentally a design problem. Learn how art and design influenced the process for developing a new dashboarding tool called StatusWolf.
As with many other types of projects, the most crucial part of any data-oriented project is choosing an appropriate problem or opportunity to focus on in the first place.
3-Hours: Mesos is a cluster manager that provides efficient resource isolation for distributed frameworks--much like Google's "Borg" for warehouse scale computing. We'll provide hands-on experience in how to build scalable, fault-tolerant data workflows atop Mesos. We'll use Chronos to orchestrate Hadoop jobs and other data prep, then use Marathon to launch a Rails + Redis app to serve results.
As with many other types of projects, the most crucial part of any data-oriented project is choosing an appropriate problem or opportunity to focus on in the first place.
3-Hours: Mesos is a cluster manager that provides efficient resource isolation for distributed frameworks--much like Google's "Borg" for warehouse scale computing. We'll provide hands-on experience in how to build scalable, fault-tolerant data workflows atop Mesos. We'll use Chronos to orchestrate Hadoop jobs and other data prep, then use Marathon to launch a Rails + Redis app to serve results.
The measure of success for a data scientist is not number of insights, but impact on co-workers' behavior. Moving from insight to action requires an art underutilized by the data science community: storytelling.  I will cover techniques including the Fogg model, loss aversion, and minimum viable stories, using examples of my failures and successes in driving behavioral change with data.
Why have powerful tools if you aren't asking the right questions? Good questions trump shiny tools, but our community has done little to improve how we train people in the "soft side" of data science. We will show how to borrow ideas from design, the humanities, consulting practices to structure problems and improve the questions we ask of our data.
A group of VCs who invest from very early, through later stage investments talk about all things Big Data.  There will be no 3 Vs discussion here.  The Panelists are committed to making this a lively discussion about topics ranging from the typical (what sectors do they want to invest in) to the atypical (whats out there that they dont like?
The present fossil fuel based economy must give way to a renewable energy based future. The Harvard Clean Energy Project set out to discover new molecular materials for the next generation of organic solar cells. In studying 2.3 million (m) compounds with 24m conformers in 150m density functional theory calculations, this Big Data project will benefit mankind aiding the quest for clean energy.
 We optimize ads, but not our mood. We know more about our tweets than our own bodies. That's all about to change. As wearables transform the 'quantified self' from a niche to a mainstream market, they are generating vast amounts of data about our health, habits, and lifestyles
At GeoPoll we are building a mobile integration platform to poll millions around the world via their own mobile phones.  We do this by integrating with mobile carriers in places like Afghanistan and Congo to target users by location, make messages free, & pay users directly.  This is hard.  We have learned many dos and don'ts which we would like to share.
Organizations of all types and sizes are experiencing an explosion of machine log data whose literally inhuman diversity and scale overwhelms traditional analysis tools and techniques.  We will discuss how machine learning can complement human expertise, enabling the extraction of valuable and actionable insights from log data.
This presentation will introduce Big Data in context of the Industrial Internet, describe some of the unique software and analytics opportunities, and present several current research topics making the Industrial Internet a reality.
Smart meters may be the most visible element of the so-called smart grid, but how smart is it if the plants producing the energy are dumb?  To ensure the integrity of the grid, every stage of our electrical power infrastructure  including generation, transmission and distribution  has to get smart. Sophisticated sensors connected to big data analytics are key to keeping the power flowing.
In this talk we discuss the challenges associated with data center operations management and provide details on how CloudPhysics big data platform solves these problems and enables new capabilities that were previously not possible.
With increased road congestion around the globe and growing amounts of car data we need more intelligent analytical methods to beat the traffic. This talk presents our work on traffic velocity and travel disruption analytics. We describe our approach in detail, how we went from idea to implemented algorithm and how our methods can be applied to gain deep insight into influential factors.
The measure of success for a data scientist is not number of insights, but impact on co-workers' behavior. Moving from insight to action requires an art underutilized by the data science community: storytelling.  I will cover techniques including the Fogg model, loss aversion, and minimum viable stories, using examples of my failures and successes in driving behavioral change with data.
Why have powerful tools if you aren't asking the right questions? Good questions trump shiny tools, but our community has done little to improve how we train people in the "soft side" of data science. We will show how to borrow ideas from design, the humanities, consulting practices to structure problems and improve the questions we ask of our data.
A group of VCs who invest from very early, through later stage investments talk about all things Big Data.  There will be no 3 Vs discussion here.  The Panelists are committed to making this a lively discussion about topics ranging from the typical (what sectors do they want to invest in) to the atypical (whats out there that they dont like?
The present fossil fuel based economy must give way to a renewable energy based future. The Harvard Clean Energy Project set out to discover new molecular materials for the next generation of organic solar cells. In studying 2.3 million (m) compounds with 24m conformers in 150m density functional theory calculations, this Big Data project will benefit mankind aiding the quest for clean energy.
 We optimize ads, but not our mood. We know more about our tweets than our own bodies. That's all about to change. As wearables transform the 'quantified self' from a niche to a mainstream market, they are generating vast amounts of data about our health, habits, and lifestyles
At GeoPoll we are building a mobile integration platform to poll millions around the world via their own mobile phones.  We do this by integrating with mobile carriers in places like Afghanistan and Congo to target users by location, make messages free, & pay users directly.  This is hard.  We have learned many dos and don'ts which we would like to share.
Organizations of all types and sizes are experiencing an explosion of machine log data whose literally inhuman diversity and scale overwhelms traditional analysis tools and techniques.  We will discuss how machine learning can complement human expertise, enabling the extraction of valuable and actionable insights from log data.
This presentation will introduce Big Data in context of the Industrial Internet, describe some of the unique software and analytics opportunities, and present several current research topics making the Industrial Internet a reality.
Smart meters may be the most visible element of the so-called smart grid, but how smart is it if the plants producing the energy are dumb?  To ensure the integrity of the grid, every stage of our electrical power infrastructure  including generation, transmission and distribution  has to get smart. Sophisticated sensors connected to big data analytics are key to keeping the power flowing.
In this talk we discuss the challenges associated with data center operations management and provide details on how CloudPhysics big data platform solves these problems and enables new capabilities that were previously not possible.
With increased road congestion around the globe and growing amounts of car data we need more intelligent analytical methods to beat the traffic. This talk presents our work on traffic velocity and travel disruption analytics. We describe our approach in detail, how we went from idea to implemented algorithm and how our methods can be applied to gain deep insight into influential factors.
3-Hours: What are the essential components of a data platform? This tutorial will explain how the various parts of the Hadoop and big data ecosystem fit together in production to create a data platform supporting batch, interactive and realtime analytical workloads.
90-Minutes: Machine learning is software. As such, it should follow standard software engineering practices,, however, the current tools of the trade are not modular, maintainable or reusable.  In this tutorial we will learn to work with Scalding, a Scala DSL which provides both the simplicity of languages like Apache Pig, and the power of a functional fully JVM language.
90-Minutes: Data analysts routinely report spending more time "wrangling" their data than performing analysis per se. In this tutorial we focus on the ever-present yet oft-overlooked challenges of Data Transformation, including discovery, structure, content and curation. We emphasize recent approaches that jointly emphasize interaction and inference, leveraging both human acuity and...
To seize the future data must be harnessed in actionable time. Based on a real deployment see to achieve instant results with infinite storage - filter large amounts of cold data in Hadoop, analyze in Real-Time with SAP HANA and visualize using SAP Lumira. Learn how solutions from SAP and our Hadoop partners can help your organization seize the future and gain unprecedented insight from Big Data.
The cloud provides an easy onramp to building and deploying Big Data solutions, particularly the latest technologies that favor scale-out architectures. Transitioning from initial deployment to a large-scale, highly performant operation without breaking the bank may not be easy.
 Join Trifacta's founders and their customers to learn how Data Transformation is changing the way people work with data. By increasing data analyst productivity and giving business analysts direct access to Big Data for the first time, Trifacta's technology increases the breadth of data they work with, significantly shortens "time to insight", and enables better business decisions.
The Inflection Point - Hadoop and Big Data Analytics
Application of the Paxos Protocol Towards Building a Continuously Available HBase
This session will address the exciting possibilities of bringing dramatic improvements in various industry verticals using big data analytics especially real-time analytics over high-volume data in motion.
Apache Hive is the de-facto standard for SQL-in-Hadoop today, with more enterprises relying on this open source project than on any alternative. Enterprises have asked for Hive to become more real-time and interactive and the Hive community has responded.
NewSQL has followed quickly on the heels of NoSQL - providing scale-out of NoSQL along with SQL and ACID guarantees. We'll discuss NewSQL with customer examples and contrast it with SQL on Hadoop implementations.
BotNets and cybercrime are by their very nature Big Data problems. The Microsoft Cybercrime Center is working in conjunction with law enforcement, public sector, commercial and academic partners to investigate, disable and prosecute cyber criminals...
The real promise of big data isnt about merely doing analytics cost-effectively and at scale; its about discovery. Data discovery means uncovering hidden patterns from disparate sources without needing to know which questions to ask or the data relationships in advance...
In this session, we will share the results of our study,  a price-performance comparison of a bare-metal Hadoop cluster and cloud-based Hadoop clusters.
3-Hours: What are the essential components of a data platform? This tutorial will explain how the various parts of the Hadoop and big data ecosystem fit together in production to create a data platform supporting batch, interactive and realtime analytical workloads.
90-Minutes: Machine learning is software. As such, it should follow standard software engineering practices,, however, the current tools of the trade are not modular, maintainable or reusable.  In this tutorial we will learn to work with Scalding, a Scala DSL which provides both the simplicity of languages like Apache Pig, and the power of a functional fully JVM language.
90-Minutes: Data analysts routinely report spending more time "wrangling" their data than performing analysis per se. In this tutorial we focus on the ever-present yet oft-overlooked challenges of Data Transformation, including discovery, structure, content and curation. We emphasize recent approaches that jointly emphasize interaction and inference, leveraging both human acuity and...
To seize the future data must be harnessed in actionable time. Based on a real deployment see to achieve instant results with infinite storage - filter large amounts of cold data in Hadoop, analyze in Real-Time with SAP HANA and visualize using SAP Lumira. Learn how solutions from SAP and our Hadoop partners can help your organization seize the future and gain unprecedented insight from Big Data.
The cloud provides an easy onramp to building and deploying Big Data solutions, particularly the latest technologies that favor scale-out architectures. Transitioning from initial deployment to a large-scale, highly performant operation without breaking the bank may not be easy.
 Join Trifacta's founders and their customers to learn how Data Transformation is changing the way people work with data. By increasing data analyst productivity and giving business analysts direct access to Big Data for the first time, Trifacta's technology increases the breadth of data they work with, significantly shortens "time to insight", and enables better business decisions.
The Inflection Point - Hadoop and Big Data Analytics
Application of the Paxos Protocol Towards Building a Continuously Available HBase
This session will address the exciting possibilities of bringing dramatic improvements in various industry verticals using big data analytics especially real-time analytics over high-volume data in motion.
Apache Hive is the de-facto standard for SQL-in-Hadoop today, with more enterprises relying on this open source project than on any alternative. Enterprises have asked for Hive to become more real-time and interactive and the Hive community has responded.
NewSQL has followed quickly on the heels of NoSQL - providing scale-out of NoSQL along with SQL and ACID guarantees. We'll discuss NewSQL with customer examples and contrast it with SQL on Hadoop implementations.
BotNets and cybercrime are by their very nature Big Data problems. The Microsoft Cybercrime Center is working in conjunction with law enforcement, public sector, commercial and academic partners to investigate, disable and prosecute cyber criminals...
The real promise of big data isnt about merely doing analytics cost-effectively and at scale; its about discovery. Data discovery means uncovering hidden patterns from disparate sources without needing to know which questions to ask or the data relationships in advance...
In this session, we will share the results of our study,  a price-performance comparison of a bare-metal Hadoop cluster and cloud-based Hadoop clusters.
3-Hours: Hands on introductory workshop on Predictive Modelling and Machine Learning with open source tools from the Python community such as scikit-learn and IPython.
3-Hours: This tutorial will provide an introduction to modern machine learning. Attendees will learn how to leverage some of the most popular techniques used in fraud detection, social network analysis, and personalized recommendation services.
In this talk, we'll explore how Apache Hadoop has rapidly evolved to become the new foundation for enterprise analytics - the enterprise data hub - and learn about the state-of-the-art in deploying a modern data warehouse on top of the Hadoop stack.
Attend this session to learn how you can take advantage of the new economics of data. This session will present examples of how leading organizations are evolving their enterprise data architectures to bring together the Data Warehouse, Hadoop & Data Discovery Platforms so All Users can benefit from ALL Analytics on ALL Data.
Most data centers are filled with rigid data servers that are tightly linked to specific applications, leading to data duplication, lengthy development cycles, and unnecessary costs. Learn how you can use the MarkLogic Enterprise NoSQL database platform to help create a flexible, agile data fabric that will allow you to iterate your application development, optimize your data, and reduce costs.
PostgreSQL is an advanced open source database known for its reliability. It also features a rich extension ecosystem that enables features like semi-structured data types, new SQL operators, and a columnar data store. This talk examines extensions available to PostgreSQL users and how CitusDB turns PostgreSQL into a scalable data platform for addressing real world analytics problems.
Search Engine Marketing is an important revenue opportunity for Ask.com, planed to nearly double in 2014. Fueled by growth and acquisitions such as About.com and Investopedia, the keyword portfolio will grow by 90x through 2014. SEM Analytics at Ask.com involves tens of millions of cost metrics stored daily, hundreds of millions of portfolio keywords, and billions of historical costs.
This presentation discusses how we used complex event processing (CEP) and MapReduce based technologies to track and process data from a soccer match as part of the annual DEBS event processing challenge while achieving throughput in excess of 100,000 events/sec.
Organizations are now moving beyond rigid and high latency data warehouse environments to more flexible and cost-effective "Data Lake(s)":  centrally managed repository using low cost technologies such as Hadoop, SQL, In-Memory, and others to land any and all data that might potentially be valuable for analysis and operationalizing that insight.
edo Interactive shares how they drive agile, improved decision-making by complementing native Hadoop technologies with analytical databases and ETL optimization and data visualization solutions from vendors such as Pentaho.
In the world of ever growing data volumes, how do you extract insight, trends and meaning from all that data in Hadoop? Getting relevant information in seconds (instead of hours or days) from big data requires a different approach.  Join Paul Kent and Wayne Thompson from SAS as they share how to reveal insights in your Big data and redefine how your organization solves complex problems.
Join Paxatas Nenshad Bardoliwalla for a look at the new breed of data preparation tools that use semantic algorithms to detect data types, apply machine learning to find hidden patterns, and link related columns of data automatically.
We will discuss Rackspaces vision for Data-as-a-Service, and provide a few key questions that could help you complement your technical analysis when choosing a database service. Along the way, we will also discuss parts of the portfolio of data services available at Rackspace, including SQL, MongoDB, Redis and Hadoop-based solutions.
3-Hours: Hands on introductory workshop on Predictive Modelling and Machine Learning with open source tools from the Python community such as scikit-learn and IPython.
3-Hours: This tutorial will provide an introduction to modern machine learning. Attendees will learn how to leverage some of the most popular techniques used in fraud detection, social network analysis, and personalized recommendation services.
In this talk, we'll explore how Apache Hadoop has rapidly evolved to become the new foundation for enterprise analytics - the enterprise data hub - and learn about the state-of-the-art in deploying a modern data warehouse on top of the Hadoop stack.
Attend this session to learn how you can take advantage of the new economics of data. This session will present examples of how leading organizations are evolving their enterprise data architectures to bring together the Data Warehouse, Hadoop & Data Discovery Platforms so All Users can benefit from ALL Analytics on ALL Data.
Most data centers are filled with rigid data servers that are tightly linked to specific applications, leading to data duplication, lengthy development cycles, and unnecessary costs. Learn how you can use the MarkLogic Enterprise NoSQL database platform to help create a flexible, agile data fabric that will allow you to iterate your application development, optimize your data, and reduce costs.
PostgreSQL is an advanced open source database known for its reliability. It also features a rich extension ecosystem that enables features like semi-structured data types, new SQL operators, and a columnar data store. This talk examines extensions available to PostgreSQL users and how CitusDB turns PostgreSQL into a scalable data platform for addressing real world analytics problems.
Search Engine Marketing is an important revenue opportunity for Ask.com, planed to nearly double in 2014. Fueled by growth and acquisitions such as About.com and Investopedia, the keyword portfolio will grow by 90x through 2014. SEM Analytics at Ask.com involves tens of millions of cost metrics stored daily, hundreds of millions of portfolio keywords, and billions of historical costs.
This presentation discusses how we used complex event processing (CEP) and MapReduce based technologies to track and process data from a soccer match as part of the annual DEBS event processing challenge while achieving throughput in excess of 100,000 events/sec.
Organizations are now moving beyond rigid and high latency data warehouse environments to more flexible and cost-effective "Data Lake(s)":  centrally managed repository using low cost technologies such as Hadoop, SQL, In-Memory, and others to land any and all data that might potentially be valuable for analysis and operationalizing that insight.
edo Interactive shares how they drive agile, improved decision-making by complementing native Hadoop technologies with analytical databases and ETL optimization and data visualization solutions from vendors such as Pentaho.
In the world of ever growing data volumes, how do you extract insight, trends and meaning from all that data in Hadoop? Getting relevant information in seconds (instead of hours or days) from big data requires a different approach.  Join Paul Kent and Wayne Thompson from SAS as they share how to reveal insights in your Big data and redefine how your organization solves complex problems.
Join Paxatas Nenshad Bardoliwalla for a look at the new breed of data preparation tools that use semantic algorithms to detect data types, apply machine learning to find hidden patterns, and link related columns of data automatically.
We will discuss Rackspaces vision for Data-as-a-Service, and provide a few key questions that could help you complement your technical analysis when choosing a database service. Along the way, we will also discuss parts of the portfolio of data services available at Rackspace, including SQL, MongoDB, Redis and Hadoop-based solutions.
This workshop provides a detailed discussion of the new features of Apache Hadoop 2.0. We will discuss how YARN turns Hadoop from a single use system for batch data processing into a multi-use platform for storing and processing data in many ways other than batch. We will also discuss the details of the new HDFS improvements like High Availability, Federation, and Snapshots.
3-Hours: Apache HBase is a distributed, column-oriented, key-value store for Apache Hadoop (via integration with HDFS). In this tutorial, you will learn the basic elements of building a real-time application that uses Apache HBase as a persistent data store.
Mike Gualtieri, principal analyst at Forrester Research, Inc., will facilitate a panel of production Hadoop users  including Cisco, The Climate Corporation, The Rubicon Project, and Solutionary  to discuss the challenges and best practices for deploying Hadoop in production. Join us for an engaging conversation on tips and tricks in deploying Hadoop in production.
Join us as we discuss the real-world applications of big data, examine what's working and what isn't, and discuss why you don't need to boil the big data ocean with Hadoop.
In 2012, Evernote took proactive steps to prepare for a rapidly expanding customer base by making the transition from 18-hour queries on a MySQL server to ad hoc analytics for 200 million daily eventswhile on a budget. This session explains how Evernote is scaling to hundreds of terabytes and analyzes 200 million events per day using two-tier architecture including Hadoop and analytic platform.
In this panel discussion, well hear from entertainment, healthcare, and media industry leaders as they discuss their strategy to demystify analytics end to end.  Well have a question and answer session moderated by Alpine Data Labs.
Forget the 140 characters, Twitter is Big Data. Every day sees around 100TBs of data ingested and tens of thousands of Hadoop jobs. Join us to hear how Twitter is using HPs HAVEn platform to run their Big Data analytics.  Learn why theyve integrated HP Vertica with their Hadoop infrastructure to deliver the scale and speed needed for their analytics.
We will discuss the strategic significance of infrastructure core services (compute, storage, network, and comprehensive security) required for robust big data solutions. Also the strategic significance of Hadoop 2.0, Hadoop/NoSQL convergence, and the critical need for effective modeling, query formulation, and data analysis capabilities as Hadoop becomes an enterprise platform for big data.
In this session, I will illustrate these architectures with real-world examples of city governments, retail banks, food manufacturers, pharmaceutical companies, and Intel itself applying intelligence wherever data lives.
How Comcast Turns Big Data into Real-Time Operational Insights
In this session, MemSQL CEO Eric Frenkiel will discuss the benefits for companies that augment their existing information architecture with a versatile real-time database platform to handle high volume and velocity transactional and analytical workloads.
Learn from the Amazon Elastic MapReduce team's recent experience with streaming services such as Amazon Kinesis and low-latency query engines like Impala and Phoenix. We'll clarify many of the implementation details of our Hadoop InputFormat for Amazon Kinesis and demonstrate the power and flexibility of applying existing Hadoop ecosystem technologies to the real-time data paradigm.
Big Data is really a small data mindset. At the enterprise-level, where the potential for data collection is greatest, companies are still stuck compartmentalizing data. TIBCO CTO Matt Quinn will share how the worlds leading sports teams, airlines, banks and retailers are those that change their Big Data mindset to an All Data one.
This workshop provides a detailed discussion of the new features of Apache Hadoop 2.0. We will discuss how YARN turns Hadoop from a single use system for batch data processing into a multi-use platform for storing and processing data in many ways other than batch. We will also discuss the details of the new HDFS improvements like High Availability, Federation, and Snapshots.
3-Hours: Apache HBase is a distributed, column-oriented, key-value store for Apache Hadoop (via integration with HDFS). In this tutorial, you will learn the basic elements of building a real-time application that uses Apache HBase as a persistent data store.
Mike Gualtieri, principal analyst at Forrester Research, Inc., will facilitate a panel of production Hadoop users  including Cisco, The Climate Corporation, The Rubicon Project, and Solutionary  to discuss the challenges and best practices for deploying Hadoop in production. Join us for an engaging conversation on tips and tricks in deploying Hadoop in production.
Join us as we discuss the real-world applications of big data, examine what's working and what isn't, and discuss why you don't need to boil the big data ocean with Hadoop.
In 2012, Evernote took proactive steps to prepare for a rapidly expanding customer base by making the transition from 18-hour queries on a MySQL server to ad hoc analytics for 200 million daily eventswhile on a budget. This session explains how Evernote is scaling to hundreds of terabytes and analyzes 200 million events per day using two-tier architecture including Hadoop and analytic platform.
In this panel discussion, well hear from entertainment, healthcare, and media industry leaders as they discuss their strategy to demystify analytics end to end.  Well have a question and answer session moderated by Alpine Data Labs.
Forget the 140 characters, Twitter is Big Data. Every day sees around 100TBs of data ingested and tens of thousands of Hadoop jobs. Join us to hear how Twitter is using HPs HAVEn platform to run their Big Data analytics.  Learn why theyve integrated HP Vertica with their Hadoop infrastructure to deliver the scale and speed needed for their analytics.
We will discuss the strategic significance of infrastructure core services (compute, storage, network, and comprehensive security) required for robust big data solutions. Also the strategic significance of Hadoop 2.0, Hadoop/NoSQL convergence, and the critical need for effective modeling, query formulation, and data analysis capabilities as Hadoop becomes an enterprise platform for big data.
In this session, I will illustrate these architectures with real-world examples of city governments, retail banks, food manufacturers, pharmaceutical companies, and Intel itself applying intelligence wherever data lives.
How Comcast Turns Big Data into Real-Time Operational Insights
In this session, MemSQL CEO Eric Frenkiel will discuss the benefits for companies that augment their existing information architecture with a versatile real-time database platform to handle high volume and velocity transactional and analytical workloads.
Learn from the Amazon Elastic MapReduce team's recent experience with streaming services such as Amazon Kinesis and low-latency query engines like Impala and Phoenix. We'll clarify many of the implementation details of our Hadoop InputFormat for Amazon Kinesis and demonstrate the power and flexibility of applying existing Hadoop ecosystem technologies to the real-time data paradigm.
Big Data is really a small data mindset. At the enterprise-level, where the potential for data collection is greatest, companies are still stuck compartmentalizing data. TIBCO CTO Matt Quinn will share how the worlds leading sports teams, airlines, banks and retailers are those that change their Big Data mindset to an All Data one.
Strata Program Chairs, Roger Magoulas and Alistair Croll, welcome you to the first day of keynotes.
Crossing the Chasm has been a key reference point for high-tech marketing since its publication in 1990, but a lot has changed since then, especially with the rise of cloud computing, software as a service, mobile endpoints, big data analytics, and viral marketing.
In this talk Dr. Amr Awadallah will present the Enterprise Data Hub (EDH) as the new foundation for the modern information architecture.  Built with Apache Hadoop at the core, the EDH is an extremely scalable, flexible, and fault-tolerant, data processing system designed to put data at the center of your business.
Crowdsourcing can be an effective way to collect massive amounts of data to enable deeper analysis in many situations. Explore the foundational steps that can lead to successfully crowd sourcing data though the lenses of the International Barcode of Life and Technical University Munich (TUM) ProteomicsDB projects. SAP is proud to be involved with driving the success of both these projects.
Humans are constantly curious and learning should be about making new discoveries.  With big data, we have the potential to take formal learning which is taught and combine it with informal learning which is experienced, to create personalized learning paths for every individual.
This five-minute keynote will provide a quick overview of some of the more surprising things Hadoop is capable of in 5 minutes or less.
We feel safer in big numbers, and we believe that numbers don't lie. But numbers don't actually speak for themselves - people speak for them.
How does the world change when big data reaches a billion people? What happens when anyone, from farmers to criminal investigators, gains the power to quickly derive meaningful insights from vast and varied data sources? Join Quentin Clark, Microsoft Corporate Vice President, who will highlight how simple, familiar tools and cutting-edge cloud technologies are bringing big data to all.
The gap between legendary and anonymity in sports is often less than a 1% performance difference in elite sports. Thus, finding the core, modifiable variables that determine performance and tweaking them ever so slightly can alchemize silver medals into gold ones.
The better we tune our practice, the more practice will make perfect.
Strata Program Chairs, Alistair Croll and Roger Magoulas, welcome you to the second day of keynotes.
If Big Data is the grand challenge of our time, most analytic effort is like ground control: the hard work behind the scenes that enables ambitious analysis to occur.
The emerging Internet Of Things (IOT) enables us to build smart systems.  We already have the sensory and motor parts of these systems available, but we don't have the brain.  This is where data science comes into the picture! I will talk about how we are using big data technologies in conjunction with data science here at Pivotal to build the digital brain that makes a system smart.
At Intel, we envision a future in which every organization in the world can use new sources of data to enhance its operational intelligence, fostering discoveries and innovation in science, industry, and medicine.
While the first big data systems made a new class of applications possible, organizations must now compete on the speed and sophistication with which they can draw value from data. Future data processing platforms will need to not just scale cost-effectively; but to allow ever more real-time analysis, and to support both simple queries and today's most sophisticated analytics algorithms.
Big Data without analytics is just data, but how do you perform the analytics? In this session, learn how In-Hadoop analytics is changing the game for the possibilities of Hadoop.
How do we know how many people have been killed in Syria?  If violence is escalating or decreasing?  The hard answer is we don't.  But through careful application of machine learning and other statistical techniques, we can quantify what we do, and don't, know.
Ben Fry, Principal, Fathom
David McRaney will tell the story of how the Department of War Math in World War II helped bring to light the psychology of how we miss what is important when it comes to failure, and how the modern understanding of the psychology of luck provides the best game plan for getting the best out of life.
Strata Program Chairs, Roger Magoulas and Alistair Croll, welcome you to the first day of keynotes.
Crossing the Chasm has been a key reference point for high-tech marketing since its publication in 1990, but a lot has changed since then, especially with the rise of cloud computing, software as a service, mobile endpoints, big data analytics, and viral marketing.
In this talk Dr. Amr Awadallah will present the Enterprise Data Hub (EDH) as the new foundation for the modern information architecture.  Built with Apache Hadoop at the core, the EDH is an extremely scalable, flexible, and fault-tolerant, data processing system designed to put data at the center of your business.
Crowdsourcing can be an effective way to collect massive amounts of data to enable deeper analysis in many situations. Explore the foundational steps that can lead to successfully crowd sourcing data though the lenses of the International Barcode of Life and Technical University Munich (TUM) ProteomicsDB projects. SAP is proud to be involved with driving the success of both these projects.
Humans are constantly curious and learning should be about making new discoveries.  With big data, we have the potential to take formal learning which is taught and combine it with informal learning which is experienced, to create personalized learning paths for every individual.
This five-minute keynote will provide a quick overview of some of the more surprising things Hadoop is capable of in 5 minutes or less.
We feel safer in big numbers, and we believe that numbers don't lie. But numbers don't actually speak for themselves - people speak for them.
How does the world change when big data reaches a billion people? What happens when anyone, from farmers to criminal investigators, gains the power to quickly derive meaningful insights from vast and varied data sources? Join Quentin Clark, Microsoft Corporate Vice President, who will highlight how simple, familiar tools and cutting-edge cloud technologies are bringing big data to all.
The gap between legendary and anonymity in sports is often less than a 1% performance difference in elite sports. Thus, finding the core, modifiable variables that determine performance and tweaking them ever so slightly can alchemize silver medals into gold ones.
The better we tune our practice, the more practice will make perfect.
Strata Program Chairs, Alistair Croll and Roger Magoulas, welcome you to the second day of keynotes.
If Big Data is the grand challenge of our time, most analytic effort is like ground control: the hard work behind the scenes that enables ambitious analysis to occur.
The emerging Internet Of Things (IOT) enables us to build smart systems.  We already have the sensory and motor parts of these systems available, but we don't have the brain.  This is where data science comes into the picture! I will talk about how we are using big data technologies in conjunction with data science here at Pivotal to build the digital brain that makes a system smart.
At Intel, we envision a future in which every organization in the world can use new sources of data to enhance its operational intelligence, fostering discoveries and innovation in science, industry, and medicine.
While the first big data systems made a new class of applications possible, organizations must now compete on the speed and sophistication with which they can draw value from data. Future data processing platforms will need to not just scale cost-effectively; but to allow ever more real-time analysis, and to support both simple queries and today's most sophisticated analytics algorithms.
Big Data without analytics is just data, but how do you perform the analytics? In this session, learn how In-Hadoop analytics is changing the game for the possibilities of Hadoop.
How do we know how many people have been killed in Syria?  If violence is escalating or decreasing?  The hard answer is we don't.  But through careful application of machine learning and other statistical techniques, we can quantify what we do, and don't, know.
Ben Fry, Principal, Fathom
David McRaney will tell the story of how the Department of War Math in World War II helped bring to light the psychology of how we miss what is important when it comes to failure, and how the modern understanding of the psychology of luck provides the best game plan for getting the best out of life.
In this tutorial we'll use the Cloudera Development Kit (CDK) to build a Java web app that logs application events to Hadoop, and then run ad hoc and scheduled queries against the collected data.
This tutorial will describe how to process real-time streams and using the open-source Storm framework. We will define Storm's core concepts whilst focusing on creating a simple topology that counts, in real-time, key-words and hashtags seen in Twitter's public (1%) feed.
In January Facebook launched Graph Search in the US which allows users to search their social graph. Ian Hegerty will describe how the Graph Search corpus was built from Facebook's entity graph, and how big data is used to understand users queries and provide relevant results, with minimal initial user behavioral data.
How Stuff Spreads looks at how two recent memes spread online: Gangnam Style vs Harlem Shake. The talk dissects the memes through the lens of big data to show what made them go viral, what do they have in common, how quantitative and qualitative analysis have to come together to craft insights and tell a story, and finally how to predict future memes and create a data-driven content strategy.
How do we know what we know? Increasingly discoveries are made from computed data, possibly sourced from the internet. If we are to trust these discoveries, how conclusions are reached is critical. Examples from work in Big Data analytics infrastructure for life sciences and social media analysis will illustrate the key issues.
Medical treatments have have come a long away in the last couple of decades.  On the other hand, we could be doing a lot better in monitoring people within their own homes between hospital visits using sensors.  Sensors combined with Big Data technologies are set to bring about profound changes for the future of health and social care.
Everyday objects are becoming smarter. In ten years time, every piece of clothing you own, every piece of jewelry, and every thing you carry with you will be measuring, weighing and calculating your life. In ten years, the world  your world  will be full of sensors.
In the future we will see huge growth in the amount of traffic data generated through built-in car sensors. This talk presents a case study of analytics on traffic and traffic light data. Methods will be presented that yield a deep understanding of traffic and its characteristics by analyzing past traffic data. These methods could be extended to predict traffic jams and optimize routing systems.
To keep analyzing more data, and faster, we need a secret weapon: cheating. In this brief survey, learn how you may be doing too much work in your analytics and learning processes, and how giving up a little accuracy can gain a lot of performance. With examples from Apache Hadoop, Mahout, and ML tools from Cloudera.
Analytics is useless if it doesn't lead to action. It is often desirable to put a computer in control of decision making. In this talk I'll discuss bandit algorithms, a class of decision making algorithms that solve a simple but widely applicable decision problem, and have found application in ad serving, content recommendation, and more.
Even if one has big data, sometimes there is a lack of key data. This is a problem for predictive analytics: if there is only a limited amount of training material (e.g. user ratings, categorized documents), then it is hard to generate accurate models. The talk introduces new semi-supervised learning methods to overcome this problem by utilizing the vast amount of unlabeled data.
As data scientists, uncertainty is all around us: data is noisy, missing, wrong or inherently uncertain. In this talk I want to introduce a branch of statistics called Bayesian reasoning which is a unifying, consistent, logical and practically successful way of handling uncertainty. In short, I'd like to convince people that Bayes rule is the E=MC^2 of data science.
How do you indentify duplicate data and why is it important? What do you do with such data when you find it? Data Matching using the mathematics of probability has been around since the 1950s. But, how does it actually work? What is the mathematics behind it? How do probabilities allow us to identify duplicate entries?
This talk will discuss how particle physics research can inform the field of data science. The importance of blind analyses and machine learning algorithms will be discussed as tools for filtering growing bodies of data as the big data trend continues.
In this tutorial we'll use the Cloudera Development Kit (CDK) to build a Java web app that logs application events to Hadoop, and then run ad hoc and scheduled queries against the collected data.
This tutorial will describe how to process real-time streams and using the open-source Storm framework. We will define Storm's core concepts whilst focusing on creating a simple topology that counts, in real-time, key-words and hashtags seen in Twitter's public (1%) feed.
In January Facebook launched Graph Search in the US which allows users to search their social graph. Ian Hegerty will describe how the Graph Search corpus was built from Facebook's entity graph, and how big data is used to understand users queries and provide relevant results, with minimal initial user behavioral data.
How Stuff Spreads looks at how two recent memes spread online: Gangnam Style vs Harlem Shake. The talk dissects the memes through the lens of big data to show what made them go viral, what do they have in common, how quantitative and qualitative analysis have to come together to craft insights and tell a story, and finally how to predict future memes and create a data-driven content strategy.
How do we know what we know? Increasingly discoveries are made from computed data, possibly sourced from the internet. If we are to trust these discoveries, how conclusions are reached is critical. Examples from work in Big Data analytics infrastructure for life sciences and social media analysis will illustrate the key issues.
Medical treatments have have come a long away in the last couple of decades.  On the other hand, we could be doing a lot better in monitoring people within their own homes between hospital visits using sensors.  Sensors combined with Big Data technologies are set to bring about profound changes for the future of health and social care.
Everyday objects are becoming smarter. In ten years time, every piece of clothing you own, every piece of jewelry, and every thing you carry with you will be measuring, weighing and calculating your life. In ten years, the world  your world  will be full of sensors.
In the future we will see huge growth in the amount of traffic data generated through built-in car sensors. This talk presents a case study of analytics on traffic and traffic light data. Methods will be presented that yield a deep understanding of traffic and its characteristics by analyzing past traffic data. These methods could be extended to predict traffic jams and optimize routing systems.
To keep analyzing more data, and faster, we need a secret weapon: cheating. In this brief survey, learn how you may be doing too much work in your analytics and learning processes, and how giving up a little accuracy can gain a lot of performance. With examples from Apache Hadoop, Mahout, and ML tools from Cloudera.
Analytics is useless if it doesn't lead to action. It is often desirable to put a computer in control of decision making. In this talk I'll discuss bandit algorithms, a class of decision making algorithms that solve a simple but widely applicable decision problem, and have found application in ad serving, content recommendation, and more.
Even if one has big data, sometimes there is a lack of key data. This is a problem for predictive analytics: if there is only a limited amount of training material (e.g. user ratings, categorized documents), then it is hard to generate accurate models. The talk introduces new semi-supervised learning methods to overcome this problem by utilizing the vast amount of unlabeled data.
As data scientists, uncertainty is all around us: data is noisy, missing, wrong or inherently uncertain. In this talk I want to introduce a branch of statistics called Bayesian reasoning which is a unifying, consistent, logical and practically successful way of handling uncertainty. In short, I'd like to convince people that Bayes rule is the E=MC^2 of data science.
How do you indentify duplicate data and why is it important? What do you do with such data when you find it? Data Matching using the mathematics of probability has been around since the 1950s. But, how does it actually work? What is the mathematics behind it? How do probabilities allow us to identify duplicate entries?
This talk will discuss how particle physics research can inform the field of data science. The importance of blind analyses and machine learning algorithms will be discussed as tools for filtering growing bodies of data as the big data trend continues.
The tutorial will give a first introduction running Big Data Analyses in the statistical software R. R brings together latest Big Data technologies and latest high-level statistical methods. Bring your laptop, use your web browser to access a RStudio based analyses platform in the cloud and leave with a lot of new ideas for efficient Big Data analyses with R.
Communicating Data Clearly describes how to draw clear, concise, accurate graphs that are easier to understand than many of the graphs one sees today.  The tutorial emphasizes how to avoid common mistakes that produce confusing or even misleading graphs. Graphs for one, two, three, and many variables are covered as well as general principles for creating effective graphs.
As big data analytics evolves beyond simple batch jobs, there is a need for both lower-latency processing (interactive queries and steam processing) and more complex analytics (e.g. machine learning, graph algorithms). This talk will introduce Spark and Shark, popular open source projects from Berkeley that address this need through an optimized runtime engine and in-memory computing capabilities.
In May 2013, the O'Reilly Data Sensing Lab collaborated with the Google Cloud Platform and Device Cloud by Etherios, to deploy a network of hundreds of environmental sensors at Google I/O. Learn how the Google Cloud Platform was used to build an end-to-end, scaleable, and high-throughput pipeline for data collection, processing, and analysis.
It has been said by many that 80% data science is scrubbing data. In this talk we'll cover how you can use Cascalog to scrub, transform, manipulate and mangle data into the formats you need, fix things that are wrong and filter out things that are broken. Clojure and Cascalog together provide fantastic tools for this. Learn about using Hadoop with the messy data that exists in the real world.
Fast read and write performance and scalability of distributed in-memory clusters is making it possible to retrain machine learning algorithms in real-time.  The application of such algorithms to risk, infrastructure security and other areas can be transformative.
"In order to classify documents, simply first convert them to vectors, train, test and finally apply the model." Sounds easy - in theory. Converting documents to vectors usually is the tricky part. This talk walks you through the steps necessary to convert your text documents into feature vectors that Mahout classifiers can use including a few anecdotes on drafting domain specific features.
The value of machine-generated data is highest at the moment it is generated. Operational data, sensor data and video feeds require new automated approaches to maximize the value of these streams of data.  In this session, Dr. Volkmar Uhlig will explore how to apply the lessons learned from automated high-frequency trading systems to todays Big Data problems to monetize information.
To feed LinkedIn's data-driven products, we need to run a complex graph of ETL workflows that deliver the right data to the right systems reliably on a 24x7 basis.  To achieve this goal, we have developed a metadata system that captures process dependencies, data dependencies, and execution histories -- this system also lays the foundation for a combined dataflow and workflow engine.
Apache Hadoop has become popular from its specialization in the execution of MapReduce programs. However,  it has been hard to leverage existing Hadoop infrastructure for various other processing paradigms such as real-time streaming, graph processing and message-passing. That was true until the introduction of Apache Hadoop YARN in Apache Hadoop 2.0.
Predictive Analytics has emerged as one of the primary use cases for Hadoop, leveraging various Machine Learning techniques to increase revenue or reduce costs. In this talk we provide real-world use cases from several different industries, and then discuss the open source technologies available to companies wishing to implement Predictive Analytics with Hadoop.
People want more out of Hive.  They want it to be fast, useful, and connect to their tools.  Work is being done to reduce start up time, improve the optimizer, extend it to use Tez, process records 50x faster, add support for functions like RANK, add subqueries, and add standard SQL datatypes.  We will review this work plus show current benchmarks.
NICE Systems is a leading provider of Customer Experience Management software, providing real-time offer management and predictive analytics applications based on HBase. We have recently migrated to HBase from our own custom-built data store, and in this session we will share the challenges we overcame getting HBase to perform to our demanding performance requirements.
What questions would you ask if you have a Facebook-like graph of what your customer likes, what they bought, and what they viewed? This is what we built at uSwitch by transforming flat data from Hadoop into Neo4J. This talk will walk through how we bridged big data and linked data technologies and the results of such amalgamation.
The tutorial will give a first introduction running Big Data Analyses in the statistical software R. R brings together latest Big Data technologies and latest high-level statistical methods. Bring your laptop, use your web browser to access a RStudio based analyses platform in the cloud and leave with a lot of new ideas for efficient Big Data analyses with R.
Communicating Data Clearly describes how to draw clear, concise, accurate graphs that are easier to understand than many of the graphs one sees today.  The tutorial emphasizes how to avoid common mistakes that produce confusing or even misleading graphs. Graphs for one, two, three, and many variables are covered as well as general principles for creating effective graphs.
As big data analytics evolves beyond simple batch jobs, there is a need for both lower-latency processing (interactive queries and steam processing) and more complex analytics (e.g. machine learning, graph algorithms). This talk will introduce Spark and Shark, popular open source projects from Berkeley that address this need through an optimized runtime engine and in-memory computing capabilities.
In May 2013, the O'Reilly Data Sensing Lab collaborated with the Google Cloud Platform and Device Cloud by Etherios, to deploy a network of hundreds of environmental sensors at Google I/O. Learn how the Google Cloud Platform was used to build an end-to-end, scaleable, and high-throughput pipeline for data collection, processing, and analysis.
It has been said by many that 80% data science is scrubbing data. In this talk we'll cover how you can use Cascalog to scrub, transform, manipulate and mangle data into the formats you need, fix things that are wrong and filter out things that are broken. Clojure and Cascalog together provide fantastic tools for this. Learn about using Hadoop with the messy data that exists in the real world.
Fast read and write performance and scalability of distributed in-memory clusters is making it possible to retrain machine learning algorithms in real-time.  The application of such algorithms to risk, infrastructure security and other areas can be transformative.
"In order to classify documents, simply first convert them to vectors, train, test and finally apply the model." Sounds easy - in theory. Converting documents to vectors usually is the tricky part. This talk walks you through the steps necessary to convert your text documents into feature vectors that Mahout classifiers can use including a few anecdotes on drafting domain specific features.
The value of machine-generated data is highest at the moment it is generated. Operational data, sensor data and video feeds require new automated approaches to maximize the value of these streams of data.  In this session, Dr. Volkmar Uhlig will explore how to apply the lessons learned from automated high-frequency trading systems to todays Big Data problems to monetize information.
To feed LinkedIn's data-driven products, we need to run a complex graph of ETL workflows that deliver the right data to the right systems reliably on a 24x7 basis.  To achieve this goal, we have developed a metadata system that captures process dependencies, data dependencies, and execution histories -- this system also lays the foundation for a combined dataflow and workflow engine.
Apache Hadoop has become popular from its specialization in the execution of MapReduce programs. However,  it has been hard to leverage existing Hadoop infrastructure for various other processing paradigms such as real-time streaming, graph processing and message-passing. That was true until the introduction of Apache Hadoop YARN in Apache Hadoop 2.0.
Predictive Analytics has emerged as one of the primary use cases for Hadoop, leveraging various Machine Learning techniques to increase revenue or reduce costs. In this talk we provide real-world use cases from several different industries, and then discuss the open source technologies available to companies wishing to implement Predictive Analytics with Hadoop.
People want more out of Hive.  They want it to be fast, useful, and connect to their tools.  Work is being done to reduce start up time, improve the optimizer, extend it to use Tez, process records 50x faster, add support for functions like RANK, add subqueries, and add standard SQL datatypes.  We will review this work plus show current benchmarks.
NICE Systems is a leading provider of Customer Experience Management software, providing real-time offer management and predictive analytics applications based on HBase. We have recently migrated to HBase from our own custom-built data store, and in this session we will share the challenges we overcame getting HBase to perform to our demanding performance requirements.
What questions would you ask if you have a Facebook-like graph of what your customer likes, what they bought, and what they viewed? This is what we built at uSwitch by transforming flat data from Hadoop into Neo4J. This talk will walk through how we bridged big data and linked data technologies and the results of such amalgamation.
We're getting better all the time. See how the Cato Institute used responsive design and D3.js to show how human development indicators improve as economic freedom spreads.
Maps are powerful tools for people to learn from data. In this project, we combine large-scale data processing with Hadoop and data visualization through CartoDB to make over six years of bi-monthly deforestation data accessible in an interactive map on the web. This talk will tell the story of how large-scale data paired with visualization can make data accessible in important new ways.
Many big data solutions focus on large data analysis that happens in data centers. Or they focus on data visualization in the browser. When you combine both of these techniques, you get amazing and expressive power. This talk will show how to use the Google Maps API with WebGL and Google Big Query, Cloud Storage, App Engine and Compute Engine to deliver amazing, responsive visualizations.
How do you do data journalism when you are not the Guardian, the New York Times or the Washington Post? You don't need a data team, developers, much time or any funding to get started and produce data journalism that grabs headlines and engages readers. This workshop will focus on quick start techniques for getting started and making the most of few resources.
In Mexico open data journalism is still difficult to exercise. Find In Mexico, an organization to which I belong, we developed a research project in which we use databases and the result was a map (www.mexicoinformate.org/platform) that has managed to draw attention to the slow progress of the law enforcement system in the country.
Since it launched just 2 years ago, it has leveraged the open data community to grow to by far the largest open database of companies in the world with over 50 million companies in 70 jurisdictions, and is regularly used by journalists, anti-corruption investigators, civil society, even banks and financial institutions.
This session will cover the rapidly changing way that machines are taking on different parts of our lives, making decisions for us and altering our lives with our own data. Shelley Evenson will address how designers need to keep their human focus in order to truly capitalise on the benefits of big data without allowing technology to take over.
Being good is hard. Being evil is much more fun and gets you paid a lot more. We give a survey of the field of doing high-impact evil with data and analysis. We will look at some of the simplest things you can do to make the maximum (negative) impact on your friends, your business and the world. If you happen to learn something about doing good with data that will be your problem.
Analytics best practices, data feeds and flows between tools and continents are put in parallel with legislation, showing which steps to undertake for legal compliance; how to train for data protection & assure minimal liability. Its not about security, goes beyond the cookie debate, highlighting how the EU Personal Data Protection Regulation will influence analytics & how Privacy by Design helps
To take the right decision, you need the right data. As complexity and abundance of data increase, the communication of data analysis results becomes more challenging. Grounding our talk in the pharma R&D arena, we illustrate how animated and interactive graphics can streamline communication on complex data analysis and inform decision making.
The UK Government team behind the GOV.UK website talk about their work on the Performance Platform, a suite of services and a cultural shift taking people away from immensely detailed value stream maps about a call-centre and paper process (which might be an inherently 5-day long journey), to something that's digital, lightweight, fast and pleasant to use.
Big Data are very interesting for official statistics. Results obtained by analyzing large amounts of Dutch traffic loop detection records, Mobile phone data and Dutch social media messages are discussed to illustrate this.
We're getting better all the time. See how the Cato Institute used responsive design and D3.js to show how human development indicators improve as economic freedom spreads.
Maps are powerful tools for people to learn from data. In this project, we combine large-scale data processing with Hadoop and data visualization through CartoDB to make over six years of bi-monthly deforestation data accessible in an interactive map on the web. This talk will tell the story of how large-scale data paired with visualization can make data accessible in important new ways.
Many big data solutions focus on large data analysis that happens in data centers. Or they focus on data visualization in the browser. When you combine both of these techniques, you get amazing and expressive power. This talk will show how to use the Google Maps API with WebGL and Google Big Query, Cloud Storage, App Engine and Compute Engine to deliver amazing, responsive visualizations.
How do you do data journalism when you are not the Guardian, the New York Times or the Washington Post? You don't need a data team, developers, much time or any funding to get started and produce data journalism that grabs headlines and engages readers. This workshop will focus on quick start techniques for getting started and making the most of few resources.
In Mexico open data journalism is still difficult to exercise. Find In Mexico, an organization to which I belong, we developed a research project in which we use databases and the result was a map (www.mexicoinformate.org/platform) that has managed to draw attention to the slow progress of the law enforcement system in the country.
Since it launched just 2 years ago, it has leveraged the open data community to grow to by far the largest open database of companies in the world with over 50 million companies in 70 jurisdictions, and is regularly used by journalists, anti-corruption investigators, civil society, even banks and financial institutions.
This session will cover the rapidly changing way that machines are taking on different parts of our lives, making decisions for us and altering our lives with our own data. Shelley Evenson will address how designers need to keep their human focus in order to truly capitalise on the benefits of big data without allowing technology to take over.
Being good is hard. Being evil is much more fun and gets you paid a lot more. We give a survey of the field of doing high-impact evil with data and analysis. We will look at some of the simplest things you can do to make the maximum (negative) impact on your friends, your business and the world. If you happen to learn something about doing good with data that will be your problem.
Analytics best practices, data feeds and flows between tools and continents are put in parallel with legislation, showing which steps to undertake for legal compliance; how to train for data protection & assure minimal liability. Its not about security, goes beyond the cookie debate, highlighting how the EU Personal Data Protection Regulation will influence analytics & how Privacy by Design helps
To take the right decision, you need the right data. As complexity and abundance of data increase, the communication of data analysis results becomes more challenging. Grounding our talk in the pharma R&D arena, we illustrate how animated and interactive graphics can streamline communication on complex data analysis and inform decision making.
The UK Government team behind the GOV.UK website talk about their work on the Performance Platform, a suite of services and a cultural shift taking people away from immensely detailed value stream maps about a call-centre and paper process (which might be an inherently 5-day long journey), to something that's digital, lightweight, fast and pleasant to use.
Big Data are very interesting for official statistics. Results obtained by analyzing large amounts of Dutch traffic loop detection records, Mobile phone data and Dutch social media messages are discussed to illustrate this.
For a two-sided marketplace like Airbnb, the search engine is the main driver of the health of the business. We developed an open-source technology stack and a set of analytical methods to optimize the search experience for our users and search conversion for our business. Well discuss the tools we use for data crunching, analysis and reporting, as well as our thoughts on experimental design.
A vending machine that dispenses crisps in response to financial news, laser sketches from 4000 years of solar eclipse data, and a mural created from QR codes acting as cellular automata. Featuring a diverse exploration of data, the ODI will showcase art commissioned and curated for its on-going Data as Culture programme which facilitates artists to explore data as an art material.
Farmers in the UK receive subsidies via the Common Agricultural Policy. These form a significant part of their income, approx 2billion/yr total. Changes to the data driving the payments are submitted by farmers as hand drawn imagery. I'll share what the Government Digital Service has learned working with Defra, prototyping ways to capture changes that are sustainable, cheaper and more accurate.
Crossrail will help deliver a new London. It is one of the largest civil engineering projects -- taking place literally under the feet of Strata London. We'll present how data science is being deployed at Crossrail to fundamentally change the way decisions are made and the operation is being run; from the CEO to the engineers monitoring ground movement in the tunnels.
The economy is in a mess. But good data can help fix it. Timely analysis of large data sets is beginning to provide insight into what's really happening to business growth, employment and prosperity. We'll look at some of the most exciting examples of how Big Data is changing the way we look at the economy, and how governments and businesses can use them to their advantage.
The "smart grid" isn't just about smart meters. Every stage of our electrical power infrastructure has to be "smart," including generation, transmission and distribution. Sophisticated sensors connected to software platforms that continuously gather, visualize and analyze massive amounts of data in real time to produce actionable insights are critical to optimizing our energy assets.
The Lean Startup model showed a generation of founders how to launch companies smarter and faster. At the core of this model is a constant cycle of building, measuring, and learning. In this session, we'll look at the "measure" part of this cycle, and how organizations of all sizes can use data to build a better business faster.
Making data work requires that organisations define success for their company, provide clear business goals, & articulate the right business questions. The best approach to overcoming the cognitive pitfalls that lead to failing to ask the right question come from the intelligence services. This seminar outlines what they do, and suggests how to use it effectively inside a typical business.
Some big organisations love the idea of using data to inform decision making but find the reality a little daunting to say the least. How are we demystifying data in the BBC and overcoming editorial fears about it lessening the view of the trusted human in making content decisions?
Learn how hadoop is helping TomTom to make fresher maps by continuously processing the incoming GPS data and how hbase is used to present that data to an Operator
78% of consumers use their smartphone while shopping in-store. What are they doing? More importantly, why? For all the media buzz around showrooming  look in-store, buy online - there is little insight on the issue. SapientNitro explains how key business questions drove hypotheses, data collection using novel instruments, and insights from analytic tools for testing and interpretive analysis.
How combining quantitative data analysis and qualitative social science work can complement each other, providing deeper understanding of behavior and open new doors of enquiry.
For a two-sided marketplace like Airbnb, the search engine is the main driver of the health of the business. We developed an open-source technology stack and a set of analytical methods to optimize the search experience for our users and search conversion for our business. Well discuss the tools we use for data crunching, analysis and reporting, as well as our thoughts on experimental design.
A vending machine that dispenses crisps in response to financial news, laser sketches from 4000 years of solar eclipse data, and a mural created from QR codes acting as cellular automata. Featuring a diverse exploration of data, the ODI will showcase art commissioned and curated for its on-going Data as Culture programme which facilitates artists to explore data as an art material.
Farmers in the UK receive subsidies via the Common Agricultural Policy. These form a significant part of their income, approx 2billion/yr total. Changes to the data driving the payments are submitted by farmers as hand drawn imagery. I'll share what the Government Digital Service has learned working with Defra, prototyping ways to capture changes that are sustainable, cheaper and more accurate.
Crossrail will help deliver a new London. It is one of the largest civil engineering projects -- taking place literally under the feet of Strata London. We'll present how data science is being deployed at Crossrail to fundamentally change the way decisions are made and the operation is being run; from the CEO to the engineers monitoring ground movement in the tunnels.
The economy is in a mess. But good data can help fix it. Timely analysis of large data sets is beginning to provide insight into what's really happening to business growth, employment and prosperity. We'll look at some of the most exciting examples of how Big Data is changing the way we look at the economy, and how governments and businesses can use them to their advantage.
The "smart grid" isn't just about smart meters. Every stage of our electrical power infrastructure has to be "smart," including generation, transmission and distribution. Sophisticated sensors connected to software platforms that continuously gather, visualize and analyze massive amounts of data in real time to produce actionable insights are critical to optimizing our energy assets.
The Lean Startup model showed a generation of founders how to launch companies smarter and faster. At the core of this model is a constant cycle of building, measuring, and learning. In this session, we'll look at the "measure" part of this cycle, and how organizations of all sizes can use data to build a better business faster.
Making data work requires that organisations define success for their company, provide clear business goals, & articulate the right business questions. The best approach to overcoming the cognitive pitfalls that lead to failing to ask the right question come from the intelligence services. This seminar outlines what they do, and suggests how to use it effectively inside a typical business.
Some big organisations love the idea of using data to inform decision making but find the reality a little daunting to say the least. How are we demystifying data in the BBC and overcoming editorial fears about it lessening the view of the trusted human in making content decisions?
Learn how hadoop is helping TomTom to make fresher maps by continuously processing the incoming GPS data and how hbase is used to present that data to an Operator
78% of consumers use their smartphone while shopping in-store. What are they doing? More importantly, why? For all the media buzz around showrooming  look in-store, buy online - there is little insight on the issue. SapientNitro explains how key business questions drove hypotheses, data collection using novel instruments, and insights from analytic tools for testing and interpretive analysis.
How combining quantitative data analysis and qualitative social science work can complement each other, providing deeper understanding of behavior and open new doors of enquiry.
I will discuss and showcase polyglot persistence and the lambda architecture.  Based on real-world examples and case studies from our customer base and the  wider community of practitioners, incl. the financial, energy & media industries  and the realm of public data sources (government data), we will elaborate on  opportunities and challenges of these new data management and processing memes.
Mainframe is Big Data too! Leveraging it in Hadoop creates a remarkable competitive advantage, but exploiting it without the right tools is nearly impossible, requiring you to wrestle with thousands of lines of Java, Pig, Hive, COBOL and more. This session presents a smarter way to ingest and process mainframe data in Hadoop, and how to bridge the technical, skill and cost gaps between the two.
In the era of M2M Communication and the Internet of Things on top of traditional 3V's of Big Data - Volume, Variety and Velocity we need to be able to process ephemeral data produced by dispersed sources which needs to be organized and distributed to multiple services. Real-time response, security,  compliancy,  compatibility, breaking silos - require new approaches to data management.
How can business and IT users easily explore, analyse and visualise data in Hadoop? Learn about alternatives to manually writing jobs or setting up predefined schemas and how a leading enterprise used Splunk and their Hadoop distribution to empower them with new access to Hadoop data. See how they got up and running in under an hour and enabled their developers to start writing big data apps.
The large-scale deployment of a big data strategy in the retail financial services sector poses specific challenges in terms of infrastructure, data limitations, organizational structure and portfolio definition and execution. We will share how we are addressing these challenges as well as selected demos and solutions, with focus on promising new financial product lines enabled by big data.
Attendees will leave this session with a deeper understanding of how organizations are using Hadoop to solve real business problems today, and how recent advancements in the Hadoop ecosystem are expanding the platform's capabilities to serve larger enterprise requirements for a virtual EDW.
How can companies use social and business data together to gain insight? See how Tableau's native Google BigQuery connector links seamlessly to live data in BigQuery and creates interactive visualizations without writing a single line of code. Find out how to share your results on the web and mobile in minutes.
In this presentation Chris will look at seven Map Reduce techniques that he enjoys playing with  the bits that are fun, exciting, and that can provide valuable insight into your big data.  With examples of code (and where you can look for it) for web, image and text processing he'll show things that can quickly allow you to extend your analysis beyond traditional data mining.
I will discuss and showcase polyglot persistence and the lambda architecture.  Based on real-world examples and case studies from our customer base and the  wider community of practitioners, incl. the financial, energy & media industries  and the realm of public data sources (government data), we will elaborate on  opportunities and challenges of these new data management and processing memes.
Mainframe is Big Data too! Leveraging it in Hadoop creates a remarkable competitive advantage, but exploiting it without the right tools is nearly impossible, requiring you to wrestle with thousands of lines of Java, Pig, Hive, COBOL and more. This session presents a smarter way to ingest and process mainframe data in Hadoop, and how to bridge the technical, skill and cost gaps between the two.
In the era of M2M Communication and the Internet of Things on top of traditional 3V's of Big Data - Volume, Variety and Velocity we need to be able to process ephemeral data produced by dispersed sources which needs to be organized and distributed to multiple services. Real-time response, security,  compliancy,  compatibility, breaking silos - require new approaches to data management.
How can business and IT users easily explore, analyse and visualise data in Hadoop? Learn about alternatives to manually writing jobs or setting up predefined schemas and how a leading enterprise used Splunk and their Hadoop distribution to empower them with new access to Hadoop data. See how they got up and running in under an hour and enabled their developers to start writing big data apps.
The large-scale deployment of a big data strategy in the retail financial services sector poses specific challenges in terms of infrastructure, data limitations, organizational structure and portfolio definition and execution. We will share how we are addressing these challenges as well as selected demos and solutions, with focus on promising new financial product lines enabled by big data.
Attendees will leave this session with a deeper understanding of how organizations are using Hadoop to solve real business problems today, and how recent advancements in the Hadoop ecosystem are expanding the platform's capabilities to serve larger enterprise requirements for a virtual EDW.
How can companies use social and business data together to gain insight? See how Tableau's native Google BigQuery connector links seamlessly to live data in BigQuery and creates interactive visualizations without writing a single line of code. Find out how to share your results on the web and mobile in minutes.
In this presentation Chris will look at seven Map Reduce techniques that he enjoys playing with  the bits that are fun, exciting, and that can provide valuable insight into your big data.  With examples of code (and where you can look for it) for web, image and text processing he'll show things that can quickly allow you to extend your analysis beyond traditional data mining.
Program Chairs, Edd Dumbill and Kaitlin Thaney, open the first day of keynotes.
Program Chairs, Edd Dumbill and Kaitlin Thaney, open the second day of keynotes.
If you had five minutes on stage what would you say? What if you only got 20 slides and they rotated automatically after 5 seconds? Well find out again this year, the second day of Strata in London and the day before Velocity Europefor one big, combined, rip-roaring Ignite event.
As technology further pervades enterprises, each generates more data.  Once harnessed, this data can enhance business, enabling growth.  A new home for data has arrived to better support this: the Enterprise Data Hub, with Apache Hadoop at its center.  Doug will discuss the trends that drive this and speculate on where they lead.
In this talk Felienne will summarize her recently completed PhD research on the topic of spreadsheet structure visualization, spreadsheet smells and clone detection, as well as presenting a sneak peek into the future of spreadsheet research as Delft University.
Dat aims to bring a distributed collaboration flow to big data. Git and Github have done it for source code, but we don't yet have a social data solution.
The NHS produces an amazing amount of detailed raw data about health, prescribing, doctors, hospitals, and so on. The data's a great resource for data scientists to experiment with and learn on - it's very rich, interesting, and important to society.  This session will discuss the available datasets and work through some example analyses of the data from different perspectives.
Keynote by Tim Kelsey, National Director for Patients and Information, National Health Service.
Big data has proved it's worth in a number of industries, but it's not the size or the storage that is making the difference.  The organisations that are delivering most value are the ones that have realised the need to drive analytics into the heart of their decision making process.
We hear stories of how big data is unprecedented and about the latest disruptive products to hit the market, products that are totally different and will change everything. Yet looking at the underlying concepts, most of these arent all that new and the ones that are new are being explained in the terms of the old, in the same way cars were described as horseless carriages.
Gavin Starks, CEO, Open Data Institute (ODI).
Data science may seem like a revolutionary new field, but it is merely the latest incarnation of a tradition as old as we are: storytelling. And because it is part of such an inherently human practice, it is most valuable when it takes humanity into account. This talk explores how to use data and the techniques associated with data to build things that matter, by looking back to look forward.
Program Chairs, Edd Dumbill and Kaitlin Thaney, open the first day of keynotes.
Program Chairs, Edd Dumbill and Kaitlin Thaney, open the second day of keynotes.
If you had five minutes on stage what would you say? What if you only got 20 slides and they rotated automatically after 5 seconds? Well find out again this year, the second day of Strata in London and the day before Velocity Europefor one big, combined, rip-roaring Ignite event.
As technology further pervades enterprises, each generates more data.  Once harnessed, this data can enhance business, enabling growth.  A new home for data has arrived to better support this: the Enterprise Data Hub, with Apache Hadoop at its center.  Doug will discuss the trends that drive this and speculate on where they lead.
In this talk Felienne will summarize her recently completed PhD research on the topic of spreadsheet structure visualization, spreadsheet smells and clone detection, as well as presenting a sneak peek into the future of spreadsheet research as Delft University.
Dat aims to bring a distributed collaboration flow to big data. Git and Github have done it for source code, but we don't yet have a social data solution.
The NHS produces an amazing amount of detailed raw data about health, prescribing, doctors, hospitals, and so on. The data's a great resource for data scientists to experiment with and learn on - it's very rich, interesting, and important to society.  This session will discuss the available datasets and work through some example analyses of the data from different perspectives.
Keynote by Tim Kelsey, National Director for Patients and Information, National Health Service.
Big data has proved it's worth in a number of industries, but it's not the size or the storage that is making the difference.  The organisations that are delivering most value are the ones that have realised the need to drive analytics into the heart of their decision making process.
We hear stories of how big data is unprecedented and about the latest disruptive products to hit the market, products that are totally different and will change everything. Yet looking at the underlying concepts, most of these arent all that new and the ones that are new are being explained in the terms of the old, in the same way cars were described as horseless carriages.
Gavin Starks, CEO, Open Data Institute (ODI).
Data science may seem like a revolutionary new field, but it is merely the latest incarnation of a tradition as old as we are: storytelling. And because it is part of such an inherently human practice, it is most valuable when it takes humanity into account. This talk explores how to use data and the techniques associated with data to build things that matter, by looking back to look forward.
Grab a drink, mingle with fellow attendees, and see the latest in big data technologies and products from leading companies at the Attendee Reception - happening Monday evening immediately following afternoon sessions. We'll also continue hosting Startup Showcase during the reception.
Grab a drink, mingle with fellow attendees, and see the latest in big data technologies and products from leading companies at the Attendee Reception - happening Monday evening immediately following afternoon sessions. We'll also continue hosting Startup Showcase during the reception.
From advanced visualization, collaboration, reproducibility to data manipulation, R Day at Strata covers a raft of current topics that analysts and R users need to pay attention to. The R Day tutorials come from leading luminaries and R committers, the folks keeping the R ecosystem apace of the challenges facing analysts and others who work with data.
In this session we will talk through the challenges of anomaly detection in high cardinality dimensions, and specifically how we derive value through a combination of data science and traditional business intelligence.
Leveraging a datasets summary or data profile to inform the analysis process isn't a new concept but in the changing data landscape this process needs to be rethought to handle the different shapes and sizes of big data. Trifacta's Joe Hellerstein and Adam Silberstein discuss new approaches to data profiling specifically designed for quickly understanding the content & quality of modern datasets.
Many development teams fail to set up logging properly so that when they bring in a data science team down the road, their data is missing, wrong, or lacking key fields. A quick data audit could catch many of these common mistakes, saving money, time, and insight. This talks covers the three things to check and several handy tools so you can sleep soundly, knowing your data are collecting safely.
Scientists make the best data scientists. Yet there is a skills gap that exists between quantitative data analysis done in a research context and data science in industry. The Data Science Fellows Program has helped over 150 PhDs make the transition, in this session it's founder will share lessons learned in bridging that gap, and the lessons that can be applied to building data science teams.
When building a real-time data application, we must decide what tradeoffs are permissible without eroding core functionality. As the purpose of data applications become more complex, and the size of the data stores analyzed expand, maintaining integrity and speed becomes increasingly difficult to solve.
The Jupyter/IPython Notebook is a web-based interactive computing platform for Data Science in Python, Julia, R and other languages. In this talk we will describe our recent work to bring the Notebook to larger groups of users, both on the open web and within organizations. The focus will be on new collaboration features and deploying the Notebook in these contexts.
If we want to use data to understand human behavior and to design successful interventions to change that behavior, social scientists and data scientists will need to work together. However, the two often approach problems differently and speak strikingly different languages. This talk will present success stories and tips for productive collaboration between social scientists and data scientists.
While many companies use data science to increase profits, some nonprofits are using it to save lives! Crisis Text Line connects teens in crisis to counselors via text message, and recently partnered with DataKind and Pivotal on a pro bono project to more quickly route teens to help. Go behind the scenes to learn how they came together to make an impact and how you can too!
In this talk we are addressing the following aspects of machine translation development at eBay:  - leveraging huge amounts of transactional and behavioral data for development and evaluation of our MT systems; - adapting evaluation metrics to reflect the eBay buyer experience and measuring translation quality and impact on the shopping experience of our international users.
Data scientists navel gazing in a corner. Engineers not thinking, just refactoring. Product just making slides. Thats no way to build data products. Is it even possible to have them play well together, without promising free lunches, unlimited gummy bears, and a Red Bull IV? We share our experience about what worked and what didnt, both in a startup and in a big company environment.
This session will discuss how to build a resilient, multi-modal event-detection system based on error-prone sourcesvideo, audio, natural language, and external APIs. We will briefly review event-detection techniques and then demonstrate how to combine these across multiple data streams.
In this talk, I will introduce Velox, the newest component of the Berkeley Data Analytics Stack. Velox is the missing piece in the predictive analytics stack enabling interactive applications ranging from content recommendations to personalized search by addressing the challenges of serving and managing personalized machine learning models at scale.
Bots don't drink soda, so advertisers dont want to advertise to them. Accurately counting real people is critical in the digital ad industry. This session will show how comScore uses over 1.5 trillion events of data to separate real people from bots. Ill describe how we use correlations at scale, heuristic classification, and multi-source anomaly detection to make decisions in real time.
Learn how tools based on nation-wide job market data can help both students and institutions improve outcomes from the job market level down to curriculum and course choice.
Building real-time relevance systems for mobile presents a unique blend of challenges from both modeling and architectural perspectives.  In this talk, well take an in-depth look at the machine learning infrastructure that powers Connected, LinkedIns mobile application that helps our members nurture and leverage their professional networks.
From advanced visualization, collaboration, reproducibility to data manipulation, R Day at Strata covers a raft of current topics that analysts and R users need to pay attention to. The R Day tutorials come from leading luminaries and R committers, the folks keeping the R ecosystem apace of the challenges facing analysts and others who work with data.
In this session we will talk through the challenges of anomaly detection in high cardinality dimensions, and specifically how we derive value through a combination of data science and traditional business intelligence.
Leveraging a datasets summary or data profile to inform the analysis process isn't a new concept but in the changing data landscape this process needs to be rethought to handle the different shapes and sizes of big data. Trifacta's Joe Hellerstein and Adam Silberstein discuss new approaches to data profiling specifically designed for quickly understanding the content & quality of modern datasets.
Many development teams fail to set up logging properly so that when they bring in a data science team down the road, their data is missing, wrong, or lacking key fields. A quick data audit could catch many of these common mistakes, saving money, time, and insight. This talks covers the three things to check and several handy tools so you can sleep soundly, knowing your data are collecting safely.
Scientists make the best data scientists. Yet there is a skills gap that exists between quantitative data analysis done in a research context and data science in industry. The Data Science Fellows Program has helped over 150 PhDs make the transition, in this session it's founder will share lessons learned in bridging that gap, and the lessons that can be applied to building data science teams.
When building a real-time data application, we must decide what tradeoffs are permissible without eroding core functionality. As the purpose of data applications become more complex, and the size of the data stores analyzed expand, maintaining integrity and speed becomes increasingly difficult to solve.
The Jupyter/IPython Notebook is a web-based interactive computing platform for Data Science in Python, Julia, R and other languages. In this talk we will describe our recent work to bring the Notebook to larger groups of users, both on the open web and within organizations. The focus will be on new collaboration features and deploying the Notebook in these contexts.
If we want to use data to understand human behavior and to design successful interventions to change that behavior, social scientists and data scientists will need to work together. However, the two often approach problems differently and speak strikingly different languages. This talk will present success stories and tips for productive collaboration between social scientists and data scientists.
While many companies use data science to increase profits, some nonprofits are using it to save lives! Crisis Text Line connects teens in crisis to counselors via text message, and recently partnered with DataKind and Pivotal on a pro bono project to more quickly route teens to help. Go behind the scenes to learn how they came together to make an impact and how you can too!
In this talk we are addressing the following aspects of machine translation development at eBay:  - leveraging huge amounts of transactional and behavioral data for development and evaluation of our MT systems; - adapting evaluation metrics to reflect the eBay buyer experience and measuring translation quality and impact on the shopping experience of our international users.
Data scientists navel gazing in a corner. Engineers not thinking, just refactoring. Product just making slides. Thats no way to build data products. Is it even possible to have them play well together, without promising free lunches, unlimited gummy bears, and a Red Bull IV? We share our experience about what worked and what didnt, both in a startup and in a big company environment.
This session will discuss how to build a resilient, multi-modal event-detection system based on error-prone sourcesvideo, audio, natural language, and external APIs. We will briefly review event-detection techniques and then demonstrate how to combine these across multiple data streams.
In this talk, I will introduce Velox, the newest component of the Berkeley Data Analytics Stack. Velox is the missing piece in the predictive analytics stack enabling interactive applications ranging from content recommendations to personalized search by addressing the challenges of serving and managing personalized machine learning models at scale.
Bots don't drink soda, so advertisers dont want to advertise to them. Accurately counting real people is critical in the digital ad industry. This session will show how comScore uses over 1.5 trillion events of data to separate real people from bots. Ill describe how we use correlations at scale, heuristic classification, and multi-source anomaly detection to make decisions in real time.
Learn how tools based on nation-wide job market data can help both students and institutions improve outcomes from the job market level down to curriculum and course choice.
Building real-time relevance systems for mobile presents a unique blend of challenges from both modeling and architectural perspectives.  In this talk, well take an in-depth look at the machine learning infrastructure that powers Connected, LinkedIns mobile application that helps our members nurture and leverage their professional networks.
All-Day: Strata's regular data science track has great talks with real world experience from leading edge speakers. But we didn't just stop therewe added the Hardcore Data Science day to give you a chance to go even deeper. The Hardcore day will add new techniques and technologies to your data science toolbox, shared by leading data science practitioners from startups, industry, consulting...
Big Data is existing it's buzz word phase and we are seeing applications which use big data infrastructure to power every day lives. This is a discussion from the front lines with panelists from industry and startups describing real deployed application powered by big data, but which are happy to be hiding the elephant behind beautiful interfaces.
Most people are familiar with the basic principles driving todays hottest big data and enterprise companies. But whats really going on underneath the hood? In this session, Kleiner Perkins Caufield & Byers General Partner Michael Abbott unboxes a variety of startups in the space to examine the technology, architecture, and innovations theyve harnessed to deliver superior products and services.
At Rent the Runway, we have focused on using data to make decisions since day 1. But, the best manifestation is driving the strategy and building products using data, which has been critical to our growth. This talk will share examples that illustrate this, and how data is an unlikely hero behind the scenes of successfully renting sparkly designer dresses.
Retail buyers are the backbone of the industries profitability. These individuals drive organizational goals with their performance. Many decisions are made by intuition and gut feeling, where predictive analytics would have made significant improvements in outcomes. This session takes real world experiences and shows how to transform retail performance through data driven buying decisions.
Ruthless optimization squeezes every ounce of advantage from the current business model. But it takes a leap of faithnot something the numbers tend to encourageto truly innovate. When were informed by data, are we blinded by opportunity? Or does data pave the way for the best innovations, forcing us to take a harder look at bad ideas that will never work out?
To anticipate who will succeed and invest wisely, investors spend a lot of time trying to understand the longer-term trends within an industry. In this panel discussion, well consider the big trends in Big Data, asking top-tier VCs to look over the horizon discuss the visions they have two or more years in the future.
According to industry research, only half of travelers today know exactly where they want to go before planning a trip. This session will provide a holistic view on some of our recent personalization products aimed at inspiring travel. We will start with the product vision, describe the powerful algorithms deployed, and finally explain how we evaluated the long term effects of our product.
In a 2013 New York Times column, Thomas Friedman claimed that Airbnbs real innovation is not online rentals. Its trust.  This session will discuss recent experiments conducted at Airbnb to improve the frequency and honesty of reviews, and the methods used to evaluate changes in the quality of subsequent reviews and the impact of these changes on other key business metrics.
The future is all about information. It will belong to those who can find it, understand it and know how to use it. In this panel discussion, we explore evidence-based benefits of welcoming more women into the tech community and of increasing female talent power on work teams.
In the 2014 election cycle, the Republican National Committee spent significant amount of resources on engineering and data science to help GOP senate candidates across the country. As the first ever Chief Data Officer of the RNC, Azarias led this effort. In this talk, he will discuss some of the lessons learned helping the republican party use data and engineering to win the US Senate.
There is no shortage of opinions expressed across the Web on virtually any topic. This enables a diversity of voices to be heard, but often leaves users overwhelmed. We report on our implementation of a big data platform that identifies and ranks experts on a large number of topics. It allows users to cut through the noise and discover opinions expressed by credible experts in topics of interest.
What types of organizations generate the most revenue and profits from Big Data initiatives?  They need to be agile and adopt new technologies, grow existing resource skills while attracting new skills, and yet still manage and govern data.  In this session well describe organization structures, roles, skills, and interactions that make these types of data-driven organizations successful.
Creating data analytics solutions for the cloud requires a new way of thinking about data architectures. Users expect to combine data seamlessly across services while IT demands that new tools leverage existing investments in security and administration. This talk will discuss the challenges of architecting for the cloud and present real-world case studies of the benefits of these architectures.
As the leading source of intelligent information, Thomson Reuters delivers must-have insight to the worlds financial and risk, legal, tax and accounting, intellectual property and science and media professionals, supported by the worlds most trusted news organization.
All-Day: Strata's regular data science track has great talks with real world experience from leading edge speakers. But we didn't just stop therewe added the Hardcore Data Science day to give you a chance to go even deeper. The Hardcore day will add new techniques and technologies to your data science toolbox, shared by leading data science practitioners from startups, industry, consulting...
Big Data is existing it's buzz word phase and we are seeing applications which use big data infrastructure to power every day lives. This is a discussion from the front lines with panelists from industry and startups describing real deployed application powered by big data, but which are happy to be hiding the elephant behind beautiful interfaces.
Most people are familiar with the basic principles driving todays hottest big data and enterprise companies. But whats really going on underneath the hood? In this session, Kleiner Perkins Caufield & Byers General Partner Michael Abbott unboxes a variety of startups in the space to examine the technology, architecture, and innovations theyve harnessed to deliver superior products and services.
At Rent the Runway, we have focused on using data to make decisions since day 1. But, the best manifestation is driving the strategy and building products using data, which has been critical to our growth. This talk will share examples that illustrate this, and how data is an unlikely hero behind the scenes of successfully renting sparkly designer dresses.
Retail buyers are the backbone of the industries profitability. These individuals drive organizational goals with their performance. Many decisions are made by intuition and gut feeling, where predictive analytics would have made significant improvements in outcomes. This session takes real world experiences and shows how to transform retail performance through data driven buying decisions.
Ruthless optimization squeezes every ounce of advantage from the current business model. But it takes a leap of faithnot something the numbers tend to encourageto truly innovate. When were informed by data, are we blinded by opportunity? Or does data pave the way for the best innovations, forcing us to take a harder look at bad ideas that will never work out?
To anticipate who will succeed and invest wisely, investors spend a lot of time trying to understand the longer-term trends within an industry. In this panel discussion, well consider the big trends in Big Data, asking top-tier VCs to look over the horizon discuss the visions they have two or more years in the future.
According to industry research, only half of travelers today know exactly where they want to go before planning a trip. This session will provide a holistic view on some of our recent personalization products aimed at inspiring travel. We will start with the product vision, describe the powerful algorithms deployed, and finally explain how we evaluated the long term effects of our product.
In a 2013 New York Times column, Thomas Friedman claimed that Airbnbs real innovation is not online rentals. Its trust.  This session will discuss recent experiments conducted at Airbnb to improve the frequency and honesty of reviews, and the methods used to evaluate changes in the quality of subsequent reviews and the impact of these changes on other key business metrics.
The future is all about information. It will belong to those who can find it, understand it and know how to use it. In this panel discussion, we explore evidence-based benefits of welcoming more women into the tech community and of increasing female talent power on work teams.
In the 2014 election cycle, the Republican National Committee spent significant amount of resources on engineering and data science to help GOP senate candidates across the country. As the first ever Chief Data Officer of the RNC, Azarias led this effort. In this talk, he will discuss some of the lessons learned helping the republican party use data and engineering to win the US Senate.
There is no shortage of opinions expressed across the Web on virtually any topic. This enables a diversity of voices to be heard, but often leaves users overwhelmed. We report on our implementation of a big data platform that identifies and ranks experts on a large number of topics. It allows users to cut through the noise and discover opinions expressed by credible experts in topics of interest.
What types of organizations generate the most revenue and profits from Big Data initiatives?  They need to be agile and adopt new technologies, grow existing resource skills while attracting new skills, and yet still manage and govern data.  In this session well describe organization structures, roles, skills, and interactions that make these types of data-driven organizations successful.
Creating data analytics solutions for the cloud requires a new way of thinking about data architectures. Users expect to combine data seamlessly across services while IT demands that new tools leverage existing investments in security and administration. This talk will discuss the challenges of architecting for the cloud and present real-world case studies of the benefits of these architectures.
As the leading source of intelligent information, Thomson Reuters delivers must-have insight to the worlds financial and risk, legal, tax and accounting, intellectual property and science and media professionals, supported by the worlds most trusted news organization.
All-Day: For business strategists, marketers, product managers, and entrepreneurs, Data-Driven Business looks at how to use data to make better business decisions faster. Packed with case studies, panels, and eye-opening presentations, this fast-paced day focuses on how to solve today's thorniest business problems with Big Data. It's the missing MBA for a data-driven, always-on business world.
Today we've got NoSQL. But relational databases were the noSomething. What was that something? Why and where did relational databases come from? Then why years later are we seemingly focused on rejecting the lessons that led us to relational databases? This talk Lessons from the past that help strike a balance between the dueling promises of SQL and noSQL.
We will take a close look at use cases related to log data processing listing fundamental requirements that must be satisfied by log management systems. We will look at existing products and technologies harnessed for ingesting, storing, querying, and analyzing machine data. Finally, we will attempt to construct the archetype of the ideal platform for the management of log data.
Explore several different approaches taken by organizations embarking on a data governance journey to meet their own unique business objectives.  Review best practices and lessons learned.
Efficiency, cost effectiveness, organizational capability, corporate standards, risk aversion, shareholder returns, innovation and talent management, are stated essential ingredients for any large enterprise. When it comes to today's challenge of obtaining, engaging, developing and retaining dynamic technical talent there are few LESS appealing places to seek employment.
Big data is the sexy new frontier for many businesses but its expensive to stand up in an organization and expensive to buy from an external vendor.   What is the most fundamental way to demonstrate that data science matters to the organization? This session covers the meaningful data consumption metric that every data science group needs to track.
While cutting edge startups use Spark to see their data analysed in real-time, older and bigger organisations still struggle to share their data in a structural way between its HR, finance and operations departments. This talk will discuss how the belgian MoD opened its data using open source tools and becomes more and more data-driven.
In Predictive Maintenance and Service makers of assets (like automotive) or the operators of assets (like mining or manufacturing) bring together machine sensor data and maintenance data to better understand when and why machines fail, but also predict future failures, and needed business activities. This presentation gives an overview of the topic and what SAP customers have done.
Silicon Valley may be the center of Big Data technology production, but its application is having a far bigger impact on old-school industries like agriculture and brick-and-mortar retailing. This session will detail some of the world's most innovative applications from some of the world's oldest organizations.
This session tells the story of how a young technology company helped The Associated Press embrace cutting-edge data innovation at the heart of its business: the automation of corporate earnings stories. The challenges along the way offer valuable lessons for any brand engaged in a major data implementation  and any vendor who wants to help.
Big Data can transform learning from past's one way push of finely crafted content "at" learners to a two way data conversation that streams real-time feedback from students as learning challenges are tackled.  What kind of rich opportunities exist in analyzing and visualizing that two-way data stream as learners interact with open-ended content?
With LinkedIn's wealth of data we can answer questions previously limited by human resources. We can ask which industries have the most ties with health care? How do you meet Richard Branson? More seriously, what types of connections are used to find jobs? To answer these questions, we weave the algorithmic complexities and data harvesting into stories that enrich our understanding of the answers.
As climate change increases weather variability, farmers must adapt. Add to this global population growth and diet changes that require world food production to have increased by 70 percent in 2050 means farmers will struggle to meet demand. Of the 580 million farmers in the world, 500 million have little access to technology or information to ensure agile adaptation. Big helps solve this problem.
We will introduce the concept of Machine Learning Building Blocks - elements that can be mapped into hardware and software primitives and patterns. We demonstrate the implication of this concept in designing some specific workloads. Finally, we look at the Workload Optimization Framework, which includes a comprehensive Machine Learning workload suite, composed of sampled & constructed workloads
All-Day: For business strategists, marketers, product managers, and entrepreneurs, Data-Driven Business looks at how to use data to make better business decisions faster. Packed with case studies, panels, and eye-opening presentations, this fast-paced day focuses on how to solve today's thorniest business problems with Big Data. It's the missing MBA for a data-driven, always-on business world.
Today we've got NoSQL. But relational databases were the noSomething. What was that something? Why and where did relational databases come from? Then why years later are we seemingly focused on rejecting the lessons that led us to relational databases? This talk Lessons from the past that help strike a balance between the dueling promises of SQL and noSQL.
We will take a close look at use cases related to log data processing listing fundamental requirements that must be satisfied by log management systems. We will look at existing products and technologies harnessed for ingesting, storing, querying, and analyzing machine data. Finally, we will attempt to construct the archetype of the ideal platform for the management of log data.
Explore several different approaches taken by organizations embarking on a data governance journey to meet their own unique business objectives.  Review best practices and lessons learned.
Efficiency, cost effectiveness, organizational capability, corporate standards, risk aversion, shareholder returns, innovation and talent management, are stated essential ingredients for any large enterprise. When it comes to today's challenge of obtaining, engaging, developing and retaining dynamic technical talent there are few LESS appealing places to seek employment.
Big data is the sexy new frontier for many businesses but its expensive to stand up in an organization and expensive to buy from an external vendor.   What is the most fundamental way to demonstrate that data science matters to the organization? This session covers the meaningful data consumption metric that every data science group needs to track.
While cutting edge startups use Spark to see their data analysed in real-time, older and bigger organisations still struggle to share their data in a structural way between its HR, finance and operations departments. This talk will discuss how the belgian MoD opened its data using open source tools and becomes more and more data-driven.
In Predictive Maintenance and Service makers of assets (like automotive) or the operators of assets (like mining or manufacturing) bring together machine sensor data and maintenance data to better understand when and why machines fail, but also predict future failures, and needed business activities. This presentation gives an overview of the topic and what SAP customers have done.
Silicon Valley may be the center of Big Data technology production, but its application is having a far bigger impact on old-school industries like agriculture and brick-and-mortar retailing. This session will detail some of the world's most innovative applications from some of the world's oldest organizations.
This session tells the story of how a young technology company helped The Associated Press embrace cutting-edge data innovation at the heart of its business: the automation of corporate earnings stories. The challenges along the way offer valuable lessons for any brand engaged in a major data implementation  and any vendor who wants to help.
Big Data can transform learning from past's one way push of finely crafted content "at" learners to a two way data conversation that streams real-time feedback from students as learning challenges are tackled.  What kind of rich opportunities exist in analyzing and visualizing that two-way data stream as learners interact with open-ended content?
With LinkedIn's wealth of data we can answer questions previously limited by human resources. We can ask which industries have the most ties with health care? How do you meet Richard Branson? More seriously, what types of connections are used to find jobs? To answer these questions, we weave the algorithmic complexities and data harvesting into stories that enrich our understanding of the answers.
As climate change increases weather variability, farmers must adapt. Add to this global population growth and diet changes that require world food production to have increased by 70 percent in 2050 means farmers will struggle to meet demand. Of the 580 million farmers in the world, 500 million have little access to technology or information to ensure agile adaptation. Big helps solve this problem.
We will introduce the concept of Machine Learning Building Blocks - elements that can be mapped into hardware and software primitives and patterns. We demonstrate the implication of this concept in designing some specific workloads. Finally, we look at the Workload Optimization Framework, which includes a comprehensive Machine Learning workload suite, composed of sampled & constructed workloads
Python has become an increasingly important part of the data engineer and analytic tool landscape. Pydata at Strata provides in-depth coverage of the tools and techniques gaining traction with the data audience, including iPython Notebook, NumPy/matplotlib for visualization, SciPy, scikit-learn, and how to scale Python performance, including how to handle large, distributed data sets.
When your company stores some of the most sensitive customer data that exists, how do you build game changing big data innovations while maintaining customer trust and loyalty? Combine the two groups responsible for that vision--legal and data science--and unite them toward a common goal!  We'll discuss how Intuit turned the typical data-legal model on its head to boost data-driven innovation.
Our modern world is one where virtually everything is public by default, making the very notion of privacy radically different than the private by default era when the concept was first enshrined in law. This session will explore what we can do with the exploding volume of our personal data alongside the increasingly important question of what should we be doing with this data.
During the last government shutdown, on "The Daily Show with Jon Stewart," John Oliver noted that congress has a 90% retention rate despite a 10% approval rating. Why? Gerrymandering has become a prime suspect. Is this true, or just truthy? Come find out how a state with a 51% Democrat, 49% Republican electorate enjoys a lopsided congressional delegation of 4 Democrats and 9 Republicans.
Companies using data especially the ones deploying analytics driven workflow are challenged about right mix of first party and third party data. A large part of challenges are due to lack of clarity about data sources and its reliability, privacy laws and logistics needed for mass scale data aggregation.
In 2014, FINRA developed a new system to analyze the complex linkages between orders and trades in the US equities capital markets.  This session will highlight the outcomes of the big data solution that allowed FINRAs analysts to more efficiently conduct regulatory reviews and improve accessibility to over a trillion market events, and highlight the lessons learned during the implementation.
Privacy laws as to a companys obligations on data collection, use, disclosure are changing rapidly. Failing to understand how the laws affect a companys personal data assets can result in media exposes, regulatory investigations, Congressional hearings and lawsuits. This session will provide guidance on privacy by design compliance and practical tips to avoid becoming a target of scrutiny.
The ability of software to recognize patterns in usage, data, or other inputs to improve a users experience & productivity is an expected attribute of modern software. Trifactas Jeffrey Heer and Tye Rattenbury discuss design and software architecture principles for creating intelligent software that incorporates learning to make the process of transforming data more intuitive and efficient.
We lots of things "data visualization," from a news interactive, to spreadsheets, to an infographic counting calories. These surface similarities hide deep differences in what it means to interact with data. In this talk, we cross disciplinesfrom data science to designto enliven our techniques and encourage us to try new methods for creating visualizations.
Even the most data-driven organizations still incorporate art into their decision-making process. Values, culture, social norms, and biases influence decisions as much as the data. This isnt always a bad thingdata can sometimes fail to tell the whole story. And, by combining data with the intellectual assets that reside in the heads of employees we can create new capabilities.
IDEO's Hybrid team brings all the design tools from IDEO's product design process to work with clients on data oriented projects. The team will share elements of their process and case studies to show how incorporating human-centered techniques from design can improve data as an input to decision making.
Python has become an increasingly important part of the data engineer and analytic tool landscape. Pydata at Strata provides in-depth coverage of the tools and techniques gaining traction with the data audience, including iPython Notebook, NumPy/matplotlib for visualization, SciPy, scikit-learn, and how to scale Python performance, including how to handle large, distributed data sets.
When your company stores some of the most sensitive customer data that exists, how do you build game changing big data innovations while maintaining customer trust and loyalty? Combine the two groups responsible for that vision--legal and data science--and unite them toward a common goal!  We'll discuss how Intuit turned the typical data-legal model on its head to boost data-driven innovation.
Our modern world is one where virtually everything is public by default, making the very notion of privacy radically different than the private by default era when the concept was first enshrined in law. This session will explore what we can do with the exploding volume of our personal data alongside the increasingly important question of what should we be doing with this data.
During the last government shutdown, on "The Daily Show with Jon Stewart," John Oliver noted that congress has a 90% retention rate despite a 10% approval rating. Why? Gerrymandering has become a prime suspect. Is this true, or just truthy? Come find out how a state with a 51% Democrat, 49% Republican electorate enjoys a lopsided congressional delegation of 4 Democrats and 9 Republicans.
Companies using data especially the ones deploying analytics driven workflow are challenged about right mix of first party and third party data. A large part of challenges are due to lack of clarity about data sources and its reliability, privacy laws and logistics needed for mass scale data aggregation.
In 2014, FINRA developed a new system to analyze the complex linkages between orders and trades in the US equities capital markets.  This session will highlight the outcomes of the big data solution that allowed FINRAs analysts to more efficiently conduct regulatory reviews and improve accessibility to over a trillion market events, and highlight the lessons learned during the implementation.
Privacy laws as to a companys obligations on data collection, use, disclosure are changing rapidly. Failing to understand how the laws affect a companys personal data assets can result in media exposes, regulatory investigations, Congressional hearings and lawsuits. This session will provide guidance on privacy by design compliance and practical tips to avoid becoming a target of scrutiny.
The ability of software to recognize patterns in usage, data, or other inputs to improve a users experience & productivity is an expected attribute of modern software. Trifactas Jeffrey Heer and Tye Rattenbury discuss design and software architecture principles for creating intelligent software that incorporates learning to make the process of transforming data more intuitive and efficient.
We lots of things "data visualization," from a news interactive, to spreadsheets, to an infographic counting calories. These surface similarities hide deep differences in what it means to interact with data. In this talk, we cross disciplinesfrom data science to designto enliven our techniques and encourage us to try new methods for creating visualizations.
Even the most data-driven organizations still incorporate art into their decision-making process. Values, culture, social norms, and biases influence decisions as much as the data. This isnt always a bad thingdata can sometimes fail to tell the whole story. And, by combining data with the intellectual assets that reside in the heads of employees we can create new capabilities.
IDEO's Hybrid team brings all the design tools from IDEO's product design process to work with clients on data oriented projects. The team will share elements of their process and case studies to show how incorporating human-centered techniques from design can improve data as an input to decision making.
A full-day, hands-on tutorial introducing Apache Spark and libraries for building workflows: Spark SQL, Spark Streaming, MLlib, GraphX, etc.
The most frustrating part of data science is when customers dont get it: endless revisions, recommendations not implemented, or data products not adopted. Exciting new research in neurology, cognitive psychology, and behavioral economics have a lot to say about why. Well explore the findings and implications for designing more successful human-data interfaces.
In far too many organizations, data scientists and designers work in silos, and quibble about whos more important. This is a huge missed opportunity. At Intuit, we are reimagining how our data and design teams to work together to fuel innovation and surpass Intuits business goals. I will walk through methods we are using to bridge these two wildly different groups and share stories of success.
Data products are poised to go mainstream, but only if they are designed well.  Most data products are designed by developers for developers. This talk discusses methods from Stanford's D.School used by companies like Yahoo!, Samsung, and Audi to design break-out products. These principles can help developers get beyond technology and design products for everyday users.
From its inception, Palantir Technologies has been about integrating the best of big data technology into systems that enable subject matter experts (as opposed to data scientists or programmers) to move through huge volumes of data and do their own data analysis. Equal parts data system design and UX, we break down the design of building systems usable by mere mortals.
Designing data visualizations presents us with unique and interesting challenges: how to tell a compelling story; how to deliver important information in a forthright, clear format; and how to make visualizations beautiful and engaging. In this talk, Julie will share a few disruptive designs and connect those back to vizipedia, her compiled data visualization library.
As Director of UX Design at software analytic company New Relic, my core focus is trying to present the over 200 billion data points across more than three million applications we monitor in a way that provides meaning so customers can make good decisions for their business. Today Ill share some of what Ive learned along the way.
Sensor devices are proliferating fast now that manufacturing price is under $100. And while more data is being generated, more is also being thrown out because of the resource gap between compute and network. We must make new computational trade-offs whilst ensuring quality. In this talk, we discuss these trade-offs and examine architectures for peer-based and crowd-sourced model generation.
Thirty years ago, data warehouses revolutionized data storage at big companies, storing summarized data in a strict structure and making it possible to efficiently analyze data. We believe that modern technology lets you adopt a simpler and more powerful scheme to organize historical data: time ordered raw event logs. In this session, we'll explain why raw data is better.
Finding anomalies is essential for a wide range of applications, including cybersecurity, event detection and health and status monitoring.   Anomaly techniques that scale successfully to large datasets tend to integrate machine learning with good data engineering.  We discuss three case studies and extract eight techniques that have proved effective for detecting anomalies in large scale systems.
The unprecedented increase in streaming data sources requires a new approach to analytical algorithms. Systems must be highly automated, adapt to changing statistics, and naturally deal with temporal data streams. They must require no batch training and should deploy custom models on the fly. It will be impossible to build scalable practical systems without these properties.
The US is in an oil boom, driven by new technologies that enable the economic production of shale resources. Conventional exploration techniques dont work well for these unconventional reserves. In this talk, Kaggles Chief Scientist will discuss Kaggles pioneering work in machine learning for oil exploration. ML for energy applications differs dramatically from consumer web applications.
A full-day, hands-on tutorial introducing Apache Spark and libraries for building workflows: Spark SQL, Spark Streaming, MLlib, GraphX, etc.
The most frustrating part of data science is when customers dont get it: endless revisions, recommendations not implemented, or data products not adopted. Exciting new research in neurology, cognitive psychology, and behavioral economics have a lot to say about why. Well explore the findings and implications for designing more successful human-data interfaces.
In far too many organizations, data scientists and designers work in silos, and quibble about whos more important. This is a huge missed opportunity. At Intuit, we are reimagining how our data and design teams to work together to fuel innovation and surpass Intuits business goals. I will walk through methods we are using to bridge these two wildly different groups and share stories of success.
Data products are poised to go mainstream, but only if they are designed well.  Most data products are designed by developers for developers. This talk discusses methods from Stanford's D.School used by companies like Yahoo!, Samsung, and Audi to design break-out products. These principles can help developers get beyond technology and design products for everyday users.
From its inception, Palantir Technologies has been about integrating the best of big data technology into systems that enable subject matter experts (as opposed to data scientists or programmers) to move through huge volumes of data and do their own data analysis. Equal parts data system design and UX, we break down the design of building systems usable by mere mortals.
Designing data visualizations presents us with unique and interesting challenges: how to tell a compelling story; how to deliver important information in a forthright, clear format; and how to make visualizations beautiful and engaging. In this talk, Julie will share a few disruptive designs and connect those back to vizipedia, her compiled data visualization library.
As Director of UX Design at software analytic company New Relic, my core focus is trying to present the over 200 billion data points across more than three million applications we monitor in a way that provides meaning so customers can make good decisions for their business. Today Ill share some of what Ive learned along the way.
Sensor devices are proliferating fast now that manufacturing price is under $100. And while more data is being generated, more is also being thrown out because of the resource gap between compute and network. We must make new computational trade-offs whilst ensuring quality. In this talk, we discuss these trade-offs and examine architectures for peer-based and crowd-sourced model generation.
Thirty years ago, data warehouses revolutionized data storage at big companies, storing summarized data in a strict structure and making it possible to efficiently analyze data. We believe that modern technology lets you adopt a simpler and more powerful scheme to organize historical data: time ordered raw event logs. In this session, we'll explain why raw data is better.
Finding anomalies is essential for a wide range of applications, including cybersecurity, event detection and health and status monitoring.   Anomaly techniques that scale successfully to large datasets tend to integrate machine learning with good data engineering.  We discuss three case studies and extract eight techniques that have proved effective for detecting anomalies in large scale systems.
The unprecedented increase in streaming data sources requires a new approach to analytical algorithms. Systems must be highly automated, adapt to changing statistics, and naturally deal with temporal data streams. They must require no batch training and should deploy custom models on the fly. It will be impossible to build scalable practical systems without these properties.
The US is in an oil boom, driven by new technologies that enable the economic production of shale resources. Conventional exploration techniques dont work well for these unconventional reserves. In this talk, Kaggles Chief Scientist will discuss Kaggles pioneering work in machine learning for oil exploration. ML for energy applications differs dramatically from consumer web applications.
Hadoop and The Internet of Things has enabled data driven companies to leverage new data sources and apply new analytical techniques in creative ways that provide competitive advantage.  We will discuss real world case studies from the field that describe the strategies, architectures, and results from forward thinking companies across a variety of verticals.
Twitter's users generate tens of billions of tweet views per day. Aggregating these events in real time - in a robust enough way to incorporate into our products - presents a massive scaling challenge. In this talk I'll introduce TSAR (the TimeSeries AggregatoR), a robust, flexible, and scalable service for real-time event aggregation designed to solve this problem and a range of similar ones
Capturing and integrating device-based and other health data for research is frustratingly difficult. We explain the open source technology framework for capturing and routing device-based health data for use by healthcare providers and for access, via a Trusted Analytic Container, to researchers we developed, working with O'Reilly and the Robert Wood Johnson Foundation.
I will introduce USAs next big astronomy project (LSST) and describe how this telescope requires massive data stream analytics  to discover and respond to exotic rapidly changing events in the Universe. I will discuss parallels between big data astronomy and Decision Science-as-a-Service for Business, Cybersecurity Information and Event Management, and Marketing Automation using Hadoop.
We often face the need to analyze the count of discrete events which occur at a specific time and place whether they be crime events, taxi requests, or phone calls.  Forecasting these space-time events brings particular challenges: finding suitable tools for geographic processing and techniques for modeling the data.  The session will cover the lessons learned in building such a system.
In this session, Apigee VP of products Anant Jhingran will discuss how the combination of APIs and data is leading to the next generation of adaptive apps.
The audience will learn about the novel ways of using ranking algorithms in intrusion detection systems, how to provide consistent security monitoring in a constantly changing environment and finally, data scientists will walk away with an actionable framework for testing their system even with the lack of labelled attack data.
LinkedIn's Security Data Science team is tasked with detecting bad activity on the LinkedIn site and building proactive solutions to keep it from happening in the first place.  In this talk we'll explore various types of abuse we see at LinkedIn and discuss some of the solutions we've built to defend against it.
This talk by Voltage Security CTO Terence Spies presents options for securing data and speeding Hadoop implementation. Attendees will leave with strategies to de-risk Hadoop implementations in multi-platform Enterprise environments.
Consumers are widely adopting wearable technology  Deloitte predicts there will be 100 million wearable cameras, smartwatches, fitness trackers and other gadgets on the market by 2020. With this mass adoption of wearable devices, comes a new data ecosystem that must be protected. Embracing the protection of this new, intricate data ecosystem is imperative to the success of wearable industry.
There is often debate in the Hadoop community of the correct hardware combination for a cluster. In this talks, attendees will learn how varying different components impacts performance and how to chose the right components for their own workloads.
Hadoop and The Internet of Things has enabled data driven companies to leverage new data sources and apply new analytical techniques in creative ways that provide competitive advantage.  We will discuss real world case studies from the field that describe the strategies, architectures, and results from forward thinking companies across a variety of verticals.
Twitter's users generate tens of billions of tweet views per day. Aggregating these events in real time - in a robust enough way to incorporate into our products - presents a massive scaling challenge. In this talk I'll introduce TSAR (the TimeSeries AggregatoR), a robust, flexible, and scalable service for real-time event aggregation designed to solve this problem and a range of similar ones
Capturing and integrating device-based and other health data for research is frustratingly difficult. We explain the open source technology framework for capturing and routing device-based health data for use by healthcare providers and for access, via a Trusted Analytic Container, to researchers we developed, working with O'Reilly and the Robert Wood Johnson Foundation.
I will introduce USAs next big astronomy project (LSST) and describe how this telescope requires massive data stream analytics  to discover and respond to exotic rapidly changing events in the Universe. I will discuss parallels between big data astronomy and Decision Science-as-a-Service for Business, Cybersecurity Information and Event Management, and Marketing Automation using Hadoop.
We often face the need to analyze the count of discrete events which occur at a specific time and place whether they be crime events, taxi requests, or phone calls.  Forecasting these space-time events brings particular challenges: finding suitable tools for geographic processing and techniques for modeling the data.  The session will cover the lessons learned in building such a system.
In this session, Apigee VP of products Anant Jhingran will discuss how the combination of APIs and data is leading to the next generation of adaptive apps.
The audience will learn about the novel ways of using ranking algorithms in intrusion detection systems, how to provide consistent security monitoring in a constantly changing environment and finally, data scientists will walk away with an actionable framework for testing their system even with the lack of labelled attack data.
LinkedIn's Security Data Science team is tasked with detecting bad activity on the LinkedIn site and building proactive solutions to keep it from happening in the first place.  In this talk we'll explore various types of abuse we see at LinkedIn and discuss some of the solutions we've built to defend against it.
This talk by Voltage Security CTO Terence Spies presents options for securing data and speeding Hadoop implementation. Attendees will leave with strategies to de-risk Hadoop implementations in multi-platform Enterprise environments.
Consumers are widely adopting wearable technology  Deloitte predicts there will be 100 million wearable cameras, smartwatches, fitness trackers and other gadgets on the market by 2020. With this mass adoption of wearable devices, comes a new data ecosystem that must be protected. Embracing the protection of this new, intricate data ecosystem is imperative to the success of wearable industry.
There is often debate in the Hadoop community of the correct hardware combination for a cluster. In this talks, attendees will learn how varying different components impacts performance and how to chose the right components for their own workloads.
What happens if you take everything that is happening in your company--every click, every impression, every database change, every application log--and make it all available as a real-time stream of well structured data? Companies such as LinkedIn have done this experiment and this talk will describe how this changes the way data is thought about and put to use in an organization.
Processing data from social media streams and sensors devices in real-time is becoming increasingly prevalent and there are plenty open source solutions to choose from. To help practitioners decide what to use when we compare three popular Apache projects allowing to do stream processing: Apache Storm, Apache Spark and Apache Samza.
Map Reduce, Millwheel and other technologies changed the way data scientists approached data problems. New technologies like Spark and Cloud Dataflow deal with the complexity of stringing together map reduces and creating end-to-end programming logic from multiple steps by making Big Data into a concrete set of executable operations. Gain insights into programming options and what comes next.
The explosion of internal data sources, data lakes (e.g., Hadoop), external public data sources, and feeds from the Internet of Things is creating a tsunami of diverse data sources for enterprises to leverage. Top-down data-integration and data-scientist tools wont scale to meet integration demands. Learn how a scalable data curation platform can help enterprises with data integration at scale.
Not all big data problems require big cluster solutions. Doradus OLAP compresses data into compact shards, yielding fast analytical queries using little disk even for big data sets. Learn how Doradus leverages OLAP techniques, columnar storage, and Cassandra to yield sophisticated query features while using amazingly little disk space.
I will talk about how Drill achieves high performance with flexibility and ease of use.  Includes:  First read planning and statistics.  Flexible code generation depending on workload. Code optimization and planning techniques. Dynamic schema subsets. Advanced memory use and moving between Java and C.  Making a static typing appear dynamic through any-time and multi-phase planning.
Spark is an open-source computation platform for Big Data. All the major Hadoop vendors have embraced Spark as a replacement for MapReduce, because Spark offers much better performance, a more powerful and productive API, and support for event processing. Spark's secrets for success are the Scala programming language and Functional Programming. We'll explore why.
Apache Spark is a popular engine for large scale analytics. This talk will give insights into tuning and debugging a production Spark deployment. It will start with details about Spark internals and an overview of the runtime behavior of a Spark application. I'll explain how to diagnose performance bottlenecks and get the best performance out of Spark jobs.
In this talk, well walk through a recent implementation at one of worlds top 5 mobile operators (a company with 300 million subscribers)
YARN and MESOS are often positioned as competitors for managing datacenter resources, but in reality they work together to seamlessly share datacenter resources. Why force IT to choose between these two great technologies, when we can show you how they work in concert.
Writing efficient Spark programs requires a deeper understanding of Spark internals.  In this talk, we present practical tips for writing better Spark programs for the beginner or intermediate Spark programmer.
What happens if you take everything that is happening in your company--every click, every impression, every database change, every application log--and make it all available as a real-time stream of well structured data? Companies such as LinkedIn have done this experiment and this talk will describe how this changes the way data is thought about and put to use in an organization.
Processing data from social media streams and sensors devices in real-time is becoming increasingly prevalent and there are plenty open source solutions to choose from. To help practitioners decide what to use when we compare three popular Apache projects allowing to do stream processing: Apache Storm, Apache Spark and Apache Samza.
Map Reduce, Millwheel and other technologies changed the way data scientists approached data problems. New technologies like Spark and Cloud Dataflow deal with the complexity of stringing together map reduces and creating end-to-end programming logic from multiple steps by making Big Data into a concrete set of executable operations. Gain insights into programming options and what comes next.
The explosion of internal data sources, data lakes (e.g., Hadoop), external public data sources, and feeds from the Internet of Things is creating a tsunami of diverse data sources for enterprises to leverage. Top-down data-integration and data-scientist tools wont scale to meet integration demands. Learn how a scalable data curation platform can help enterprises with data integration at scale.
Not all big data problems require big cluster solutions. Doradus OLAP compresses data into compact shards, yielding fast analytical queries using little disk even for big data sets. Learn how Doradus leverages OLAP techniques, columnar storage, and Cassandra to yield sophisticated query features while using amazingly little disk space.
I will talk about how Drill achieves high performance with flexibility and ease of use.  Includes:  First read planning and statistics.  Flexible code generation depending on workload. Code optimization and planning techniques. Dynamic schema subsets. Advanced memory use and moving between Java and C.  Making a static typing appear dynamic through any-time and multi-phase planning.
Spark is an open-source computation platform for Big Data. All the major Hadoop vendors have embraced Spark as a replacement for MapReduce, because Spark offers much better performance, a more powerful and productive API, and support for event processing. Spark's secrets for success are the Scala programming language and Functional Programming. We'll explore why.
Apache Spark is a popular engine for large scale analytics. This talk will give insights into tuning and debugging a production Spark deployment. It will start with details about Spark internals and an overview of the runtime behavior of a Spark application. I'll explain how to diagnose performance bottlenecks and get the best performance out of Spark jobs.
In this talk, well walk through a recent implementation at one of worlds top 5 mobile operators (a company with 300 million subscribers)
YARN and MESOS are often positioned as competitors for managing datacenter resources, but in reality they work together to seamlessly share datacenter resources. Why force IT to choose between these two great technologies, when we can show you how they work in concert.
Writing efficient Spark programs requires a deeper understanding of Spark internals.  In this talk, we present practical tips for writing better Spark programs for the beginner or intermediate Spark programmer.
Impala is the massively parallel analytic database delivering interactive performance on Hadoop. In this half-day tutorial, we'll walk you through hands-on exercises, taking you from zero to up and running with Impala.
Hadoop is emerging as the standard for big data processing & analytics. However, as usage of the Hadoop clusters grow, so do the demands of managing and monitoring these systems. In this tutorial, attendees will get an overview of all phases for successfully managing Hadoop clusters, with an emphasis on production systems.
There's many confusing and painful things about setting up and operating a 900 node Hadoop cluster used as the centerpiece in many of Spotify's Big Data initiatives, we'll go over a few interesting stories and frustrations which have influenced the direction of our architectural choices and the lessons we've learned from them.
While we frequently talk about how to build interesting products on top of machine and event data, the reality is that collecting, organizing, providing access to, and managing this data is where most people get stuck. In this session, well describe one such system, in detail, handling terabytes an hour of event-oriented data, providing real time streaming, search, and SQL access to data.
Businesses are moving from large-scale batch data analysis to large-scale real-time data analysis. Apache Storm has emerged as one of the most popular platforms for the purpose. This talk covers proven design patterns for real time stream processing. Patterns that have been vetted in large-scale production deployments that process 10s of billions of events/day and 10s of terabytes of data/day.
We will describe how we have used Storm, stream-processing and machine-learned classifiers to improve access to tickets during onsales and how this can extend to similar recipes for trend prediction and anomaly detection. We will also describe how we use tools such Kafka, Storm and Hbase to build an optimal solution for real-time n-squared marketing.
In this session, attendees will gain an understanding of the technology and processes crucial to delivering a secure platform. Additionally, theyll benefit from insights on how to ensure their organizations Hadoop environment complies with stringent security requirements. Recent implementations of compliance programs will be highlighted as part of the discussion.
FINRA, an independent regulator charged with protecting investors, processes 30 billion market events per day and analyzes the data in search of patterns that indicate possible manipulation of US financial markets. This talk provides an overview of FINRA's Big Data architecture behind that process.
The Netflix Data Platform is a constantly evolving, large scale infrastructure running in the (AWS) cloud. We are especially focused on performance and ease of use, with initiatives including Presto integration, Spark, and our Big Data Portal and API. This talk will dive into the various technologies we use, the motivations behind our approach, and the business benefits we get.
Search is more than typing words into a box. It's evolved into the backbone for todays analytics demands, and an asset for businesses to ask the right questions to make sense of their data. This session will highlight how a versatile, agile search and analytics platform can give shape to data, and uncover the "uncommonly common trends within, to make the right data-driven decisions.
With hundreds of developers from a variety of organizations participating, Hadoop moves quickly. This talk will survey the important changes admins and users should be aware of and their impacts to various use cases.
The maturation of big data technologies has enabled numerous organizations to derive insights from vast quantities of data. The next set of challenges we face involve building applications that allow us to visualize, navigate, and interpret this data. Creating intuitive user interfaces is often a cumbersome process requiring complex data transformations, integrations, and queries.
Pandora is not the Netflix for music. The success of Pandora lies in the unique combination of a curated music catalog of 1.5M+ tracks with a sophisticated set of machine learning models that integrates contextual user feedback from more than 250M people. This talk will unveil Pandoras dynamic ensemble learning approach that provides a truly personalized experience for each of our listeners.
Impala is the massively parallel analytic database delivering interactive performance on Hadoop. In this half-day tutorial, we'll walk you through hands-on exercises, taking you from zero to up and running with Impala.
Hadoop is emerging as the standard for big data processing & analytics. However, as usage of the Hadoop clusters grow, so do the demands of managing and monitoring these systems. In this tutorial, attendees will get an overview of all phases for successfully managing Hadoop clusters, with an emphasis on production systems.
There's many confusing and painful things about setting up and operating a 900 node Hadoop cluster used as the centerpiece in many of Spotify's Big Data initiatives, we'll go over a few interesting stories and frustrations which have influenced the direction of our architectural choices and the lessons we've learned from them.
While we frequently talk about how to build interesting products on top of machine and event data, the reality is that collecting, organizing, providing access to, and managing this data is where most people get stuck. In this session, well describe one such system, in detail, handling terabytes an hour of event-oriented data, providing real time streaming, search, and SQL access to data.
Businesses are moving from large-scale batch data analysis to large-scale real-time data analysis. Apache Storm has emerged as one of the most popular platforms for the purpose. This talk covers proven design patterns for real time stream processing. Patterns that have been vetted in large-scale production deployments that process 10s of billions of events/day and 10s of terabytes of data/day.
We will describe how we have used Storm, stream-processing and machine-learned classifiers to improve access to tickets during onsales and how this can extend to similar recipes for trend prediction and anomaly detection. We will also describe how we use tools such Kafka, Storm and Hbase to build an optimal solution for real-time n-squared marketing.
In this session, attendees will gain an understanding of the technology and processes crucial to delivering a secure platform. Additionally, theyll benefit from insights on how to ensure their organizations Hadoop environment complies with stringent security requirements. Recent implementations of compliance programs will be highlighted as part of the discussion.
FINRA, an independent regulator charged with protecting investors, processes 30 billion market events per day and analyzes the data in search of patterns that indicate possible manipulation of US financial markets. This talk provides an overview of FINRA's Big Data architecture behind that process.
The Netflix Data Platform is a constantly evolving, large scale infrastructure running in the (AWS) cloud. We are especially focused on performance and ease of use, with initiatives including Presto integration, Spark, and our Big Data Portal and API. This talk will dive into the various technologies we use, the motivations behind our approach, and the business benefits we get.
Search is more than typing words into a box. It's evolved into the backbone for todays analytics demands, and an asset for businesses to ask the right questions to make sense of their data. This session will highlight how a versatile, agile search and analytics platform can give shape to data, and uncover the "uncommonly common trends within, to make the right data-driven decisions.
With hundreds of developers from a variety of organizations participating, Hadoop moves quickly. This talk will survey the important changes admins and users should be aware of and their impacts to various use cases.
The maturation of big data technologies has enabled numerous organizations to derive insights from vast quantities of data. The next set of challenges we face involve building applications that allow us to visualize, navigate, and interpret this data. Creating intuitive user interfaces is often a cumbersome process requiring complex data transformations, integrations, and queries.
Pandora is not the Netflix for music. The success of Pandora lies in the unique combination of a curated music catalog of 1.5M+ tracks with a sophisticated set of machine learning models that integrates contextual user feedback from more than 250M people. This talk will unveil Pandoras dynamic ensemble learning approach that provides a truly personalized experience for each of our listeners.
Apache Cassandra has proven to be one of the best solutions for storing and retrieving time series data. Add in Apache Spark and Kafka, you have an amazing time series solution. We will talk data models, go through deployment and code to build a functional, real-time application. Languages used: Java, Scala
This tutorial will provide an introduction to the individual components of the ELK stack followed by a discussion of use cases and a hands-on lab. This includes installing and configuring Elasticsearch, Logstash, and Kibana.
The Hadoop ecosystem is a vibrant and growing set of tools for taming data at massive scales. It's also less than straightforward at times. During this talk we'll take a light-hearted and interactive plunge into the dark corners of Hadoop to shine light on some of the trap doors and blind alleys one may encounter in the wild. Attendees will leave dazed, confused, and a hopefully little wiser.
As the volume of data and number of applications moving to Apache Hadoop has increased, so has the need to secure that data and those applications. In this presentation, we'll take a brief look at where Hadoop security is today and then peer into the future.
HBase can be a good solution for hierarchical time series data. And we can access the data using both R and Python. This case study is a sanitized version of a solution we brought to a client that provided real business valuewithout requiring significant investment or time. We show how to move to a simple, scalable NoSQL solution without alienating the scientists who work with the data.
The next generation of MapReduce, YARN, has widely touted job throughput and Apache Hadoop cluster utilization benefits. Less known are the pitfalls littering the migration path to YARN. Learn from our extensive field experience to avoid those pitfalls and get your YARN cluster configured right the first time.
The good news: Hadoop has a lot of tools.  The bad news: Hadoop has a lot of tools, and conflicting priorities. This talk shows how advances in YARN and Mesos allow you to run multiple distinct workloads together.  We show how to use SLA and latency rules along with preemption in YARN to maintain high throughput while guaranteeing latency for applications such as HBase and Drill.
Hadoop has allowed us to move towards a unified source of truth for all of organization's data. Managing data location, schema knowledge and evolution, fine-grained business rules based access control, and audit and compliance needs will become critical with increasing scale of operations. In this talk, we will share an approach in tackling these challenges with a data discovery tool.
Organizations do not store, process and analyze data for their amusement. They plan to use the data to drive business decisions. If data validity is uncertain, the data is useless for decision making. In this session we will show how to design architectures that allow to prove and improve data validity at every step of the decision making process.
n this talk, attendee will learn about Impalas approach to on-the-fly, automatic data transformation, which in conjunction with the ability to handle nested structures such as JSON and XML documents, addresses the needs of at-source analytics  including direct querying of your input schema, immediate querying of data as it lands in HDFS, and high performance on par with specialized engines.
You will never look at SSDs the same after this presentation. We discuss how SSDs improve the performance of MapReduce workloads. We identify cost-per-performance as a more pertinent metric than cost-per-capacity when evaluating SSDs versus HDDs for performance, and quantify that SSDs can achieve up to 70% higher performance for 2.5x higher cost-per-performance.
Vendors and pundits suggest plug-n-play options for Hadoop security - do this and in <20 mins, your petabytes of data is now secure. What happens when PowerPoint approaches fail in a real-world enterprise  deployment? In this session, we will review techniques that worked, controls that completely failed, and create business processes we had to stand up.
Genomics applications like the Genome Analysis Toolkit (GATK) have long used techniques like MapReduce to parallelize I/O, but have never before run on Hadoop.  We will describe what we did to build an end-to-end GATK-based genome analysis  pipeline on Hadoop, show how it scaled at lower platform cost, and demonstrate the results.
Apache Cassandra has proven to be one of the best solutions for storing and retrieving time series data. Add in Apache Spark and Kafka, you have an amazing time series solution. We will talk data models, go through deployment and code to build a functional, real-time application. Languages used: Java, Scala
This tutorial will provide an introduction to the individual components of the ELK stack followed by a discussion of use cases and a hands-on lab. This includes installing and configuring Elasticsearch, Logstash, and Kibana.
The Hadoop ecosystem is a vibrant and growing set of tools for taming data at massive scales. It's also less than straightforward at times. During this talk we'll take a light-hearted and interactive plunge into the dark corners of Hadoop to shine light on some of the trap doors and blind alleys one may encounter in the wild. Attendees will leave dazed, confused, and a hopefully little wiser.
As the volume of data and number of applications moving to Apache Hadoop has increased, so has the need to secure that data and those applications. In this presentation, we'll take a brief look at where Hadoop security is today and then peer into the future.
HBase can be a good solution for hierarchical time series data. And we can access the data using both R and Python. This case study is a sanitized version of a solution we brought to a client that provided real business valuewithout requiring significant investment or time. We show how to move to a simple, scalable NoSQL solution without alienating the scientists who work with the data.
The next generation of MapReduce, YARN, has widely touted job throughput and Apache Hadoop cluster utilization benefits. Less known are the pitfalls littering the migration path to YARN. Learn from our extensive field experience to avoid those pitfalls and get your YARN cluster configured right the first time.
The good news: Hadoop has a lot of tools.  The bad news: Hadoop has a lot of tools, and conflicting priorities. This talk shows how advances in YARN and Mesos allow you to run multiple distinct workloads together.  We show how to use SLA and latency rules along with preemption in YARN to maintain high throughput while guaranteeing latency for applications such as HBase and Drill.
Hadoop has allowed us to move towards a unified source of truth for all of organization's data. Managing data location, schema knowledge and evolution, fine-grained business rules based access control, and audit and compliance needs will become critical with increasing scale of operations. In this talk, we will share an approach in tackling these challenges with a data discovery tool.
Organizations do not store, process and analyze data for their amusement. They plan to use the data to drive business decisions. If data validity is uncertain, the data is useless for decision making. In this session we will show how to design architectures that allow to prove and improve data validity at every step of the decision making process.
n this talk, attendee will learn about Impalas approach to on-the-fly, automatic data transformation, which in conjunction with the ability to handle nested structures such as JSON and XML documents, addresses the needs of at-source analytics  including direct querying of your input schema, immediate querying of data as it lands in HDFS, and high performance on par with specialized engines.
You will never look at SSDs the same after this presentation. We discuss how SSDs improve the performance of MapReduce workloads. We identify cost-per-performance as a more pertinent metric than cost-per-capacity when evaluating SSDs versus HDDs for performance, and quantify that SSDs can achieve up to 70% higher performance for 2.5x higher cost-per-performance.
Vendors and pundits suggest plug-n-play options for Hadoop security - do this and in <20 mins, your petabytes of data is now secure. What happens when PowerPoint approaches fail in a real-world enterprise  deployment? In this session, we will review techniques that worked, controls that completely failed, and create business processes we had to stand up.
Genomics applications like the Genome Analysis Toolkit (GATK) have long used techniques like MapReduce to parallelize I/O, but have never before run on Hadoop.  We will describe what we did to build an end-to-end GATK-based genome analysis  pipeline on Hadoop, show how it scaled at lower platform cost, and demonstrate the results.
The best insight you produce is only as good as your ability to explain it. As data scientists and engineers, our task is not only to execute robust analyses, but also to convince decision-makers to act on data. Through an example-driven approach, attendees will examine features of great graphics, techniques of effective visualization, and learn to use D3.js to create their own data narrative.
What are the essential components of a data platform? This tutorial will explain how the various parts of the Hadoop and big data ecosystem fit together in production to create a data platform supporting batch, interactive and realtime analytical workloads.
Spark users have been pushing the boundary of data analytics. In this talk, we focus on the scalability dimension, including: - Multiple real-world use cases on PBs of data and on clusters with thousands of machines - Configuration and performance tuning tips learned from these deployments - Changes in recent releases of Spark for better handling of these workloads
Hive is Hadoop's de facto standard SQL on big data, and Spark is gaining significant momentum as a new data processing platform beyond MapReduce. The marriage of the two will generate more waves of momentum in both communities.
Getting the full value from data often requires the combination of stream processing on new events combined with large scale historical analysis. While both these activities are served by Sparks execution framework, leveraging multiple persistence layers is key to efficiently and extensibly enabling these use cases.
Spark Streaming extends Apache Spark to do large scale stream processing. In this talk, I am going to explain about how Spark Streaming is revolutionizing the way Big "Streaming" Data applications are being written, and making it as easy as 1-2-3.
In this talk, we introduce the general data architecture of Tencent with a focus on our Spark use cases on a GAIA (our improved resource manager based on YARN) cluster of 8000+ nodes. We contrast Spark with the previous MapReduce use cases, followed by tuning methods and optimizations for large scale clusters.
H2O's powerful Machine Learning algorithms coupled with Spark's SQL and scala data munging, a potent combination solving real-world problems.
YARN has featured in the marketing of Hadoop distributions for the past 2 years. All the major vendors now ship production versions. What is the real world state of deployment? What does it let you do? What are the limitations? In this talk we review three distinct deployments look at benefits and challenges, and highlights lessons for those considering to take the plunge.
In this from-the-trenches, DevOps-focused talk we explore operational issues in running Hadoop on top of Docker containers in a production, multi-tenant setup. With Hadoop's native Docker support still in the works and Docker being more of a development tool, a production deployment of the two together is like swimming in treacherous waters... Here's a lantern and a lifeboat to the rescue.
Parquet is a columnar format designed to be efficient and interoperable across the hadoop ecosystem. Its integration in most processing frameworks and serialization models makes it easy to use in existing ETL and processing pipelines, while giving flexibility of choice on the query engine.
Starting in Hive 0.14, insert values, update, and delete have been added to Hive SQL.  In addition, ACID compliant transactions have been added so that users get a consistent view of data while reading and writing.  This talk will cover the intended use cases, architecture, and performance of insert, update, and delete in Hive.
Once just the realm of Java jockeys and data scientists, Hadoop has become a mainstream tool for business analysts with the rapid proliferation of SQL-on-Hadoop solutions. But there are pitfalls that can plague implementations as IT teams get their first exposure to production Hadoop environments. Well discuss the most common pitfalls companies face and how to get around them.
The best insight you produce is only as good as your ability to explain it. As data scientists and engineers, our task is not only to execute robust analyses, but also to convince decision-makers to act on data. Through an example-driven approach, attendees will examine features of great graphics, techniques of effective visualization, and learn to use D3.js to create their own data narrative.
What are the essential components of a data platform? This tutorial will explain how the various parts of the Hadoop and big data ecosystem fit together in production to create a data platform supporting batch, interactive and realtime analytical workloads.
Spark users have been pushing the boundary of data analytics. In this talk, we focus on the scalability dimension, including: - Multiple real-world use cases on PBs of data and on clusters with thousands of machines - Configuration and performance tuning tips learned from these deployments - Changes in recent releases of Spark for better handling of these workloads
Hive is Hadoop's de facto standard SQL on big data, and Spark is gaining significant momentum as a new data processing platform beyond MapReduce. The marriage of the two will generate more waves of momentum in both communities.
Getting the full value from data often requires the combination of stream processing on new events combined with large scale historical analysis. While both these activities are served by Sparks execution framework, leveraging multiple persistence layers is key to efficiently and extensibly enabling these use cases.
Spark Streaming extends Apache Spark to do large scale stream processing. In this talk, I am going to explain about how Spark Streaming is revolutionizing the way Big "Streaming" Data applications are being written, and making it as easy as 1-2-3.
In this talk, we introduce the general data architecture of Tencent with a focus on our Spark use cases on a GAIA (our improved resource manager based on YARN) cluster of 8000+ nodes. We contrast Spark with the previous MapReduce use cases, followed by tuning methods and optimizations for large scale clusters.
H2O's powerful Machine Learning algorithms coupled with Spark's SQL and scala data munging, a potent combination solving real-world problems.
YARN has featured in the marketing of Hadoop distributions for the past 2 years. All the major vendors now ship production versions. What is the real world state of deployment? What does it let you do? What are the limitations? In this talk we review three distinct deployments look at benefits and challenges, and highlights lessons for those considering to take the plunge.
In this from-the-trenches, DevOps-focused talk we explore operational issues in running Hadoop on top of Docker containers in a production, multi-tenant setup. With Hadoop's native Docker support still in the works and Docker being more of a development tool, a production deployment of the two together is like swimming in treacherous waters... Here's a lantern and a lifeboat to the rescue.
Parquet is a columnar format designed to be efficient and interoperable across the hadoop ecosystem. Its integration in most processing frameworks and serialization models makes it easy to use in existing ETL and processing pipelines, while giving flexibility of choice on the query engine.
Starting in Hive 0.14, insert values, update, and delete have been added to Hive SQL.  In addition, ACID compliant transactions have been added so that users get a consistent view of data while reading and writing.  This talk will cover the intended use cases, architecture, and performance of insert, update, and delete in Hive.
Once just the realm of Java jockeys and data scientists, Hadoop has become a mainstream tool for business analysts with the rapid proliferation of SQL-on-Hadoop solutions. But there are pitfalls that can plague implementations as IT teams get their first exposure to production Hadoop environments. Well discuss the most common pitfalls companies face and how to get around them.
This all-day, hands-on training program provides a quick start to building and deploying predictive applications at scale.  You will learn simple and effective ways of building powerful machine learning models and deployment them.  We will walk you through all the steps of prototyping and production: data cleaning, feature engineerings, model building and evaluation, and deployment.
This session will cover approaches to building real-time pipelines with MemSQL, Hadoop, and Spark, including: How Novus built the premier financial portfolio management platform using MemSQL as a real-time data store and query engine Introduction to the MemSQL Spark connector Strategies for integrating Spark and Hadoop with real-time systems for transaction processing and operational analytics
With a global team of 30 Data Scientists doing innovative work in almost every vertical market, Pivotal has a rich view into the trends impacting enterprises and their approach to Big Data.
Learn how SAS applications use YARN in order to be a good citizen in a busy Hadoop cluster. Best practices and customer examples for several different user scenarios will be shared and discussed.
One of the first industries to invest heavily in Big Data analytics was financial services, where firms have been pushing the boundaries on speed and scale in dynamically processing large volumes of structured market data for the past twenty years to gain competitive advantage. . .
In this session youll learn about how to apply Data Discovery and Deep Data Storage for new breakthroughs in data warehousing. Well discuss the benefits of using Hadoop technologies like Spark, Kafka, and Hive together with enterprise information architecture and data governance best practices.
This session uses actual case studies to illustrate how organizations are innovating, changing and growing their businesses with Big Data.  The presentation will discuss the data requirements and the front end analytic applications used to deliver game changing Big Data initiatives. . .
This session will outline strategies for cost effectively turning enormous streams of Social Data into intelligence for use in any application.
This session covers a number of these challenges that Hadoop presents to efficient query processing and discussed a number of the novel approaches that modern SQL-on-Hadoop solutions take in order to overcome these hurdles.
This all-day, hands-on training program provides a quick start to building and deploying predictive applications at scale.  You will learn simple and effective ways of building powerful machine learning models and deployment them.  We will walk you through all the steps of prototyping and production: data cleaning, feature engineerings, model building and evaluation, and deployment.
This session will cover approaches to building real-time pipelines with MemSQL, Hadoop, and Spark, including: How Novus built the premier financial portfolio management platform using MemSQL as a real-time data store and query engine Introduction to the MemSQL Spark connector Strategies for integrating Spark and Hadoop with real-time systems for transaction processing and operational analytics
With a global team of 30 Data Scientists doing innovative work in almost every vertical market, Pivotal has a rich view into the trends impacting enterprises and their approach to Big Data.
Learn how SAS applications use YARN in order to be a good citizen in a busy Hadoop cluster. Best practices and customer examples for several different user scenarios will be shared and discussed.
One of the first industries to invest heavily in Big Data analytics was financial services, where firms have been pushing the boundaries on speed and scale in dynamically processing large volumes of structured market data for the past twenty years to gain competitive advantage. . .
In this session youll learn about how to apply Data Discovery and Deep Data Storage for new breakthroughs in data warehousing. Well discuss the benefits of using Hadoop technologies like Spark, Kafka, and Hive together with enterprise information architecture and data governance best practices.
This session uses actual case studies to illustrate how organizations are innovating, changing and growing their businesses with Big Data.  The presentation will discuss the data requirements and the front end analytic applications used to deliver game changing Big Data initiatives. . .
This session will outline strategies for cost effectively turning enormous streams of Social Data into intelligence for use in any application.
This session covers a number of these challenges that Hadoop presents to efficient query processing and discussed a number of the novel approaches that modern SQL-on-Hadoop solutions take in order to overcome these hurdles.
In this session, we will show you how easy it is to spin up a 32 node Storm cluster and give all attendees a free unlimited 30-day pass to deploy your own Hadoop cluster on Microsoft Azure.
Join this session to hear about lessons learnt in building these domain specific solutions, Intels reference architecture for data science and analytics services deployment in the cloud, and the new Intel initiative to advance the state of art in big data analytics on Hadoop and Spark.
Tasked with improving engagement and data integrity with emphasis on a self-serve framework, Sears Hometown and Outlet (SHO) forged ahead along their journey in Big Data. With the help of Pentaho and CapGemini, SHO has transitioned from costly and rigid legacy systems to a dynamic, company owned/managed system. . .
As IT and big data/analytics investments increase, so do data silos. To get full value from these investments, businesses must embrace the variety of data silos - now. Current top-down approaches are tapped out. Innovations in data unification can overcome silos virtually, delivering 360-degree views of customers, long-tail opportunities in supply chains, and other business opportunities.
Learn how Hadoop 2.0 and its YARN architecture can make a serious impact on the previously intractable problem of data quality and serve as a super-charged marshaling area for accessing, cleansing and delivering high-quality data
In this session, GoPro discusses their process for transforming the extreme volume and variety of datasets landing in GoPros data lake into usable formats for analysis tools or predictive modeling demands improving their ability to overcome the current technical and human bottlenecks that typically limit the productivity of these efforts.
In this lively, technical session, Nitesh Ambastha, Global Head of Data IT, Private Banking & Wealth Management Products at Credit Suisse talks about what his organization demands from vendors who sell data preparation, data quality and governance technologies.
 During this session learn how you can rapidly deploy a modern data platform and watch a live demo that highlights how our easy-to-use control panels and APIs with simple bridges allow you to manage, integrate, and gain insights from your data environments in minutes.
This talk will address an emerging problem in the Modern Enterprise Data Landscape and a possible realization of a "Smart Enterprise Big Data Bus" using an open source stack including Apache Kafka and Apache Storm.
In this session you will hear from big data expert, Clint Sharp, with real world experience on the architectural patterns and platform integrations used to solve real business problems with data.
In this session, we will show you how easy it is to spin up a 32 node Storm cluster and give all attendees a free unlimited 30-day pass to deploy your own Hadoop cluster on Microsoft Azure.
Join this session to hear about lessons learnt in building these domain specific solutions, Intels reference architecture for data science and analytics services deployment in the cloud, and the new Intel initiative to advance the state of art in big data analytics on Hadoop and Spark.
Tasked with improving engagement and data integrity with emphasis on a self-serve framework, Sears Hometown and Outlet (SHO) forged ahead along their journey in Big Data. With the help of Pentaho and CapGemini, SHO has transitioned from costly and rigid legacy systems to a dynamic, company owned/managed system. . .
As IT and big data/analytics investments increase, so do data silos. To get full value from these investments, businesses must embrace the variety of data silos - now. Current top-down approaches are tapped out. Innovations in data unification can overcome silos virtually, delivering 360-degree views of customers, long-tail opportunities in supply chains, and other business opportunities.
Learn how Hadoop 2.0 and its YARN architecture can make a serious impact on the previously intractable problem of data quality and serve as a super-charged marshaling area for accessing, cleansing and delivering high-quality data
In this session, GoPro discusses their process for transforming the extreme volume and variety of datasets landing in GoPros data lake into usable formats for analysis tools or predictive modeling demands improving their ability to overcome the current technical and human bottlenecks that typically limit the productivity of these efforts.
In this lively, technical session, Nitesh Ambastha, Global Head of Data IT, Private Banking & Wealth Management Products at Credit Suisse talks about what his organization demands from vendors who sell data preparation, data quality and governance technologies.
 During this session learn how you can rapidly deploy a modern data platform and watch a live demo that highlights how our easy-to-use control panels and APIs with simple bridges allow you to manage, integrate, and gain insights from your data environments in minutes.
This talk will address an emerging problem in the Modern Enterprise Data Landscape and a possible realization of a "Smart Enterprise Big Data Bus" using an open source stack including Apache Kafka and Apache Storm.
In this session you will hear from big data expert, Clint Sharp, with real world experience on the architectural patterns and platform integrations used to solve real business problems with data.
Are you looking for a deeper understanding of how to integrate components in the Apache Hadoop ecosystem to implement data management and processing solutions? Then this tutorial is for you. We'll provide a clickstream analytics example illustrating how to architect solutions with Apache Hadoop along with providing best practices and recommendations for using Hadoop and related tools.
In the second (afternoon) half of the Architecture Day tutorial, attendees will apply the best practices they learned in the morning session to build a data application for sessionizing user data.
Whats important about a technology is what you can use it to do. Weve looked at what a number of groups are doing with Apache Hadoop and NoSQL in production, and wed like to relay what worked well for them and what did not. . .
R has emerged as the language of data science.  In this session, IBM will discuss and demonstrate Big R, a comprehensive set of capabilities that provides end-to-end integration with open source R, transparent execution on Hadoop, and seamless access to machine learning algorithms based on SystemML. Learn also about how Big R and Spark can be used with new geo-spatial and text analytic tooling.
Hadoop is now widely used to support mission-critical applications that operate within a data lake infrastructure, but how can it overcome complete data center failures to guarantee continuous operation? In this session, we lay out the blueprint for a multi-data center Hadoop that solves the storage and compute problems in operating over the WAN using single coordinated, Paxos-based file system.
This talk discuss how to do realtime analytics with a SQL like query language. We will discuss role of Complex Event Processing in realtime analytics, and then discuss a scalable CEP engine that let users write their queries using declarative SQL like CEP query language, but let them execute those queries  using a graph of CEP nodes deployed on top of Apache Storm
CIOs and business executives alike are looking for ways to mine the potential value of their customer, product and operational data as they consider where and how to start their Big Data journey. What are the organizational ramifications of big data? How can CIOs foster a culture of data-driven decision-making? How can the data lake play support an organizations business transformation efforts?
Organizations are experiencing unprecedented complexity in managing their data, with the rise of Big Data, Cloud and overall hyper connectivity of our world. Cisco is building solutions to help our customers adopt Big Data solutions, solve business problems using Analytics, and harness the power of an intelligent infrastructure to provide highly differentiated Data and Analytics solutions.
Join us as we explore the big data services of AWS and watch a speaker-led tutorial and a link to a lab in which you can take with you.
Armed with just a browser, data scientists can develop sophisticated machine learning models, and deploy them in a few clicks in cloud-hosted APIs that can be called from any device. The APIs scale elastically to power high volume intelligent apps for phones, websites and the internet of things. . .
Tens of thousands of simultaneous game players generate lots of data. For online game-maker Kixeye, that data provides insights that drive decisions about game play and monetization. In this session Kixeye and Snowflake will discuss Snowflakes data warehouse cloud service and how Kixeye uses it to get data insight with the performance, elasticity, and flexibility made possible by the cloud.
Are you looking for a deeper understanding of how to integrate components in the Apache Hadoop ecosystem to implement data management and processing solutions? Then this tutorial is for you. We'll provide a clickstream analytics example illustrating how to architect solutions with Apache Hadoop along with providing best practices and recommendations for using Hadoop and related tools.
In the second (afternoon) half of the Architecture Day tutorial, attendees will apply the best practices they learned in the morning session to build a data application for sessionizing user data.
Whats important about a technology is what you can use it to do. Weve looked at what a number of groups are doing with Apache Hadoop and NoSQL in production, and wed like to relay what worked well for them and what did not. . .
R has emerged as the language of data science.  In this session, IBM will discuss and demonstrate Big R, a comprehensive set of capabilities that provides end-to-end integration with open source R, transparent execution on Hadoop, and seamless access to machine learning algorithms based on SystemML. Learn also about how Big R and Spark can be used with new geo-spatial and text analytic tooling.
Hadoop is now widely used to support mission-critical applications that operate within a data lake infrastructure, but how can it overcome complete data center failures to guarantee continuous operation? In this session, we lay out the blueprint for a multi-data center Hadoop that solves the storage and compute problems in operating over the WAN using single coordinated, Paxos-based file system.
This talk discuss how to do realtime analytics with a SQL like query language. We will discuss role of Complex Event Processing in realtime analytics, and then discuss a scalable CEP engine that let users write their queries using declarative SQL like CEP query language, but let them execute those queries  using a graph of CEP nodes deployed on top of Apache Storm
CIOs and business executives alike are looking for ways to mine the potential value of their customer, product and operational data as they consider where and how to start their Big Data journey. What are the organizational ramifications of big data? How can CIOs foster a culture of data-driven decision-making? How can the data lake play support an organizations business transformation efforts?
Organizations are experiencing unprecedented complexity in managing their data, with the rise of Big Data, Cloud and overall hyper connectivity of our world. Cisco is building solutions to help our customers adopt Big Data solutions, solve business problems using Analytics, and harness the power of an intelligent infrastructure to provide highly differentiated Data and Analytics solutions.
Join us as we explore the big data services of AWS and watch a speaker-led tutorial and a link to a lab in which you can take with you.
Armed with just a browser, data scientists can develop sophisticated machine learning models, and deploy them in a few clicks in cloud-hosted APIs that can be called from any device. The APIs scale elastically to power high volume intelligent apps for phones, websites and the internet of things. . .
Tens of thousands of simultaneous game players generate lots of data. For online game-maker Kixeye, that data provides insights that drive decisions about game play and monetization. In this session Kixeye and Snowflake will discuss Snowflakes data warehouse cloud service and how Kixeye uses it to get data insight with the performance, elasticity, and flexibility made possible by the cloud.
HP will discuss two innovations to help you take on analytics for your Hadoop Data.
This presentation will highlight why the concept of the "data lake" is a game changing paradigm. Attendees will examine data lake architecture and tradeoffs in construction and operations in data lake design as seen from Think Big consulting engagements.
In this session you will hear of some of the fascinating use cases for SQL in Hadoop based on real-world customer examples. You will learn some of the innovative techniques that have emerged to overcome limitations of the Hadoop platform that enable features one expects in a proven mature database.
Justin Michaels of Couchbase will provide an overview of the use case and review how this is handled within Couchbase  while providing real-time access to user data. Matt Ingenthron of Couchbase will talk about key features of the underlying components to enable processing at the scale required by deployments such as AT&T and PayPal.
In this session Greg will share his insights on this gap from his years of experience in the visual exploratory data discovery & advanced analytics space working with customers and most of the major players in the Big and small data management ecosystem.
PostgreSQL has recently become the most popular database for technology companies. In part, it owes this success to rethinking the monolithic SQL database, and providing an extensible architecture instead. In this talk, we will describe key challenges associated with scaling out SQL. We will then show PostgreSQL extensions that overcome these challenges, and describe how they do so.
Allstates foundation is data.  We extract value from our data by applying machine learning to make data-driven decisions. In this session, we discuss Allstates drive for better business results by using machine learning on Hadoop.
This session will examine Hive performance past, present and future.
HP will discuss two innovations to help you take on analytics for your Hadoop Data.
This presentation will highlight why the concept of the "data lake" is a game changing paradigm. Attendees will examine data lake architecture and tradeoffs in construction and operations in data lake design as seen from Think Big consulting engagements.
In this session you will hear of some of the fascinating use cases for SQL in Hadoop based on real-world customer examples. You will learn some of the innovative techniques that have emerged to overcome limitations of the Hadoop platform that enable features one expects in a proven mature database.
Justin Michaels of Couchbase will provide an overview of the use case and review how this is handled within Couchbase  while providing real-time access to user data. Matt Ingenthron of Couchbase will talk about key features of the underlying components to enable processing at the scale required by deployments such as AT&T and PayPal.
In this session Greg will share his insights on this gap from his years of experience in the visual exploratory data discovery & advanced analytics space working with customers and most of the major players in the Big and small data management ecosystem.
PostgreSQL has recently become the most popular database for technology companies. In part, it owes this success to rethinking the monolithic SQL database, and providing an extensible architecture instead. In this talk, we will describe key challenges associated with scaling out SQL. We will then show PostgreSQL extensions that overcome these challenges, and describe how they do so.
Allstates foundation is data.  We extract value from our data by applying machine learning to make data-driven decisions. In this session, we discuss Allstates drive for better business results by using machine learning on Hadoop.
This session will examine Hive performance past, present and future.
Learn to develop machine learning, exploratory and predictive models at scale on data stored in-memory. This hands-on course will address exploratory statistical modeling with SAS Visual Statistics,  a GUI designed for rapidly screening models and segments.
Learn to develop machine learning, exploratory and predictive models at scale on data stored in-memory. This hands-on course will address exploratory statistical modeling with SAS Visual Statistics,  a GUI designed for rapidly screening models and segments.
Join the Spark team for an informal question and answer session. Several of the Spark committers, trainers, etc., from Databricks will be on hand to field a wide range of detailed questions. Even if you don't have a specific question, join in to hear what others are asking.
What does successful big data and data science really look like? As consultants out in the field, we've learned a lot of lessons and have great stories to tell about success, failure, and how to negotiate a path through a fast-moving technology landscape.
Join the presenters of the PyData Tutorials for further discussions on some of the most used tools in the Python data stack. This is a great opportunity to ask questions and share  insight with those who have authored or contributed to: * scikit-learn * NumPy * Bokeh * IPython * Numba * Blaze * pandas * scikit-image
Join the authors of Hadoop Application Architectures for an open Q/A session on considerations and recommendations for architecture and design of applications using Hadoop. Talk to us about your use-case and its big data architecture, or just come to listen in.
Learn to develop machine learning, exploratory and predictive models at scale on data stored in-memory. This hands-on course will address exploratory statistical modeling with SAS Visual Statistics,  a GUI designed for rapidly screening models and segments.
Learn to develop machine learning, exploratory and predictive models at scale on data stored in-memory. This hands-on course will address exploratory statistical modeling with SAS Visual Statistics,  a GUI designed for rapidly screening models and segments.
Join the Spark team for an informal question and answer session. Several of the Spark committers, trainers, etc., from Databricks will be on hand to field a wide range of detailed questions. Even if you don't have a specific question, join in to hear what others are asking.
What does successful big data and data science really look like? As consultants out in the field, we've learned a lot of lessons and have great stories to tell about success, failure, and how to negotiate a path through a fast-moving technology landscape.
Join the presenters of the PyData Tutorials for further discussions on some of the most used tools in the Python data stack. This is a great opportunity to ask questions and share  insight with those who have authored or contributed to: * scikit-learn * NumPy * Bokeh * IPython * Numba * Blaze * pandas * scikit-image
Join the authors of Hadoop Application Architectures for an open Q/A session on considerations and recommendations for architecture and design of applications using Hadoop. Talk to us about your use-case and its big data architecture, or just come to listen in.
This three-day curriculum features advanced lectures and hands-on technical exercises for advanced Spark usage in data exploration, analysis, and building Big Data applications. Course materials emphasize architectural design patterns and best practices for leveraging Spark in the context of other popular, complementary frameworks for building and managing Enterprise data workflows.
This three-day curriculum features advanced lectures and hands-on technical exercises for advanced Spark usage in data exploration, analysis, and building Big Data applications. Course materials emphasize architectural design patterns and best practices for leveraging Spark in the context of other popular, complementary frameworks for building and managing Enterprise data workflows.
This three-day curriculum features advanced lectures and hands-on technical exercises for advanced Spark usage in data exploration, analysis, and building Big Data applications. Course materials emphasize architectural design patterns and best practices for leveraging Spark in the context of other popular, complementary frameworks for building and managing Enterprise data workflows.
This three-day curriculum features advanced lectures and hands-on technical exercises for advanced Spark usage in data exploration, analysis, and building Big Data applications. Course materials emphasize architectural design patterns and best practices for leveraging Spark in the context of other popular, complementary frameworks for building and managing Enterprise data workflows.
This three-day curriculum features advanced lectures and hands-on technical exercises for advanced Spark usage in data exploration, analysis, and building Big Data applications. Course materials emphasize architectural design patterns and best practices for leveraging Spark in the context of other popular, complementary frameworks for building and managing Enterprise data workflows.
This three-day curriculum features advanced lectures and hands-on technical exercises for advanced Spark usage in data exploration, analysis, and building Big Data applications. Course materials emphasize architectural design patterns and best practices for leveraging Spark in the context of other popular, complementary frameworks for building and managing Enterprise data workflows.
Cloudera Universitys four-day course for designing and building Big Data applications prepares you to analyze and solve real-world problems using Apache Hadoop and associated tools in the enterprise data hub.
Cloudera Universitys four-day course for designing and building Big Data applications prepares you to analyze and solve real-world problems using Apache Hadoop and associated tools in the enterprise data hub.
Cloudera Universitys four-day course for designing and building Big Data applications prepares you to analyze and solve real-world problems using Apache Hadoop and associated tools in the enterprise data hub.
Cloudera Universitys four-day course for designing and building Big Data applications prepares you to analyze and solve real-world problems using Apache Hadoop and associated tools in the enterprise data hub.
Cloudera Universitys four-day course for designing and building Big Data applications prepares you to analyze and solve real-world problems using Apache Hadoop and associated tools in the enterprise data hub.
Cloudera Universitys four-day course for designing and building Big Data applications prepares you to analyze and solve real-world problems using Apache Hadoop and associated tools in the enterprise data hub.
Cloudera Universitys four-day course for designing and building Big Data applications prepares you to analyze and solve real-world problems using Apache Hadoop and associated tools in the enterprise data hub.
Cloudera Universitys four-day course for designing and building Big Data applications prepares you to analyze and solve real-world problems using Apache Hadoop and associated tools in the enterprise data hub.
Program Chairs Roger Magoulas, Doug Cutting, and Alistair Croll welcome you to the first day of Strata + Hadoop World Keynotes.
As Hadoop and the surrounding projects & vendors mature, their impact on the data management sector is growing. Amr will talk about his views on how that impact will change over the next five years. How central will Hadoop be to the data center of 2020? What industries will benefit most? Which technologies are at risk of displacement or encroachment?
MemSQL CEO Eric Frenkiel will discuss the need for simplicity in enterprise data architecture, the convergence of transactions and analytics, and what is required to operationalize Spark and Hadoop in the enterprise.pipelines by integrating their technology with Hadoop, and Spark.
Wearables contribute to Big Data and the insights are already realizing significant gains in key industries.
To get value out of todays big and fast data, organizations must evolve beyond traditional analytic cycles that are heavy with data transformation and schema management. . .
In a landmark partnership, IBM and Twitter are combining advances in analytics, cloud and cognitive computing in a manner that has the potential to transform how institutions understand customers, markets and trends. Adam Kocoloski, CTO of IBM Cloud Data Services and co-founder of Cloudant will explain how when it comes to gaining insights from Big Data, the future is brighter than we know.
Data Science, where are we going? What impact can we expect?
Advances in data science empower leaders to make better decisions for society. By using new kinds of information unavailable during the last several millennia of government, we can avoid mistakes of the past. We will discuss how data and statistical inference are informing how we manage the global climate rationally, a defining policy challenge for our generation.
Our experience of the sensory world does not need to be constrained by our physical limitations. When navigating the environment our senses interact to perceive a robust non-veridical experience. Understanding these interactions and being able to define them perceptually and algorithmically allows technological developments that can facilitate sensory enhancement and optimization.
Program Chairs, Roger Magoulas, Doug Cutting, and Alistair Croll, welcome you to the second day of Strata + Hadoop World keynotes.
Open data is quickly gaining momentum and when applied as data for good, it becomes a much more powerful concept that we should all consider as good data stewards. Organizations to cities are starting to share data like traffic conditions or climate sensors and allowing others to use this open data to improve quality of life.
 Roughly every decade, some kind of military or enterprise technology makes its way into the mainstream: the personal computer; the consumer Internet; the mobile phone; the Internet of Things. What happens when Big Data turns into a consumer product? Strata chair Alistair Croll offers some speculation about what data will do to the way we live, love, work, and play.
The exponential growth of digitally stored data and the transition of data science from academia to real world applications hold the promise of improving nearly every aspect of our lives.
We are often told that past holds lessons on how to approach the present, but we rarely look to older technologies for inspiration. Rarer still do we look at the historical experiences of less industrialized nations to teach us about the technological problems of today.
As the Apache Spark userbase grows, the developer community is working to adapt it for ever-wider use cases. 2014 saw fast adoption of Spark in the enterprise and major improvements in its performance, scalability and standard libraries.
In the wake of the Open Data Platform initiative announced earlier this week, Roman Shaposhnik, Director of Open Source strategy at Pivotal and a VP of Apache Software Foundation Incubator will talk about how a well-defined, fully validated ODP common core  platform is going to address  some of the biggest customer pain points around rapid evolution and standardization in the big data area
Join Microsofts Joseph Sirosh for a surprising conversation about a farmer's dilemma, a professor's ingenuity and how cloud, data and devices came together to fundamentally re-imagine an age old way of doing business.
Keynote with Jeffrey Heer, Co-Founder, Trifacta
Program Chairs Roger Magoulas, Doug Cutting, and Alistair Croll welcome you to the first day of Strata + Hadoop World Keynotes.
As Hadoop and the surrounding projects & vendors mature, their impact on the data management sector is growing. Amr will talk about his views on how that impact will change over the next five years. How central will Hadoop be to the data center of 2020? What industries will benefit most? Which technologies are at risk of displacement or encroachment?
MemSQL CEO Eric Frenkiel will discuss the need for simplicity in enterprise data architecture, the convergence of transactions and analytics, and what is required to operationalize Spark and Hadoop in the enterprise.pipelines by integrating their technology with Hadoop, and Spark.
Wearables contribute to Big Data and the insights are already realizing significant gains in key industries.
To get value out of todays big and fast data, organizations must evolve beyond traditional analytic cycles that are heavy with data transformation and schema management. . .
In a landmark partnership, IBM and Twitter are combining advances in analytics, cloud and cognitive computing in a manner that has the potential to transform how institutions understand customers, markets and trends. Adam Kocoloski, CTO of IBM Cloud Data Services and co-founder of Cloudant will explain how when it comes to gaining insights from Big Data, the future is brighter than we know.
Data Science, where are we going? What impact can we expect?
Advances in data science empower leaders to make better decisions for society. By using new kinds of information unavailable during the last several millennia of government, we can avoid mistakes of the past. We will discuss how data and statistical inference are informing how we manage the global climate rationally, a defining policy challenge for our generation.
Our experience of the sensory world does not need to be constrained by our physical limitations. When navigating the environment our senses interact to perceive a robust non-veridical experience. Understanding these interactions and being able to define them perceptually and algorithmically allows technological developments that can facilitate sensory enhancement and optimization.
Program Chairs, Roger Magoulas, Doug Cutting, and Alistair Croll, welcome you to the second day of Strata + Hadoop World keynotes.
Open data is quickly gaining momentum and when applied as data for good, it becomes a much more powerful concept that we should all consider as good data stewards. Organizations to cities are starting to share data like traffic conditions or climate sensors and allowing others to use this open data to improve quality of life.
 Roughly every decade, some kind of military or enterprise technology makes its way into the mainstream: the personal computer; the consumer Internet; the mobile phone; the Internet of Things. What happens when Big Data turns into a consumer product? Strata chair Alistair Croll offers some speculation about what data will do to the way we live, love, work, and play.
The exponential growth of digitally stored data and the transition of data science from academia to real world applications hold the promise of improving nearly every aspect of our lives.
We are often told that past holds lessons on how to approach the present, but we rarely look to older technologies for inspiration. Rarer still do we look at the historical experiences of less industrialized nations to teach us about the technological problems of today.
As the Apache Spark userbase grows, the developer community is working to adapt it for ever-wider use cases. 2014 saw fast adoption of Spark in the enterprise and major improvements in its performance, scalability and standard libraries.
In the wake of the Open Data Platform initiative announced earlier this week, Roman Shaposhnik, Director of Open Source strategy at Pivotal and a VP of Apache Software Foundation Incubator will talk about how a well-defined, fully validated ODP common core  platform is going to address  some of the biggest customer pain points around rapid evolution and standardization in the big data area
Join Microsofts Joseph Sirosh for a surprising conversation about a farmer's dilemma, a professor's ingenuity and how cloud, data and devices came together to fundamentally re-imagine an age old way of doing business.
Keynote with Jeffrey Heer, Co-Founder, Trifacta
For CIOs, IT executives, and technology professionals, Strata's Enterprise Big Data day lays out the roadmap to get your organization up to speed on big data. In this all-day event, hear how to create a big data strategy, understand the issues of managing data, and learn how data science can be used powerfully in your organization.
AMPLabs open source data analysis projects, Spark and Shark, deliver iterative queries up to 100x faster than Hadoop MapReduce.  Hear how companies are using Spark-based data platforms for fast, interactive analysis on big data.
This session is an overview of Apache Drill, another big data system inspired by a Google white paper.
In many modern web and big data applications the data arrives in a streaming fashion and needs to be processed on the fly. Due to the size of data, the computations need to be done incrementally, and hence sketches of data are used that take a small amount of memory but allow for fast updates and queries. We will present the techniques to design these sketches and provide clarifying examples.
With the growth in volume and velocity of data, businesses need a scalable solution alongside batch processing to process events on the fly and provide real time insights. In this session, we will describe how we used Storm to analyze network data to detect causes of network performance degradation.
While the industry has been busy abandoning the relational database and calling it a fundamentally limited technology, several trends are conspiring to revive the good old RDBMS.  While it might not resemble the MySQL or Oracle database you are running today, this talk will explore how hardware trends, software trends, and industry research are point to SQL, structure, and ACID at scale.
This talk introduces an open-source distributed file system that will double the capacity of your Hadoop cluster and speed up your MapReduce jobs. The talk will describe the Reed-Solomon implementation and its implications for cluster performance, how it leverages the speed of modern networks to achieve better storage efficiency and make Hadoop jobs run faster.
Many of the services that are critical to Googles ad business have historically been backed by MySQL. We have recently migrated several of these services to F1, a new RDBMS developed at Google. F1 implements rich relational database features, including a strictly enforced schema, a powerful parallel SQL query engine, general transactions, change tracking and notication, and indexing.
In this talk, we describe using Redis, an open source, in-memory key-value store, to capture large volumes of data from numerous remote sources while also allowing real-time monitoring and analytics. With this approach, we were able to capture a high volume of continuous data from numerous remote environmental sensors while consistently querying our database for real time monitoring and analytics.
This talk discusses the market needs that are giving birth to the "scientific database", what these systems have to offer that is currently lacking in either the data management or statistical worlds, and how scientific databases will co-exist and co-evolve with Hadoop and other leading big data platforms.
The Cloudera Impala project is for the first time making scalable parallel database technology, which is the underpinning of Google's Dremel as well as that of commercial analytic DBMSs, available to the Hadoop community.
This talk will discuss how Druid allows users to have interactive queries on real-time data at scale; we feature a case study with Netflix leveraging Druid to obtain at-the-moment insight as it ingests over two terabytes per hour.
For CIOs, IT executives, and technology professionals, Strata's Enterprise Big Data day lays out the roadmap to get your organization up to speed on big data. In this all-day event, hear how to create a big data strategy, understand the issues of managing data, and learn how data science can be used powerfully in your organization.
AMPLabs open source data analysis projects, Spark and Shark, deliver iterative queries up to 100x faster than Hadoop MapReduce.  Hear how companies are using Spark-based data platforms for fast, interactive analysis on big data.
This session is an overview of Apache Drill, another big data system inspired by a Google white paper.
In many modern web and big data applications the data arrives in a streaming fashion and needs to be processed on the fly. Due to the size of data, the computations need to be done incrementally, and hence sketches of data are used that take a small amount of memory but allow for fast updates and queries. We will present the techniques to design these sketches and provide clarifying examples.
With the growth in volume and velocity of data, businesses need a scalable solution alongside batch processing to process events on the fly and provide real time insights. In this session, we will describe how we used Storm to analyze network data to detect causes of network performance degradation.
While the industry has been busy abandoning the relational database and calling it a fundamentally limited technology, several trends are conspiring to revive the good old RDBMS.  While it might not resemble the MySQL or Oracle database you are running today, this talk will explore how hardware trends, software trends, and industry research are point to SQL, structure, and ACID at scale.
This talk introduces an open-source distributed file system that will double the capacity of your Hadoop cluster and speed up your MapReduce jobs. The talk will describe the Reed-Solomon implementation and its implications for cluster performance, how it leverages the speed of modern networks to achieve better storage efficiency and make Hadoop jobs run faster.
Many of the services that are critical to Googles ad business have historically been backed by MySQL. We have recently migrated several of these services to F1, a new RDBMS developed at Google. F1 implements rich relational database features, including a strictly enforced schema, a powerful parallel SQL query engine, general transactions, change tracking and notication, and indexing.
In this talk, we describe using Redis, an open source, in-memory key-value store, to capture large volumes of data from numerous remote sources while also allowing real-time monitoring and analytics. With this approach, we were able to capture a high volume of continuous data from numerous remote environmental sensors while consistently querying our database for real time monitoring and analytics.
This talk discusses the market needs that are giving birth to the "scientific database", what these systems have to offer that is currently lacking in either the data management or statistical worlds, and how scientific databases will co-exist and co-evolve with Hadoop and other leading big data platforms.
The Cloudera Impala project is for the first time making scalable parallel database technology, which is the underpinning of Google's Dremel as well as that of commercial analytic DBMSs, available to the Hadoop community.
This talk will discuss how Druid allows users to have interactive queries on real-time data at scale; we feature a case study with Netflix leveraging Druid to obtain at-the-moment insight as it ingests over two terabytes per hour.
For CIOs, IT executives, and technology professionals, Strata's Enterprise Big Data day lays out the roadmap to get your organization up to speed on big data. In this all-day event, hear how to create a big data strategy, understand the issues of managing data, and learn how data science can be used powerfully in your organization.
AMPLabs open source data analysis projects, Spark and Shark, deliver iterative queries up to 100x faster than Hadoop MapReduce.  Hear how companies are using Spark-based data platforms for fast, interactive analysis on big data.
This session is an overview of Apache Drill, another big data system inspired by a Google white paper.
In many modern web and big data applications the data arrives in a streaming fashion and needs to be processed on the fly. Due to the size of data, the computations need to be done incrementally, and hence sketches of data are used that take a small amount of memory but allow for fast updates and queries. We will present the techniques to design these sketches and provide clarifying examples.
With the growth in volume and velocity of data, businesses need a scalable solution alongside batch processing to process events on the fly and provide real time insights. In this session, we will describe how we used Storm to analyze network data to detect causes of network performance degradation.
While the industry has been busy abandoning the relational database and calling it a fundamentally limited technology, several trends are conspiring to revive the good old RDBMS.  While it might not resemble the MySQL or Oracle database you are running today, this talk will explore how hardware trends, software trends, and industry research are point to SQL, structure, and ACID at scale.
This talk introduces an open-source distributed file system that will double the capacity of your Hadoop cluster and speed up your MapReduce jobs. The talk will describe the Reed-Solomon implementation and its implications for cluster performance, how it leverages the speed of modern networks to achieve better storage efficiency and make Hadoop jobs run faster.
Many of the services that are critical to Googles ad business have historically been backed by MySQL. We have recently migrated several of these services to F1, a new RDBMS developed at Google. F1 implements rich relational database features, including a strictly enforced schema, a powerful parallel SQL query engine, general transactions, change tracking and notication, and indexing.
In this talk, we describe using Redis, an open source, in-memory key-value store, to capture large volumes of data from numerous remote sources while also allowing real-time monitoring and analytics. With this approach, we were able to capture a high volume of continuous data from numerous remote environmental sensors while consistently querying our database for real time monitoring and analytics.
This talk discusses the market needs that are giving birth to the "scientific database", what these systems have to offer that is currently lacking in either the data management or statistical worlds, and how scientific databases will co-exist and co-evolve with Hadoop and other leading big data platforms.
The Cloudera Impala project is for the first time making scalable parallel database technology, which is the underpinning of Google's Dremel as well as that of commercial analytic DBMSs, available to the Hadoop community.
This talk will discuss how Druid allows users to have interactive queries on real-time data at scale; we feature a case study with Netflix leveraging Druid to obtain at-the-moment insight as it ingests over two terabytes per hour.
HBase is one of the more popular open source NoSQL databases that have cropped up over the last few years. Building applications that use HBase effectively is challenging. This tutorial is geared towards teaching the basics of building applications using HBase and covers concepts that a developer should know while using HBase as a backend store for their application.
An introduction to D3, one of the most powerful Javascript data visualization libraries.
Opower, the global leader in the field of energy information and analysis, works with 80 utility companies worldwide to give families context, insights, and advice about how to save energy. With access to an unprecedented (and still growing) amount of energy datacurrently drawn from 50 million US homesOpower is uncovering unique trends in how people are using energy at home.
As an ecommerce site with more than 800,000 different sellers, Etsy is particularly interested in understanding how shoppers find the items they seek.  This talk will discuss the challenges of funnel analysis at Etsy, the corresponding deficiencies of several widely used web analytics tools, and our event sequence matching tool implemented in Hadoop.
The Infrastructure team at Stumbleupon leverages the state of the art tools and technologies to build platforms that enable us collect, categorize, organize, store and analyze huge volumes of data. The platform is fast and robust that it adds minimal latency to the site.Timely collection and analysis of data helps data scientists, analysts and executives make the best decisions and validate them.
Big Data is about more than petabytes; it is also about new paradigms, languages, and tools.  This talk will cover work going on in Hadoop projects to coordinate sharing of data and user code between tools.
In this session well first discuss our experience extending Hadoop development to new platforms & languages and then discuss our experiments and experiences building supporting developer tools and plugins for those platforms.
All is quiet on the log file front, but yet the system is down.  What next? Three parts practical know-how (heres my toolbox) and one part position paper (must-haves for comprehensibility), this talk will cover the tricks of the trade for debugging distributed systems. Motivated by experience gained diagnosing Hadoop, well dig into the JVM, Linux esoterica, and outlier visualization.
Join Flip Kromer, co-founder and CTO of Infochimps, as he walks you through a series of decision trees, making you rethink your use of Hadoop in the cloud and opening up possibilities for new patterns of work that are uniquely developer-friendly. Patterns of work like tuning your cluster to the job, and why the first priority of any analytics cluster should be downtime.
Apache Hadoop is an innovative emerging technology causing CIOs to rethink their data architecture - making this an exciting time to be a big data technologist.  This tag-team presentation brings leaders in both Apache Hadoop and data warehousing on the stage, to answer these questions by sharing their vision for the future of big data management and analytics.
This talk examines the notion of a "workflow" as a general abstraction for common use cases encountered in Data Science, particularly for building Enterprise apps. Patterns of workflows provide recipes for integrating different frameworks, plus the means for optimizing large-scale apps. We review this approach in the context of a sample app based on the Cascading open source project.
We will describe the BigData Top100 List initiativean new, open, community-based effort for benchmarking big data systems.
This talks dives into the extreme details of Building Recommendation Platforms. It covers the end to end Architecture and Design of such a system. It dives into the various ML Algorithms to be used along with their details. It also covers the Solutions to commonly seen Recommendation Patterns and detailed Use Cases along with their Solution.
HBase is one of the more popular open source NoSQL databases that have cropped up over the last few years. Building applications that use HBase effectively is challenging. This tutorial is geared towards teaching the basics of building applications using HBase and covers concepts that a developer should know while using HBase as a backend store for their application.
An introduction to D3, one of the most powerful Javascript data visualization libraries.
Opower, the global leader in the field of energy information and analysis, works with 80 utility companies worldwide to give families context, insights, and advice about how to save energy. With access to an unprecedented (and still growing) amount of energy datacurrently drawn from 50 million US homesOpower is uncovering unique trends in how people are using energy at home.
As an ecommerce site with more than 800,000 different sellers, Etsy is particularly interested in understanding how shoppers find the items they seek.  This talk will discuss the challenges of funnel analysis at Etsy, the corresponding deficiencies of several widely used web analytics tools, and our event sequence matching tool implemented in Hadoop.
The Infrastructure team at Stumbleupon leverages the state of the art tools and technologies to build platforms that enable us collect, categorize, organize, store and analyze huge volumes of data. The platform is fast and robust that it adds minimal latency to the site.Timely collection and analysis of data helps data scientists, analysts and executives make the best decisions and validate them.
Big Data is about more than petabytes; it is also about new paradigms, languages, and tools.  This talk will cover work going on in Hadoop projects to coordinate sharing of data and user code between tools.
In this session well first discuss our experience extending Hadoop development to new platforms & languages and then discuss our experiments and experiences building supporting developer tools and plugins for those platforms.
All is quiet on the log file front, but yet the system is down.  What next? Three parts practical know-how (heres my toolbox) and one part position paper (must-haves for comprehensibility), this talk will cover the tricks of the trade for debugging distributed systems. Motivated by experience gained diagnosing Hadoop, well dig into the JVM, Linux esoterica, and outlier visualization.
Join Flip Kromer, co-founder and CTO of Infochimps, as he walks you through a series of decision trees, making you rethink your use of Hadoop in the cloud and opening up possibilities for new patterns of work that are uniquely developer-friendly. Patterns of work like tuning your cluster to the job, and why the first priority of any analytics cluster should be downtime.
Apache Hadoop is an innovative emerging technology causing CIOs to rethink their data architecture - making this an exciting time to be a big data technologist.  This tag-team presentation brings leaders in both Apache Hadoop and data warehousing on the stage, to answer these questions by sharing their vision for the future of big data management and analytics.
This talk examines the notion of a "workflow" as a general abstraction for common use cases encountered in Data Science, particularly for building Enterprise apps. Patterns of workflows provide recipes for integrating different frameworks, plus the means for optimizing large-scale apps. We review this approach in the context of a sample app based on the Cascading open source project.
We will describe the BigData Top100 List initiativean new, open, community-based effort for benchmarking big data systems.
This talks dives into the extreme details of Building Recommendation Platforms. It covers the end to end Architecture and Design of such a system. It dives into the various ML Algorithms to be used along with their details. It also covers the Solutions to commonly seen Recommendation Patterns and detailed Use Cases along with their Solution.
For business strategists, marketers, product managers, and entrepreneurs, Data Driven Business looks at how to use data to make better business decisions faster. Packed with case studies, panels, and eye-opening presentations, this fast-paced day focuses on how to solve today's thorniest business problems with Big Data. It's the missing MBA for a data-driven, always-on business world.
While many libraries are available today to help create interactive visualizations, they are generally not integrated with the data analysis tool chain. This talk will focus on how to combine agile data manipulations with web-based visualization libraries to create a more efficient workflow for data science.
Visualization is a powerful way to understand data, but today building the right data set and accompanying data visualization requires sophisticated programming skills. We discuss an approach to a unified language describing both visualization and database queries. This approach could be used by both programmers and business users, accelerating data exploration and speeding time to insight.
Whether the user is a business user or an IT user, with today's data complexity, there are a number of design principles that are key to achieving success. Hear how to approach product designing for today's data challenges and meet new user expectations for fast and timely insights at scale.
This session explores applications of Shneidermans mantra for visual data analysis (overview first, zoom and filter, then details-on-demand) as a framework in the context of three complex analytical applications at Wells Fargo:  (1) Analytics process, (2) Interactive meeting facilitation and (3) Dashboard design.
From markup languages like SVG to OpenGL based APIs like WebGL, the browser provides several ways for creating visualizations. In this talk we'll show some web based visualizations we worked on for different projects and for Twitter, and show what standards were used to create them.  We'll dissect each example showing what was used not only for rendering but also for data handling and interaction.
The human eye can detect infinitesimal patterns in the world around us. Shouldnt we make use of this amazing skill when recognizing patterns or detecting anomalies in big data? In this session well explore why rendering every pixel is a challenge with big data and look at how these limitations can be overcome.
The majority of data we consume today are presented in lists, one-dimensional orderings that limit the users ability to understand context or perform strategic analyses.  For unstructured data, we need to re-imagine what types of visualisations enable exploration in the way that geographic maps can.
This talk discusses the broad design considerations necessary for effective visualizations. Attendees will learn what's required for a visualization to be successful, gain insight for critically evaluating visualizations they encounter, and come away with new ways to think about the visualization design process.
The majority of the world's data is now unstructured, non-English text. How can we extract useful information from it? Many of our assumptions about English do no carry over to other languages. This talk will give a high-level overview of how languages vary, what current language technologies can (and cannot) achieve, and how we can process and visualize this information at scale.
Crunch 40 years worth of daily global satellite data at the push of a button, perform spatial analyses on GBs of your own GIS data and securely share the results privately or publish to 1B Google Earth users. This talk will focus on how what was once the realm of a few is now easily and intuitively accessible from the comfort of your Chrome browser.
The Great Debate series returns to Strata. In this Oxford-style debate, two opposing teams take opposing positions. We poll the audience, and the teams try to sway opinions. It'll be a fast-paced, sometimes irreverent look at some of the core challenges of putting data to work.
For business strategists, marketers, product managers, and entrepreneurs, Data Driven Business looks at how to use data to make better business decisions faster. Packed with case studies, panels, and eye-opening presentations, this fast-paced day focuses on how to solve today's thorniest business problems with Big Data. It's the missing MBA for a data-driven, always-on business world.
While many libraries are available today to help create interactive visualizations, they are generally not integrated with the data analysis tool chain. This talk will focus on how to combine agile data manipulations with web-based visualization libraries to create a more efficient workflow for data science.
Visualization is a powerful way to understand data, but today building the right data set and accompanying data visualization requires sophisticated programming skills. We discuss an approach to a unified language describing both visualization and database queries. This approach could be used by both programmers and business users, accelerating data exploration and speeding time to insight.
Whether the user is a business user or an IT user, with today's data complexity, there are a number of design principles that are key to achieving success. Hear how to approach product designing for today's data challenges and meet new user expectations for fast and timely insights at scale.
This session explores applications of Shneidermans mantra for visual data analysis (overview first, zoom and filter, then details-on-demand) as a framework in the context of three complex analytical applications at Wells Fargo:  (1) Analytics process, (2) Interactive meeting facilitation and (3) Dashboard design.
From markup languages like SVG to OpenGL based APIs like WebGL, the browser provides several ways for creating visualizations. In this talk we'll show some web based visualizations we worked on for different projects and for Twitter, and show what standards were used to create them.  We'll dissect each example showing what was used not only for rendering but also for data handling and interaction.
The human eye can detect infinitesimal patterns in the world around us. Shouldnt we make use of this amazing skill when recognizing patterns or detecting anomalies in big data? In this session well explore why rendering every pixel is a challenge with big data and look at how these limitations can be overcome.
The majority of data we consume today are presented in lists, one-dimensional orderings that limit the users ability to understand context or perform strategic analyses.  For unstructured data, we need to re-imagine what types of visualisations enable exploration in the way that geographic maps can.
This talk discusses the broad design considerations necessary for effective visualizations. Attendees will learn what's required for a visualization to be successful, gain insight for critically evaluating visualizations they encounter, and come away with new ways to think about the visualization design process.
The majority of the world's data is now unstructured, non-English text. How can we extract useful information from it? Many of our assumptions about English do no carry over to other languages. This talk will give a high-level overview of how languages vary, what current language technologies can (and cannot) achieve, and how we can process and visualize this information at scale.
Crunch 40 years worth of daily global satellite data at the push of a button, perform spatial analyses on GBs of your own GIS data and securely share the results privately or publish to 1B Google Earth users. This talk will focus on how what was once the realm of a few is now easily and intuitively accessible from the comfort of your Chrome browser.
The Great Debate series returns to Strata. In this Oxford-style debate, two opposing teams take opposing positions. We poll the audience, and the teams try to sway opinions. It'll be a fast-paced, sometimes irreverent look at some of the core challenges of putting data to work.
This hands-on tutorial teaches you how to use Hive, a high-level, data warehouse tool for Hadoop. Hive provides a SQL-like query language, HiveQL, that is easy to learn for people with prior SQL experience, making Hive attractive for data warehousing teams. Hive leverages the power of Hadoop for working with massive data sets without requiring expertise in MapReduce programming.
This hands-on session will show how a dataset turns into a story, the narrative process the Guardian's team goes through, the tools used and the lessons learned.
In today's world, decisions are made for us based on data. On one hand, this is appealing, but on the other hand its disorienting. To address this, designers need to focus on the things that make us uniquely human and focus on the translation between the abstract and human. This presentation will look at the ways humans make decisions and how big data and technology can enable this, not lead it.
When a data scientist crosses over to the dark side, look out. High-quality spam, large-scale CAPTCHA-breaking, impolite spiders, oh my! This talk will explore attack vectors that can be exploited by black-hat data scientists. We'll also discuss countermeasures and defenses that are available to the good guys, and assess their effectiveness.
From politicians to marketers everyone tries to influence. Data analytics of traditional as well as social media data has made it easier to spot deliberate attempts to skew the public opinion. The talk will give insights into new measurements by analyzing large events such as the London Olympics. Those measures will help to disguise the more and more sophisticated attempts of fake influence.
This session will demonstrate to attendees how easy it is to crowdsource identity theft to commit fraud and make money. We will look at which segments of the population are easy targets for large scale identity fraud. Attendees will be given methodologies to combat this type of fraud leveraging Big Data and various technologies.
At Strata 2012 in New York, we discussed the hazards of curbing big data inferences by defining a new category of thoughtcrime. After all, acting on thoughts might constitute a crime, but thoughts, in isolation, cannot be criminal. It's time to go deeper. Let's create and evaluate a predictive criminal model that highlights where the sensitivities lie, both technically and ethically.
Privacy laws as to a companys obligations on data collection, use, disclosure are changing rapidly. Failing to understand how the laws affect a companys personal data assets can result in media exposes, regulatory investigations, Congressional hearings and lawsuits. This session will provide guidance on privacy by design compliance and practical tips to avoid becoming a target of scrutiny.
At Strata RX, we announced the release of DocGraph, the largest open named social graph data set that we know of. This data set included links between doctor who commonly team together in the Medicare dataset.  Since then, we have added tremendous depth to the data by crowdfunding the acquisition of doctor credentialing data. Come learn how healthcare works under the cover.
Hear from MailChimps Chief Scientist John Foreman as he dishes on dirty data and demonstrates the latest in MailChimps anti-abuse artificial intelligence. MailChimp sends 3 billion emails a month for their millions of users, and they can't afford to let a drop of spam go out. Learn how the company is using cutting edge NoSQL solutions and predictive models to leave the bad guys out in the cold.
A way to introduce the idea that access to Big Data in many countries  especially Argentina  is still a work in progress and somewhat politicized. Despite that, media like La Nacion Newspaper, are working with developers and experts in Data Viz to address the lack of transparency and accountability.
Big data tools made it possible to gain extremely valuable insight from large scale analysis of web data, but until recently few people had access to the data. Now tools like Grep the Web and increased raw access to web data grant anyone the power to do such analysis. This presentation addresses practical applications of web data analysis that you can incorporate into your research or products.
Electronic discovery has transformed the way cases are litigated. Gone are the days of manual review, where litigators spent days poring over emails, messages, and documents. Today's e-discovery technologies mine through vast troves of information, looking for the needle in the proverbial haystack that will blow a case wide open.
This hands-on tutorial teaches you how to use Hive, a high-level, data warehouse tool for Hadoop. Hive provides a SQL-like query language, HiveQL, that is easy to learn for people with prior SQL experience, making Hive attractive for data warehousing teams. Hive leverages the power of Hadoop for working with massive data sets without requiring expertise in MapReduce programming.
This hands-on session will show how a dataset turns into a story, the narrative process the Guardian's team goes through, the tools used and the lessons learned.
In today's world, decisions are made for us based on data. On one hand, this is appealing, but on the other hand its disorienting. To address this, designers need to focus on the things that make us uniquely human and focus on the translation between the abstract and human. This presentation will look at the ways humans make decisions and how big data and technology can enable this, not lead it.
When a data scientist crosses over to the dark side, look out. High-quality spam, large-scale CAPTCHA-breaking, impolite spiders, oh my! This talk will explore attack vectors that can be exploited by black-hat data scientists. We'll also discuss countermeasures and defenses that are available to the good guys, and assess their effectiveness.
From politicians to marketers everyone tries to influence. Data analytics of traditional as well as social media data has made it easier to spot deliberate attempts to skew the public opinion. The talk will give insights into new measurements by analyzing large events such as the London Olympics. Those measures will help to disguise the more and more sophisticated attempts of fake influence.
This session will demonstrate to attendees how easy it is to crowdsource identity theft to commit fraud and make money. We will look at which segments of the population are easy targets for large scale identity fraud. Attendees will be given methodologies to combat this type of fraud leveraging Big Data and various technologies.
At Strata 2012 in New York, we discussed the hazards of curbing big data inferences by defining a new category of thoughtcrime. After all, acting on thoughts might constitute a crime, but thoughts, in isolation, cannot be criminal. It's time to go deeper. Let's create and evaluate a predictive criminal model that highlights where the sensitivities lie, both technically and ethically.
Privacy laws as to a companys obligations on data collection, use, disclosure are changing rapidly. Failing to understand how the laws affect a companys personal data assets can result in media exposes, regulatory investigations, Congressional hearings and lawsuits. This session will provide guidance on privacy by design compliance and practical tips to avoid becoming a target of scrutiny.
At Strata RX, we announced the release of DocGraph, the largest open named social graph data set that we know of. This data set included links between doctor who commonly team together in the Medicare dataset.  Since then, we have added tremendous depth to the data by crowdfunding the acquisition of doctor credentialing data. Come learn how healthcare works under the cover.
Hear from MailChimps Chief Scientist John Foreman as he dishes on dirty data and demonstrates the latest in MailChimps anti-abuse artificial intelligence. MailChimp sends 3 billion emails a month for their millions of users, and they can't afford to let a drop of spam go out. Learn how the company is using cutting edge NoSQL solutions and predictive models to leave the bad guys out in the cold.
A way to introduce the idea that access to Big Data in many countries  especially Argentina  is still a work in progress and somewhat politicized. Despite that, media like La Nacion Newspaper, are working with developers and experts in Data Viz to address the lack of transparency and accountability.
Big data tools made it possible to gain extremely valuable insight from large scale analysis of web data, but until recently few people had access to the data. Now tools like Grep the Web and increased raw access to web data grant anyone the power to do such analysis. This presentation addresses practical applications of web data analysis that you can incorporate into your research or products.
Electronic discovery has transformed the way cases are litigated. Gone are the days of manual review, where litigators spent days poring over emails, messages, and documents. Today's e-discovery technologies mine through vast troves of information, looking for the needle in the proverbial haystack that will blow a case wide open.
Learn how to wrangle data in R: from acquiring and cleaning data, to changing data formats and performing targeted, groupwise calculations. This course will emphasize the 'reshape2' and 'plyr' packages.
This tutorial will be a hands-on introduction to the essential tools for working with structured data in Python, 'pandas' and 'NumPy'
Location Intelligence (LI) transforms how public health and agriculture initiatives are managed and monitored by translating big complex data from multiple sources and varying temporal and spatial scales into local, actionable insight. This empowers national governments and global development organizations to focus on saving lives and building healthy, sustainable communities.
Prepare for the coming zombie apocalypse or subjugation by our vampire overlords by tracking the spread of these threats and understand the characteristics of the populations already infected using a combination of social media analytics and classic market research cluster analysis.  Learn about new methods for unpacking consumer conversations and tracking true attitudinal consumer segments.
Hadoop is great for analyzing data at rest.  But what if your business problem requires the ability to analyze and respond in real-time and without a human in the loop?
The world of mapping is undergoing another revolution. New techniques for visualizing and querying increasingly large amounts of data can lead to new ways of interacting with and discovering meaning in your data. In this session, we'll talk about the latest in vector mapping and how you can use it to explore the hidden stories in your data.
Dealing with the flood of data that confronts researchers is the fundamental challenge of 21st century research. Citizen Science has allowed researchers within the Zooniverse to take on research problems at a scale impossible without the attention of a large community of volunteers.
Factual believes that some data problems are bigger than any one company.  This talk describes how Factual combines both machines and other (human) data communities to their best effect, within the context of similar data-centric, community-driven applications.
This talk will discuss Rest Devices proprietary low-cost sensor technology, its use of and vision for big biometric data, and the need for design integration in all facets of product development, be it software or hardware.
I will discuss how a wearable sensing platform, the Sociometric Badge, allows us to measure and analyze human behavior in the real-world, particularly in the workplace.  Well discuss how we use the badges to recognize concepts such as persuasiveness and social support and how we have used the badges in real companies to drive organizational change and put hard numbers behind management methods.
While audience analysis is an old topic, it is being reimagined as personas along topic distributions as opposed to the usual demographic terms.  This provides deeper insights into the communities among the internet that provide interesting insights into how the internet is consumed.
Learn how LinkedIn endorsements used data mining techniques to develop a viral social tagging and reputation system.
Today's smartphones have evolved into incredibly rich sensing and computing devices, that can be used to infer complex and interesting things about us, our environment, and our communities. This talk will give an overview of user-centric, continuous mobile sensing, and our work, originating at the MIT Media Lab, to develop open tools to democratize this capability.
Learn how to wrangle data in R: from acquiring and cleaning data, to changing data formats and performing targeted, groupwise calculations. This course will emphasize the 'reshape2' and 'plyr' packages.
This tutorial will be a hands-on introduction to the essential tools for working with structured data in Python, 'pandas' and 'NumPy'
Location Intelligence (LI) transforms how public health and agriculture initiatives are managed and monitored by translating big complex data from multiple sources and varying temporal and spatial scales into local, actionable insight. This empowers national governments and global development organizations to focus on saving lives and building healthy, sustainable communities.
Prepare for the coming zombie apocalypse or subjugation by our vampire overlords by tracking the spread of these threats and understand the characteristics of the populations already infected using a combination of social media analytics and classic market research cluster analysis.  Learn about new methods for unpacking consumer conversations and tracking true attitudinal consumer segments.
Hadoop is great for analyzing data at rest.  But what if your business problem requires the ability to analyze and respond in real-time and without a human in the loop?
The world of mapping is undergoing another revolution. New techniques for visualizing and querying increasingly large amounts of data can lead to new ways of interacting with and discovering meaning in your data. In this session, we'll talk about the latest in vector mapping and how you can use it to explore the hidden stories in your data.
Dealing with the flood of data that confronts researchers is the fundamental challenge of 21st century research. Citizen Science has allowed researchers within the Zooniverse to take on research problems at a scale impossible without the attention of a large community of volunteers.
Factual believes that some data problems are bigger than any one company.  This talk describes how Factual combines both machines and other (human) data communities to their best effect, within the context of similar data-centric, community-driven applications.
This talk will discuss Rest Devices proprietary low-cost sensor technology, its use of and vision for big biometric data, and the need for design integration in all facets of product development, be it software or hardware.
I will discuss how a wearable sensing platform, the Sociometric Badge, allows us to measure and analyze human behavior in the real-world, particularly in the workplace.  Well discuss how we use the badges to recognize concepts such as persuasiveness and social support and how we have used the badges in real companies to drive organizational change and put hard numbers behind management methods.
While audience analysis is an old topic, it is being reimagined as personas along topic distributions as opposed to the usual demographic terms.  This provides deeper insights into the communities among the internet that provide interesting insights into how the internet is consumed.
Learn how LinkedIn endorsements used data mining techniques to develop a viral social tagging and reputation system.
Today's smartphones have evolved into incredibly rich sensing and computing devices, that can be used to infer complex and interesting things about us, our environment, and our communities. This talk will give an overview of user-centric, continuous mobile sensing, and our work, originating at the MIT Media Lab, to develop open tools to democratize this capability.
As big data makes inroads into all aspects of society, how governments regard the technology will be critical for its success. If the past is a guide, the state will embrace big data for its own uses (both good and ill). It will recognize that its authority is threatened and lash out
Strata Program Chairs, Edd Dumbill and Alistair Croll, welcome you to the first day of keynotes.
In this talk, EA CTO Rajat Taneja will dive in to the challenges and complexities facing the gaming industry, how to harness the power of data and share examples of how technologies like machine learning and predictive analytics have been put in place to improve the customer experience.
Hadoop is the engine powering the Big Data era, an unstoppable force boasting massive investments and a rich ecosystem. But this is only the beginning: Hadoop has the potential to reach beyond Big Data and become the Foundation for Change, catalyzing new levels of business productivity and transformation. Hadoop will become the Foundation for Change.
Many companies have figured out how to generate incremental value through the use of recommendation engines.  As such, the underlying algorithms are considered a valuable asset.   But what happens when a companys entire business model rests on its ability to get relevant products in front of the customer? When this happens you see a massive commitment to algorithms, data, and data scientists.
Data science for consumer internet products relies on our ability to effectively analyze and understand ubiquitous computing in terms of a holistic product experience, as individuals consume and create data on mobile and desktop devices in their day-to-day lives.  I'll talk about mobile data science challenges   from product development to data-driven decision making.
The excitement about Big Data stems from the results: the impact on revenue, the decrease in costs, the Big gains in competitive advantage that result from Hadoop and HBase applications.  This keynote provides insights into how the combination of scale, efficiency and analytic flexibility creates the power to expand the applications for Hadoop to transform companies as well as entire industries.
More than ever before, students are using the Internet to study, leaving behind a trail of valuable data. How can we leverage this data to improve education?
Code for America fellows have been tackling not only the promise of data in Americas cities, but the reality of the challenges, for the past two years. In February 2013, six new fellows will be working on our hardest problem yet: using data to unclog the criminal justice system in Louisville and New York City. If the public sector can innovate using data, and results benefit us all.
Program Chairs, Edd Dumbill and Alistair Croll, welcome you to the second day of keynotes.
In this key note, we will explore some of the challenges of big data operating in a truly global context.
Microsoft keynote, featuring Dave Campbell, Vice President of Product Development for the SQL Server product suite.
In this talk, we present the broad data challenge and discuss  potential starting points for solutions.  We illustrate these approaches using data from a "meta-catalog" of over 1,000,000 open datasets that have been collected from about two hundred governments from around the world.
How software can transform human lives by bringing intelligence to wherever big data lives.
Hadoop and SAP HANA are taking the world by storm. SAP HANA is the fastest growing commercial database in the market, being adopted by the worlds top enterprises for real-time analytics and applications.
Big data gives us a powerful new way to see patterns in information - but what can't we see? When does big data not tell us the whole story? This talk opens up the question of the biases we bring to big data, and how we might work beyond them.
Designing for human fault-tolerance leads to important conclusions on the fundamental ways data systems should be architected.
The Victory Lab presents a secret history of modern American politics, pulling back the curtain on the tactics and strategies used by some of the era's most important figures-including Barack Obama and Mitt Romney-with iconoclastic insights into human decision-making, marketing and how analytics can put any business on the road to victory.
How must big companies evolve in order to realize big value from big data? Investing in data, technology and data scientists is just a first step.
As big data makes inroads into all aspects of society, how governments regard the technology will be critical for its success. If the past is a guide, the state will embrace big data for its own uses (both good and ill). It will recognize that its authority is threatened and lash out
Strata Program Chairs, Edd Dumbill and Alistair Croll, welcome you to the first day of keynotes.
In this talk, EA CTO Rajat Taneja will dive in to the challenges and complexities facing the gaming industry, how to harness the power of data and share examples of how technologies like machine learning and predictive analytics have been put in place to improve the customer experience.
Hadoop is the engine powering the Big Data era, an unstoppable force boasting massive investments and a rich ecosystem. But this is only the beginning: Hadoop has the potential to reach beyond Big Data and become the Foundation for Change, catalyzing new levels of business productivity and transformation. Hadoop will become the Foundation for Change.
Many companies have figured out how to generate incremental value through the use of recommendation engines.  As such, the underlying algorithms are considered a valuable asset.   But what happens when a companys entire business model rests on its ability to get relevant products in front of the customer? When this happens you see a massive commitment to algorithms, data, and data scientists.
Data science for consumer internet products relies on our ability to effectively analyze and understand ubiquitous computing in terms of a holistic product experience, as individuals consume and create data on mobile and desktop devices in their day-to-day lives.  I'll talk about mobile data science challenges   from product development to data-driven decision making.
The excitement about Big Data stems from the results: the impact on revenue, the decrease in costs, the Big gains in competitive advantage that result from Hadoop and HBase applications.  This keynote provides insights into how the combination of scale, efficiency and analytic flexibility creates the power to expand the applications for Hadoop to transform companies as well as entire industries.
More than ever before, students are using the Internet to study, leaving behind a trail of valuable data. How can we leverage this data to improve education?
Code for America fellows have been tackling not only the promise of data in Americas cities, but the reality of the challenges, for the past two years. In February 2013, six new fellows will be working on our hardest problem yet: using data to unclog the criminal justice system in Louisville and New York City. If the public sector can innovate using data, and results benefit us all.
Program Chairs, Edd Dumbill and Alistair Croll, welcome you to the second day of keynotes.
In this key note, we will explore some of the challenges of big data operating in a truly global context.
Microsoft keynote, featuring Dave Campbell, Vice President of Product Development for the SQL Server product suite.
In this talk, we present the broad data challenge and discuss  potential starting points for solutions.  We illustrate these approaches using data from a "meta-catalog" of over 1,000,000 open datasets that have been collected from about two hundred governments from around the world.
How software can transform human lives by bringing intelligence to wherever big data lives.
Hadoop and SAP HANA are taking the world by storm. SAP HANA is the fastest growing commercial database in the market, being adopted by the worlds top enterprises for real-time analytics and applications.
Big data gives us a powerful new way to see patterns in information - but what can't we see? When does big data not tell us the whole story? This talk opens up the question of the biases we bring to big data, and how we might work beyond them.
Designing for human fault-tolerance leads to important conclusions on the fundamental ways data systems should be architected.
The Victory Lab presents a secret history of modern American politics, pulling back the curtain on the tactics and strategies used by some of the era's most important figures-including Barack Obama and Mitt Romney-with iconoclastic insights into human decision-making, marketing and how analytics can put any business on the road to victory.
How must big companies evolve in order to realize big value from big data? Investing in data, technology and data scientists is just a first step.
An introduction Spark and Shark, two components of the open-source Berkeley Data Analytics Stack (BDAS) in development at UC Berkeley. Spark is a high-speed cluster computing system compatible with Hadoop that can outperform it by up to 100x. Shark is a port of Apache Hive onto Spark that is fully compatible with, and up to 100x faster than, Hive.
This tutorial provides a solid foundation for those seeking to understand large scale data processing with MapReduce and Hadoop, plus its associated ecosystem. This session is intended for those who are new to Hadoop and are seeking to understand where Hadoop is appropriate and how it fits with existing systems. No programming experience is required.
Learn first-hand how advanced analytics are enabling modern enterprises to deal with big data challenges.
In this talk, we'll examine compelling, real-world examples that offer a blueprint for integrating big data technologies, delivering rapid visibility and insights to IT professionals, data analysts and business users, and that accelerate the adoption of big data in the enterprise.
Real-world examples of utility companies around the world using Hadoop to optimize their services and changing Hadoop in the process.
Discussion of how big data is impacting modern business, which market trends are driving the adoption of big data solutions, and how big data professionals can choose the right technology to transform their business.
This talk is about the emergence of a new class of analytic databases based on principles first popularized by Google Dremel. These systems have been designed with the goal of enabling real-time SQL on Hadoop, while also supporting schema-on-read, semi-structured data, and pluggable storage engines. In this talk we will explain the novel architectural features that make these goals a reality.
Managing data in Hadoop gets complex quickly - *Loom* is the data set management system for Hadoop that makes it easy. *Loom* provides tools to track the lineage and provenance of all registered HDFS data, and *Activescan* so that all of the critical information about data sets is collected dynamically.
MapReduce, Hadoop, and other NoSQL big data approaches opened opportunities for data scientists in every industry to develop new data-intensive applications. But what about the more traditional SQL users or analysts? How can they unlock insights through standard business intelligence (BI) tools or ANSI SQL access?
Come learn about ACG, Analytical Compute Grid, a solution Rackspace built leveraging OpenStack, Big Data and NoSQL to help end users manage complex information and data.
Attend this session to hear how NetApp was able to solve their big data problem.  Since the design and implementation of the solution, NetApp has a number of takeaways and best practices required to convert theory into practice, allowing  completion of an enterprise-level implementation of such a solution.
Enterprises are moving forward with the vision of creating a central repository of all enterprise data stored inexpensively and processed efficiently in Hadoop. Only a fraction have yet been successful. This session will explore the pitfalls of implementing the Hadoop Data Reservoir and the requirements that lead to success.
Opposites attract and thats the case with Hadoop and analytic databases. Both have a role to play in your Big Data projects. This session explores the various approaches to cementing the bond between Hadoop to your analytic database, how SAP customers are integrating Hadoop into BI and advanced analytic environments, and why youll want to do that too.
An introduction Spark and Shark, two components of the open-source Berkeley Data Analytics Stack (BDAS) in development at UC Berkeley. Spark is a high-speed cluster computing system compatible with Hadoop that can outperform it by up to 100x. Shark is a port of Apache Hive onto Spark that is fully compatible with, and up to 100x faster than, Hive.
This tutorial provides a solid foundation for those seeking to understand large scale data processing with MapReduce and Hadoop, plus its associated ecosystem. This session is intended for those who are new to Hadoop and are seeking to understand where Hadoop is appropriate and how it fits with existing systems. No programming experience is required.
Learn first-hand how advanced analytics are enabling modern enterprises to deal with big data challenges.
In this talk, we'll examine compelling, real-world examples that offer a blueprint for integrating big data technologies, delivering rapid visibility and insights to IT professionals, data analysts and business users, and that accelerate the adoption of big data in the enterprise.
Real-world examples of utility companies around the world using Hadoop to optimize their services and changing Hadoop in the process.
Discussion of how big data is impacting modern business, which market trends are driving the adoption of big data solutions, and how big data professionals can choose the right technology to transform their business.
This talk is about the emergence of a new class of analytic databases based on principles first popularized by Google Dremel. These systems have been designed with the goal of enabling real-time SQL on Hadoop, while also supporting schema-on-read, semi-structured data, and pluggable storage engines. In this talk we will explain the novel architectural features that make these goals a reality.
Managing data in Hadoop gets complex quickly - *Loom* is the data set management system for Hadoop that makes it easy. *Loom* provides tools to track the lineage and provenance of all registered HDFS data, and *Activescan* so that all of the critical information about data sets is collected dynamically.
MapReduce, Hadoop, and other NoSQL big data approaches opened opportunities for data scientists in every industry to develop new data-intensive applications. But what about the more traditional SQL users or analysts? How can they unlock insights through standard business intelligence (BI) tools or ANSI SQL access?
Come learn about ACG, Analytical Compute Grid, a solution Rackspace built leveraging OpenStack, Big Data and NoSQL to help end users manage complex information and data.
Attend this session to hear how NetApp was able to solve their big data problem.  Since the design and implementation of the solution, NetApp has a number of takeaways and best practices required to convert theory into practice, allowing  completion of an enterprise-level implementation of such a solution.
Enterprises are moving forward with the vision of creating a central repository of all enterprise data stored inexpensively and processed efficiently in Hadoop. Only a fraction have yet been successful. This session will explore the pitfalls of implementing the Hadoop Data Reservoir and the requirements that lead to success.
Opposites attract and thats the case with Hadoop and analytic databases. Both have a role to play in your Big Data projects. This session explores the various approaches to cementing the bond between Hadoop to your analytic database, how SAP customers are integrating Hadoop into BI and advanced analytic environments, and why youll want to do that too.
Communicating Data Clearly describes how to draw clear, concise, accurate graphs that are easier to understand than many of the graphs one sees today.  The tutorial emphasizes how to avoid common mistakes that produce confusing or even misleading graphs. Graphs for one, two, three, and many variables are covered as well as general principles for creating effective graphs.
Building on our previous tutorial introducing BDAS, the open-source Berkeley Data Analytics Stack, in this tutorial we will provide each audience member with a Spark/Shark cluster on EC2 and walk through hands-on coding examples. Lessons will cover the Spark and Shark command line interfaces, writing a standalone program, and data clustering using a distributed machine learning algorithm on Spark.
Communicating Data Clearly describes how to draw clear, concise, accurate graphs that are easier to understand than many of the graphs one sees today.  The tutorial emphasizes how to avoid common mistakes that produce confusing or even misleading graphs. Graphs for one, two, three, and many variables are covered as well as general principles for creating effective graphs.
Building on our previous tutorial introducing BDAS, the open-source Berkeley Data Analytics Stack, in this tutorial we will provide each audience member with a Spark/Shark cluster on EC2 and walk through hands-on coding examples. Lessons will cover the Spark and Shark command line interfaces, writing a standalone program, and data clustering using a distributed machine learning algorithm on Spark.
 This hands-on tutorial will give you on an overview of how AWS can quickly and easily enable you to start generating insights from your companys data.
When data volume and velocity become massive, processing and analysis solutions require specialized technologies for different parts of the data pipeline. Googles Cloud Platform is designed to help you focus on building applications, not infrastructure. Well demonstrate how to build end to end Big Data applications - from data collection, to analysis, to reporting and visualization.
2012 was particularly interesting for the variety of Big Data use-cases implemented. This session explores key patterns across horizontal and vertical use cases.
This panel will share insights on how K-16 education can benefit from developments in Big Data ecosystems.
Data Science has created quite the movement in the data world, yet confusion between data science and analytics still remain across the enterprise. Rather than approach the subject talking about semantic differences between the two, we will discuss the topics as they relate to solving problems, how businesses are approaching them and what you can start doing with data science.
Learn how Neustar has expanded their data warehouse capacity, agility for data analysis, reduced costs, and enabled new data products. Discuss challenges and opportunities in capturing 100s of TBs of compact binary network data, ad hoc analysis, integration with a scale out relational database, more agile data development, and building new products integrating multiple big data sets.
Microsoft partner, Ascribe, is using Microsofts Big Data solutions to turn emergencies into actionable data
Cloudera, the standard for Apache Hadoop in the enterprise, empowers data-driven enterprises to Ask Bigger Questions and get bigger answers from all their data at the speed of thought. Cloudera Enterprise, the platform for Big Data, enables organizations to easily derive business value from structured and unstructured data to achieve a significant competitive advantage.
Given the exponential rise in data, attorneys have an obligation to meet todays Governance, Risk and Compliance (GRC) challenges and stay on top of technology in order to achieve broader institutional benefits. Join Digital Reasoning and the Clutch Group to learn how moving from document-centric to entity-centric analytics is key in gaining valuable knowledge from unstructured information.
As enterprises deploy Hadoop, its not the volume or velocity of data that is problematic, but the variety of types and formats of their critical data.  This session discusses how leading companies have integrated Hadoop, NoSQL (HBase) and enterprise sources on one platform. Data is combined and processed in one simplified architecture.  Case studies and reference architectures will be reviewed.
Learn how HP has established itself as the premier Big Data vendor with a solid portfolio of turnkey solutions that can be deployed faster than ever, while keeping acquisition and operational costs down. Learn more at hp.com/go/information.
The emergence of Apache Hadoop over the past few years has required organizations to completely rethink architectures that have been in place for decades. And with changes in the underlying data fabric, come ripple effects, and often bottlenecks, that impact all levels of an organization both business and technical.
ParAccel runs analytic queries 100x faster than Hive with much deeper SQL Support. Hear how companies are using analytic platforms for fast, interactive analysis on big data.
 This hands-on tutorial will give you on an overview of how AWS can quickly and easily enable you to start generating insights from your companys data.
When data volume and velocity become massive, processing and analysis solutions require specialized technologies for different parts of the data pipeline. Googles Cloud Platform is designed to help you focus on building applications, not infrastructure. Well demonstrate how to build end to end Big Data applications - from data collection, to analysis, to reporting and visualization.
2012 was particularly interesting for the variety of Big Data use-cases implemented. This session explores key patterns across horizontal and vertical use cases.
This panel will share insights on how K-16 education can benefit from developments in Big Data ecosystems.
Data Science has created quite the movement in the data world, yet confusion between data science and analytics still remain across the enterprise. Rather than approach the subject talking about semantic differences between the two, we will discuss the topics as they relate to solving problems, how businesses are approaching them and what you can start doing with data science.
Learn how Neustar has expanded their data warehouse capacity, agility for data analysis, reduced costs, and enabled new data products. Discuss challenges and opportunities in capturing 100s of TBs of compact binary network data, ad hoc analysis, integration with a scale out relational database, more agile data development, and building new products integrating multiple big data sets.
Microsoft partner, Ascribe, is using Microsofts Big Data solutions to turn emergencies into actionable data
Cloudera, the standard for Apache Hadoop in the enterprise, empowers data-driven enterprises to Ask Bigger Questions and get bigger answers from all their data at the speed of thought. Cloudera Enterprise, the platform for Big Data, enables organizations to easily derive business value from structured and unstructured data to achieve a significant competitive advantage.
Given the exponential rise in data, attorneys have an obligation to meet todays Governance, Risk and Compliance (GRC) challenges and stay on top of technology in order to achieve broader institutional benefits. Join Digital Reasoning and the Clutch Group to learn how moving from document-centric to entity-centric analytics is key in gaining valuable knowledge from unstructured information.
As enterprises deploy Hadoop, its not the volume or velocity of data that is problematic, but the variety of types and formats of their critical data.  This session discusses how leading companies have integrated Hadoop, NoSQL (HBase) and enterprise sources on one platform. Data is combined and processed in one simplified architecture.  Case studies and reference architectures will be reviewed.
Learn how HP has established itself as the premier Big Data vendor with a solid portfolio of turnkey solutions that can be deployed faster than ever, while keeping acquisition and operational costs down. Learn more at hp.com/go/information.
The emergence of Apache Hadoop over the past few years has required organizations to completely rethink architectures that have been in place for decades. And with changes in the underlying data fabric, come ripple effects, and often bottlenecks, that impact all levels of an organization both business and technical.
ParAccel runs analytic queries 100x faster than Hive with much deeper SQL Support. Hear how companies are using analytic platforms for fast, interactive analysis on big data.
As more industries adopt data-driven policies, people untrained in the formal analysis of data are find themselves staring at a spreadsheet and asking what they did to deserve it. In this tutorial, two of Kaggles top data scientists will walk attendees through the basics of solving an analytics challenge, from defining the problem, to performing basic analysis, to visualizing the output.
In this hands-on tutorial, you will learn the importance of distributed search by our industry experience and knowledge of real use cases. Well introduce different architectures that incorporate distributed search techniques, share pain points experienced and lessons learned. For the hands-on part of the tutorial, you will learn how to install and use Apache Solr for real-time search on big data.
Every month Birchbox delivers a box of samples to each of its subscribers.  Boxes are targeted to subscribers based on their profile, history, and behavior. In this talk we discuss the mathematics behind allocating samples to customers (aka solving for happiness).
Everyone wants to predict the future; fame and fortune follow those who succeed. I cover the basics of forecasting including tips, tricks, and best practices, and how forecasting differs from prediction analysis. I walk through simple examples using R and link to several resources to put you on the path to becoming the next Nostradamus.
Rachel Schutt, Senior Research Scientist at Johnson Research Labs, will discuss her Columbia Data Science course: her motivations for teaching it,  how she designed the curriculum, how the NYC tech community was involved, and what impact, if any, she had on her students. She thought about the course as testing the hypothesis: It is possible to incubate awesome data science teams in the classroom.
In this talk, I will introduce the IPython Notebook, an open-source, web-based interactive computing environment for Python and other languages.  By enabling the data scientist to build documents that combine code, text, formulas, visualizations, images and video the Notebook creates a foundation for data science that is interactive, repeatable, documented and sharable.
With more data come more problems. Did you know Excel dates begin on January 1, 1900? Unless you're using the OS X version, then dates begin on January 1, 1904. Or Unix time, which begins January 1, 1970. These pervasive, easily-overlooked gremlins are the bane of any data scientist and in this session I will explore a variety of these little nuisances.
Classic data science problems involve finding stationary patterns in big datasets. However, in adversarial settings, enemies deliberately shift their approach to avoid detection. They can challenge learning systems by randomizing behavior, hiding tracks, lacing traffic and more. Successful application of machine learning requires new approaches to feature engineering, training and classification.
Learn how LivePerson and Zoomdata perform stream processing and visualization on mobile devices of structured site traffic and unstructured chat data in real-time for business decision making.  Technologies include Kafka, Storm, and d3.js for visualization on mobile devices.  Byron Ellis, Data Scientist for LivePerson will join Justin Langseth of Zoomdata to discuss and demonstrate the solution.
Most stable systems rely on feedback - from central heating to industrial plants and biological organisms. This introductory talk will explain what feedback is, why it is relevant to enterprise software development, and how to apply it to some typical problems arising in business and technical situations.
Given a machine learning (ML) problem, which method(s) should you use, and how does big data affect your choices?  I will discuss some principles derived from decades of theory and practice, illustrated through real-world ML success stories in medicine, marketing, financial services, and astronomy.
The key takeaway from this session will be an understanding of the third generation of tools for realizing machine learning algorithms - examples of these tools include Twister, HaLoop, GraphLab. Attendees will also understand why the second generation tools such as Mahout has not implemented some of the machine learning algorithms for big data. The session will also have real-life use cases.
Julia is a new mathematical programming language that is scalable, high-performance, and open source. Julia is fast, approaching and often matching the performance of C/C++, easy to learn, and designed for distributed computation. This session will demonstrate some of the special capabilities of Julia and give you the tools you need to get started using this exciting technical computing language.
As more industries adopt data-driven policies, people untrained in the formal analysis of data are find themselves staring at a spreadsheet and asking what they did to deserve it. In this tutorial, two of Kaggles top data scientists will walk attendees through the basics of solving an analytics challenge, from defining the problem, to performing basic analysis, to visualizing the output.
In this hands-on tutorial, you will learn the importance of distributed search by our industry experience and knowledge of real use cases. Well introduce different architectures that incorporate distributed search techniques, share pain points experienced and lessons learned. For the hands-on part of the tutorial, you will learn how to install and use Apache Solr for real-time search on big data.
Every month Birchbox delivers a box of samples to each of its subscribers.  Boxes are targeted to subscribers based on their profile, history, and behavior. In this talk we discuss the mathematics behind allocating samples to customers (aka solving for happiness).
Everyone wants to predict the future; fame and fortune follow those who succeed. I cover the basics of forecasting including tips, tricks, and best practices, and how forecasting differs from prediction analysis. I walk through simple examples using R and link to several resources to put you on the path to becoming the next Nostradamus.
Rachel Schutt, Senior Research Scientist at Johnson Research Labs, will discuss her Columbia Data Science course: her motivations for teaching it,  how she designed the curriculum, how the NYC tech community was involved, and what impact, if any, she had on her students. She thought about the course as testing the hypothesis: It is possible to incubate awesome data science teams in the classroom.
In this talk, I will introduce the IPython Notebook, an open-source, web-based interactive computing environment for Python and other languages.  By enabling the data scientist to build documents that combine code, text, formulas, visualizations, images and video the Notebook creates a foundation for data science that is interactive, repeatable, documented and sharable.
With more data come more problems. Did you know Excel dates begin on January 1, 1900? Unless you're using the OS X version, then dates begin on January 1, 1904. Or Unix time, which begins January 1, 1970. These pervasive, easily-overlooked gremlins are the bane of any data scientist and in this session I will explore a variety of these little nuisances.
Classic data science problems involve finding stationary patterns in big datasets. However, in adversarial settings, enemies deliberately shift their approach to avoid detection. They can challenge learning systems by randomizing behavior, hiding tracks, lacing traffic and more. Successful application of machine learning requires new approaches to feature engineering, training and classification.
Learn how LivePerson and Zoomdata perform stream processing and visualization on mobile devices of structured site traffic and unstructured chat data in real-time for business decision making.  Technologies include Kafka, Storm, and d3.js for visualization on mobile devices.  Byron Ellis, Data Scientist for LivePerson will join Justin Langseth of Zoomdata to discuss and demonstrate the solution.
Most stable systems rely on feedback - from central heating to industrial plants and biological organisms. This introductory talk will explain what feedback is, why it is relevant to enterprise software development, and how to apply it to some typical problems arising in business and technical situations.
Given a machine learning (ML) problem, which method(s) should you use, and how does big data affect your choices?  I will discuss some principles derived from decades of theory and practice, illustrated through real-world ML success stories in medicine, marketing, financial services, and astronomy.
The key takeaway from this session will be an understanding of the third generation of tools for realizing machine learning algorithms - examples of these tools include Twister, HaLoop, GraphLab. Attendees will also understand why the second generation tools such as Mahout has not implemented some of the machine learning algorithms for big data. The session will also have real-life use cases.
Julia is a new mathematical programming language that is scalable, high-performance, and open source. Julia is fast, approaching and often matching the performance of C/C++, easy to learn, and designed for distributed computation. This session will demonstrate some of the special capabilities of Julia and give you the tools you need to get started using this exciting technical computing language.
This tutorial offers a basic introduction to practicing data science. We'll walk through several typical projects that range from conceptualization to acquiring data, to analyzing and visualizing it, to drawing conclusions.
Learn how to leverage data exhaust, the digital byproduct of our online activities, to solve problems and discover insights about the world around you.  We will walk through a real world example which combines several datasets and statistical techniques to discover insights and make predictions about attendees at O'Reilly Strata.
A discussion of Big Data approaches to analysis problems in marketing, forecasting, academia and enterprise computing. We focus on practices to enhance collaboration and employ rich statistical methods: a Magnetic, Agile and Deep (MAD) approach to analytics. While the approach is language-agnostic, we show that sophisticated statistics can be easily scaled in traditional environments like SQL.
If you're a new startup looking for investment, or a team at a large company seeking the green light for a new product, nothing convinces like real running code. But how do you solve the chicken-and-egg problem of filling your early prototype with real data? We'll discuss how to use open datasets and public web APIs as a proxy for the final product while you're still in the development stage.
How do you build a crack team of data scientists on a shoestring budget? In this 40-minute presentation from the co-founder of Infochimps, Flip Kromer will draw from his experiences as a teacher and his vast programming and data experience to share lessons learned in building a team of smart, enthusiastic hires.
Certain recent academic developments in large data have immediate and sweeping applications in industry. They offer forward-thinking businesses the opportunity to achieve technical competitive advantages. However, these little-known techniques have not been discussed outside academiauntil now. What if you knew about important new large data techniques that your competition don't yet know about?
Many of the tools Google created to store, query, analyze, visualize data are exposed to external developers. This talk will give you an overview of Google services for Data Crunchers: Google Storage for developers, BigQuery, Machine Learning API, App Engine, Visualization API.
Sharing data on the Web comes with a tough trade-off between minimalism and enabling creative new scenarios. This session will explore Web APIs that focus on exposing data and let clients decide how to use it. We'll share our experiences while designing the Open Data Protocol (odata.org), what we found to be great and terrible ideas and what we hear from folks running OData Web APIs.
Most analytics systems rely on large offline computations, which means results come in hours or days behind. Twitter is all about realtime, but with over 160 million users producing over 90 million tweets per day, we need realtime analytics that scaled horizontally.  This talk discusses the development of that infrastructure, as well as the products we are beginning to build on top of it.
The rise of sensor network data and the expectation for low latency query responses combine to obsolete available databases and storage platforms.   We have built a platform for web-scale OLAP and in this talk I will cover how we made our infrastructure capable of real-time update and query performance over hundreds of terabytes of multidimensional data.
To many people, Big Data means Open Data: social graphs, voting records, weather patterns, and more. But who owns data? Most of our laws were written for atoms, not bits; they're woefully out of date in an information age. When you share data, does it become more or less valuable? If someone adds to your data, is it still yours? This panel will tackle the gray area of data ownership.
Does information really want to be free? While the Internet is full of open data, there's plenty of data companies are willing to pay handsomely for -- particularly if it's timely and well aggregated. As a result, data marketplaces are a burgeoning business. This panel will look at the market for data, and where it's headed.
Data doesn't just show us the pastit can help predict the future. Several new firms harvest massive amounts of open data, trying to anticipate everything the right ad placement to the next terrorist attack. In this session, we bring together the founders of these firms to discuss the technologyand ethicsof looking into the future.
OpenTSDB is an open-source, distributed time series database designed to monitor large clusters of commodity machines at an unprecedented level of granularity. OpenTSDB allows operation teams to keep track of all the metrics exposed by operating systems, applications and network equipment, and makes the data easily accessible.
Apache Avro provides an expressive, efficient standard for representing large data sets.  Avro data is programming-language neutral and MapReduce-friendly.  Hopefully it can replace gzipped CSV-like formats as a dominant format for data.
Topics for any discipline that focuses on quantitative or technical data have always depended on the datasets that were available at the time. Crowdsourcing has changed that  democratizing the data-collection process and cutting researchers reliance on stagnant, overused datasets. Tools like Amazon Mechanical Turk allow anyone to gather data overnight, rather than waiting years.
Much useful business data is in "semi-structured" form: government filings, insurance claims, customer comment forms, etc. Although most search tools don't take advantage of it, knowing a little structure goes a long way.  This talk will show how semi-structured data can be interpreted, summarized, and applied to produce business value in several real-life examples.
Data modeling competitions allow companies and researchers to post a problem and have it scrutinised by the world's best data scientists. By exposing a problem to a wide audience, competitions are a great way to get the most out of a dataset. In just a few months, Kaggle's competitions have helped to progress the state of the art in chess ratings and HIV research.
Information is changing healthcare forever. From the study of epidemics, to machine learning that can improve diagnosis, to the sequencing of the human genome, we're doing the math of life itself. This panel of practitioners will show us what they're doing in healthcare, pharmaceuticals, and genomics, and how it will change the way we discover, treat, and eliminate disease.
Can machines help us make better decisions? In this panel, real-world practitioners from the travel, finance, and energy industry give us an inside look at how they're applying machine learning to their industries, oprimizing the use of resources and helping with decision support.
Join practitioners from a range of industries to learn how they're putting new tools and massive data sets to work. We'll hear how music, geophysics, and the legal system are all changing by putting huge, rich information into the hands of business.
The ability to collect, crunch, act upon, and share huge amounts of data disrupts nearly every industry, tearing down barriers to entry and creating entirely new businesses. This panel of investors will discuss where they see the opportunities in the Big Data industry, and how they think about the value of new ventures in the space.
Today's web analyst has moved far beyond funnels and visitors. Automated systems decide who gets what content, and language parsing tries to distill sentiment from millions of online interactions. This panel will look at where web analytics is headed, and how new algorithms and approaches are yielding fresh insights into online commerce.
Data integration and viz technology have given rise to an appetite for government datathe Gov 2.0 movement. Do government agencies have good data? Sort of: I believe that an understanding of data limitations has gotten short shrift in the drive to develop the next app. I'll discuss why a knowledge of the complexities of government data is crucial to building quality decision-making tools.
In a first, Forbes presented all federal campaign contributions by Americas wealthiest people in our September 2010 online edition of the Forbes 400. We  combined human effort and homegrown database code to sort through 6 million political donations and find the 20,000 that came from Americas richest people.
New technologies are driving a new era of global collaboration among scientists and researchers.  Digital scholarship, the ability to create, collect, publish and collaborate in new digital mediums, is driving the exponential growth of data related to scholarly research.  This talk will highlight evolving strategies used to appraise and predict success of institutions and researchers.
Virtual worlds are a goldmine of untapped insights, even for predicting physical behaviors. Not only will we share PARC findings and methods developed to extract key data from online games, but more importantly, we'll discuss how social scientists converted and processed raw behavioral metrics into meaningful psychological variables that can be applied to a broad spectrum of business applications.
Hadoop and HBase make it easy to store terabytes of data, but how do you scale your search mechanism to sift through these mountains of bits and retrieve large result sets in a matter of milliseconds?  Careful use of the Solr search server, based on Lucene, made these requirements come to life in our production environment.  Come learn how we query terabytes of data in a highly available system.
With thousands of datapoints per second from nodes around the world, how can you tell when something isn't right? The bottom line is: it's hard, but with the right tools it is achievable.
This tutorial will explain MapReduce and how to develop big data applications in Java and high level languages such as Pig and Hive SQL. Using examples it will cover how to prototype, debug, monitor, test and optimize big data applications for Hadoop. Attendees will get hands-on instruction and will leave with a solid understanding of how to analyze data on Hadoop clusters and practical examples.
Interactive visualizations have become the new media for telling stories online. This session will focus on going from a good visualization to a great visualization by focusing on organization, user interface, and formatting. You should expect to leave this session confident in your ability to consistently create excellent interactive visuals.
When faced endless data and the need to manage it, there are a variety of proven design patterns that will help designers create usable, efficient, and effective interfaces.  From distributing workload to reducing sensory overload, well review the techniques that are enabling the highly scalable user interfaces of today and tomorrow.
While the majority of charts were designed to handle a variety of data, there is a certain novelty of presenting data in a very succinct way. By designing a presentation method restricted to specific data points, we can realize an economy of space and interface.
The state of open data today is a real mess. It's very difficult to find the data you need and be confident that it's timely and accurate. There is a growing list of companies now vying to become the key destinations for people to gather around new datasets and be excited together. What projects, partnerships and even ventures would be created if there was a marketplace for data?
"Many hands make light work", as the saying goes. That's true when thousands of people can collaborate on a data set. In this session, we'll look at collective interfaces that allow many distributed users to examine and share data with one another, and how that's changing traditional desktop visualization tools.
The world's available scientific and factual data is growing at an alarming pace, but how do we use all this information?  How do we incorporate it into our decision making process?  Joshua Martell, will give an inside look into how Wolfram|Alpha works, what it takes to make data "computable", understand user input, and present meaningful results.
After Kennedy, you couldn't win an election without TV. After Obama, it was social media. But tomorrow's citizen gets their information from visualizations. In this panel, three acclaimed designers show how they apply visualization to big data, making complex, controversial topics easy to understand and explore.
Open access to information promises to connect citizens to their representatives, improving government transparency and helping educators transform the classroom. In this real-world panel, practitioners in government and the public sector will give us a glimpse into how data and new interfaces are transforming how we teach and govern.
Live demonstration of ambient computing using projector-camera pairs to scan the room and place interactive simulations into the space. All surfaces are rendered interactive. We will demonstrate a 3D sandtable for firefighter training and STEM education where the 3D sand becomes and interactive surface.
We will discuss the impact of the information explosion, the effectiveness of current technological directions, and explore the success that new perception-based, human-computer interfaces provide in analyzing and understanding complex data.  Real examples will be used to illustrate that effective man-machine environments are essential in productively dealing with multi-dimensional information.
Artistic visualizations and infographics tell the stories of rich data in unique, compelling ways and synthesize datasets in ways that allow them to be interpreted, absorbed, and experienced in ways beyond the spreadsheet, pie chart, and bar graph.
Ram Peddibhotla, a Director from Intels Software and Services Group, will discuss how the future of mobile involves ubiquity across multiple hardware platforms.  Specifically, Ram will discuss how open source software will shape the next generation of computing devices, improving compatibility.
How do you go about building a product around data using Hadoop? This talk will present how LinkedIn builds and maintains such features as People You May Know. We will present our architecture for doing so (open-sourced) as well as knowledge we've gained in the process.
With growing amounts of digital data at the fingertips of software developers the need for a scalable, easy to use framework is tremendous. This talk introduces Apache Mahout - a project with the goal of implementing scalable machine learning algorithms for the masses.
Apache Cassandra is a second-generation distributed database originally open-sourced by Facebook. Its write-optimized shared-nothing architecture results in excellent performance and scalability. This tutorial will cover application design with Cassandra through a series of exercises with Twissandra, a simple Twitter clone written in Python and Django.
Much of the world's most valuable information is trapped in digital sand, siloed in servers scattered around the globe. In this talk I'll discuss the promise of big data, which will come to pass in the coming decade, driven by advances in three principle areas: sensor networks, cloud computing, and machine learning.
With Big Data comes Big Promises.  Mine the blogsphere and discover the secret of eternal wealth.  Feast on the Twitter feeds for the wisdom of the ages.  We have visited this land in the past, naming it data warehousing and business intelligence.  Will we learn the lessons of history?  Can we do it differently today?  Lets take this present moment to review the past and imagine the future.
There's never just one way to do things, but for big bets like Big Data, it helps to learn about paths that others have taken.  In this presentation, Bob Page, VP Data & Analytics Platforms for eBay, gives a "behinds the scenes" look at the systems and procedures in that power decision-making at the world's largest online marketplace.
There has been an explosion in database technology designed to handle big data and deep analytics from both established vendors and startups. This session will provide a quick tour of the primary technology innovations and systems powering the analytic database landscape.
Retailers and their suppliers have always operated on the cutting edge of data science. In fact, this industry is responsible for many of the technology advances that have contributed to the exponential growth of data, analytics, and related technology. This session covers the history of data science in retail, current trends, and explores future directions in the big data age.
This presentation will focus on how businesses can maximize big data analytics for deeper customer insights.
This presentation lays bare the dark underbelly of analytics in the enterprise. Drawing on darkly humorous experiences, the speaker will explain why executives treat analytics as an occult phenomenon. The talk will give executives the mental tools to separate strategically valuable analytics projects from fishing expeditions, and provide litmus tests to keep the witch doctors honest.
Big Data and predictive analytics can deliver incredible insight that can be used for purposes both good, and not so good. Drawing on real world examples, this session will examine the fine line between competitive advantage and bad behavior, and implications to a complex cast of stakeholders. Lets begin a dialog on ethics now instead of waiting for our first major crisis.
"Water, water everywhere, nor any drop to drink." - Rime of the Ancient Mariner. People feel overwhelmed with data. But the problem is not with the amount of data. The problem is that data is not presented in a form that people can understand and use. Juice Analytics will present and demonstrate proven techniques to design information applications to present data in enjoyable and rewarding ways.
This tutorial describes how to draw clear, concise, accurate graphs that are easier to understand than many of the graphs one sees today.  The tutorial emphasizes how to avoid common mistakes that produce confusing or even misleading graphs. Graphs for one, two, three, and many variables are covered as well as general principles for creating effective graphs.
This talk demonstrates how an eclectic blend of storage, analysis, and visualization techniques can be used to gain a lot of serious insight from Twitter data, but also to answer fun quesions such as "What does Justin Bieber and the Tea Party have (and not have) in common?"
Riak Core is a general implementation of a distributed systems model, enabling you to build a customized, scalable, highly-available distributed system without too huge an investment.  Justin will explain that model, its history, and how it can be used to build new data processing systems.
The move to cloud infrastructure and the need to handle big data have created the perfect catalysts for organizations to introduce new infrastructure software and break ties from their expensive incumbent vendors. Ed will share a detailed strategy on how to leverage open source database solutions like PostgreSQL to contain database cost and free budget for other, more valuable initiatives.
Developing a social network map is fundamental to comprehensively understanding a person. Social networks are dynamic and better derived from real-world data than static configurations.  However, the vast majority of this real world data is unstructured.  This preso will show how Synthesys uses very large scale unstructured data to create social network maps for reporting and further analysis.
Our talk summarizes some recent thinking in the field of vertical search and illustrates it in the context of a new version of Westlaw, called WestlawNext.  We argue that getting the right allocation of function between person and machine is the key to making specialist content more findable and search results more understandable.
This session explores how to get more done, faster with high-performance Map/Reduce and expand the universe of Hadoop possibilities with tools to speed and simplify development and deployment of analytic applications.
Windows Azure Marketplace includes data, imagery, and real-time web services from leading commercial data providers and authoritative public data sources. Customers have access to datasets such as demographic, environmental, financial, retail, weather and sports.
Social media websites are producing ginormous amounts of data and creating a massive demand for insight related to users, how they engage with features, where they are coming from, why they are visiting, what excites them, and so forth.
From customer behaviors & usage statistics to security postures & operational analytics, Splunk's ability to make sense of all types of machine data, structured or unstructured, and mash it up w/ other business data provides complete real-time visibility & operational intelligence. This tutorial demos a new approach for analyzing your organization's petabytes of data to derive real-time insights.
The OpenStack project was launched last summer by Rackspace, NASA, and a number of other cloud technology leaders in an effort to build a fully-open cloud computing platform. It is a collection of scalable, standards-based projects currently consisting of OpenStack Compute and OpenStack Object Storage. This session will introduce the projects and describe how they can help manage your data.
Organizations today possess massive data - in tera- and petabytes - that needs to be effectively collected, stored and processed. Hadoop is a cost effective option that helps manage this big data. To derive real returns from these big data systems, one needs to extract useful insights and business intelligence.
If you are a leading enterprise or web company, then two things are almost certainly true. Data is the lifeblood of your business. And you face an ever-increasing need to scale your applications and data services.
BoF topics are entirely up to you. Wednesday's Lunchtime BoF sessions will happen on the hotel side of the Hyatt Regency, Mezzanine Level.
BoF topics are entirely up to you. Thursday's Lunchtime BoF sessions will happen on the hotel side of the Hyatt Regency, Mezzanine Level.
Alistair Croll and Edd Dumbill welcome you back to Strata.
Edd Dumbill and Alistair Croll welcome you to Strata.
Ours is a new era of big behavioral data. Unprecedented business model experimentation is rapidly eroding individual privacy despite rising consumer concerns. Successfully managing privacy is a key differentiator for services providers. In the B2B space, the stakes to get privacy right are even higher. This talk will discuss the implications of privacy in order to succeed in the B2B space.
Big data and analytics have developed a mythology rooted in underlying assumptions. We need to ignore these myths and think clearly about how organizations use data, which means understanding how people use information and make decisions.
Zane Adam from Microsoft speaks about the Azure Data Marketplace.
The tools we use play a key role in how we use and respond to big data. Hear about the changes being led by key architects of future big data systems.
90,000 items on Afghanistan, 291,000 on Iraq - and another 251,000 cables. Managing the Wikileaks release is just one of the huge data journalism projects the Guardian's data team has embarked on. This talk will look at how journalists can make sense of data, get stories out of it and our role in supplying open data to the world.
The convergence of big, open data, ubicomp, and new interfaces will change the way humans work, play, learn, and love. It's a slow transformation that happens one tweet, one blog, and one game at a time -- but it's also an inexorable road towards the singularity. In this panel discussion, we'll look beyond the bytes and algorithms to think about humanity awash in a sea of information.
Companies must choose to spend their money and time on the right software initiatives.  With exploding volumes of critical data, getting new insight and mastery over business operations demands new investments in BI at multiple levels. Ed will show a proven path for how to avoid exorbitant database software fees and shift that spend to be used in areas like BI where you can realize a stronger ROI.
For more than 20 years now, data warehousing has put manners on unruly enterprise data. Yet, physics tells us that disorder inexorably increases unless we endlessly fight it. As information volumes and types explode into chaos, is it time to declare the warehouse dead? Or we could move from classical to quantum physics and create a new information architecture. Its time to make some new choices
Details coming soon.
A defining characteristic of modern life is the incredible proliferation of digital information. The Economist estimates that the amount of information created each year is growing at a 60% compounded rate. According to the Harvard Business Review, we humans generated more data last year than in all of previous human history.
In 2001, the Institutes of Medicine declared that between the care we have and the care we could have lies not just a gap, but a chasm, yet nothings really changed.  Healthcare remains one of the most richly endowed yet poorly equipped knowledge industries anywhere.   Using real world examples, well see how BIG DATA may be just what the doctor ordered, but only if we pick the right problems.
Data science is evolving rapidly. I'll talk about our current and slightly future technical and philosophical challenges, including realtime vs non-realtime analysis, streams of data vs traditional databases, and some of the opportunities we have to learn amazing things about the world through our data and what this means for those of us who are immersed in working with it.
The new data centricity drives that we have to rethink how we collect, store, manage, analyze and share our data, as all these processes  now require limitless resources. This talk will focus on the changes in infrastructure requirements to support  the new world and how innovations are removing barriers for companies to be successful.
Data competitions come of age: from movie recommendations to life and death. Possibly the biggest news at Strataconf is Heritage Provider Network's $3 million predictive modeling prize - the biggest data mining competition ever. It requires data scientists to build algorithms that predict who will go to hospital in the next year, so that preventive action can be taken.
Deep learning has become the go-to solution for many application areas, such as image classification or speech processing, but does it work for all application areas? Mikio Braun offers background on deep learning and shares his practical experience working with these exciting technologies.
Anima Anandkumar demonstrates how to use preconfigured Deep Learning AMIs and CloudFormation templates on AWS to help speed up deep learning development and shares use cases in computer vision and natural language processing.
TensorFlow is democratizing the world of machine intelligence. With TensorFlow (and Google's Cloud Machine Learning platform), anyone can leverage deep learning technology cheaply and without much expertise. Kazunori Sato explores how a cucumber farmer, a car auction service, and a global insurance company adopted TensorFlow and Cloud ML to solve their real-world problems.
Ted Dunning offers an overview of tensor computingcovering, in practical terms, the high-level principles behind tensor computing systemsand explains how it can be put to good use in a variety of settings beyond training deep neural networks (the most common use case).
For years, people have been talking about the great promise of conversation AI. Recently, deep learning has taken us a few steps further toward achieving tangible goals, making a big impact on technologies like speech recognition and natural language processing. Yishay Carmiel offers an overview of the impact of deep learning, recent breakthroughs, and challenges for the future.
Sherry Moore discusses TensorFlow progress and adoption over 2016 and looks ahead to TensorFlow efforts in future areas of importance, such as performance, usability, and ubiquity.
Nikolay Manchev offers an overview of the restricted Boltzmann machine as a type of neural network with wide range of applications and shares his experience using it on Hadoop (MapReduce and Spark) to process unstructured and semistructured data at a scale.
The popularity of deep learning is due in part to its capabilities in recognizing patterns from inputs such as images or sounds. Barbara Fusinska offers an overview of Microsoft Cognitive Toolbox, an open source framework offering various modules and algorithms enabling machines to learn like a human brain.
With TensorFlow, deep machine learning has transitioned from an area of research into mainstream software engineering. Martin Grner walks you through building and training a neural network that recognizes handwritten digits with >99% accuracy using Python and TensorFlow.
Deep learning is one of the most exciting techniques in machine learning. Miguel Gonzlez-Fierro explores the problem of image classification using ResNet, the deep neural network that surpassed human-level accuracy for the first time, and demonstrates how to create an end-to-end process to operationalize deep learning in computer vision for business problems using Microsoft RServer and GPU VMs.
Amitai Armon and Nir Lotan explain how to easily train and deploy deep learning models for image and text analysis problems using Intel's Deep Learning SDK, which enables you to use deep learning frameworks that were optimized to run fast on regular CPUs, including Caffe and TensorFlow.
Dask parallelizes Python libraries like NumPy, pandas, and scikit-learn, bringing a popular data science stack to the world of distributed computing. Matthew Rocklin discusses the architecture and current applications of Dask used in the wild.
R has the reputation for being slow. Colin Gillespie covers key ideas and techniques for making your R code as efficient as possible, from R setup to common R coding problems to linking R with C++ for an extra speed boost.
There are many resources available for learning how to use Spark to build collaborative filtering models. However, there are relatively few that explain how to build a large-scale, end-to-end recommender system. Seth Hendrickson demonstrates how to create such a system using Spark Streaming, Spark ML, and Elasticsearch.
Supporting multiple locales involves the maintenance and generation of localized strings. Michelle Casbon explains how machine learning and natural language processing are applied to the underserved domain of localization using primarily open source tools, including Scala, Apache Spark, Apache Cassandra, and Apache Kafka.
Nobody seems to agree just what data science is. Is it engineering, statistics...both? David Donoho's "50 Years of Data Science" offers a criticism of the hype around data science from a statistics perspective, arguing that it's not a new field. Sean Owen responds, offering counterpoints from an engineer, in search of a better understanding of how to teach and practice data science in 2017.
It is nearly impossible to sample enough training data initially to prevent autonomous driving accidents on the road, as has been sadly proven by Teslas autopilot. Michael Nolting explains that to overcome this problem, a real-time system has to be created to detect dangerous runtime situations in real time, a process much like website monitoring.
Machine learning and data science systems often fail in production in unexpected ways. David Talby shares real-world case studies showing why this happens and explains what you can do about it, covering best practices and lessons learned from a decade of experience building and operating such systems at Fortune 500 companies across several industries.
Multilevel regression and poststratification (MRP) is a method of estimating granular results from higher-level analyses. While it is generally used to estimate survey responses at a more granular level, MRP has clear applications in industry-level data science. Rumman Chowdhury reviews the methodology behind MRP and provides a hands-on programming tutorial.
Gary Willis offers a technical presentation of a novel algorithm that uses public data and an unsupervised tree-based learning algorithm to help companies leverage locational data they have on their clients. Along the way, Gary also discusses a wide range of further potential applications.
Galiya Warrier demonstrates how to apply a conversational interface (in the form of a chatbot) to communicate with an existing data science model.
"Stream" is a buzzword for several things that share the idea of timely handling of neverending data. Big data architectures are evolving to be stream oriented. Microservice architectures are inherently message driven. Dean Wampler defines "stream" based on characteristics for such systems, using specific tools as examples, and argues that big data and microservices architectures are converging.
Michael Noll explains how Apache Kafka helps you radically simplify your data processing architectures by building normal applications to serve your real-time processing needs rather than building clusters or similar special-purpose infrastructurewhile still benefiting from properties typically associated exclusively with cluster technologies.
Although the most widely used language for data analysis, SQL is only slowly being adopted by open source stream processors. One reason is that SQL's semantics and syntax were not designed with streaming data in mind. Fabian Hueske explores Apache Flink's two relational APIs for streaming analyticsstandard SQL and the LINQ-style Table APIdiscussing their semantics and showcasing their usage.
Vodafone UKs new SIEM system relies on Apache Flume and Apache Kafka to ingest over 1 million events per second. Tristan Stevens discusses the architecture, deployment, and performance-tuning techniques that enables the system to perform at IoT-scale on modest hardware and at a very low cost.
Dynamic data rebalancing is a complex process. Ben Stopford and Ismael Juma explain how to do data rebalancing and use replication quotas in the latest version of Apache Kafka.
Twitter processes billions of events per day at the instant the data is generated. To achieve real-time performance, Twitter employs Heron, an open source streaming engine tailored for large-scale environments. Karthik Ramasamy and Maosong Fu share several optimizations implemented in Heron to improve throughput by 5x and reduce latency by 5060%.
The world of big data involves an ever-changing field of players. Much as SQL is a lingua franca for declarative data analysis, Apache Beam aims to provide a portable standard for expressing robust, out-of-order data processing pipelines in a variety of languages across a variety of platforms. Tyler Akidau explains how this vision has been realized and discusses the challenges that lie ahead.
As a data-driven enterprise, ING is heavily investing in big data, analytics, and streaming processing. Bas Geerdink shares three use cases at ING and discusses their respective architectures and technology. All software is currently in production, running with modern tools such as Kafka, Cassandra, Spark, Flink, and H2O.ai.
Apache Beam's new State API brings scalability and consistency to fine-grained stateful processing while remaining portable to any Beam runner. Kenneth Knowles introduces the new state and timer features in Beam and shows how to use them to express common real-world use cases in a backend-agnostic manner.
Performance and security are often at loggerheads. Rekha Joshi explains why and offers a deep dive into how performance and security are managed in some of the most intense and critical data platform services at Intuit.
Cihan Biyikoglu explains how to substantially accelerate and radically simplify common practices in machine learning, such as running a trained model in production, to meet real-time expectations, using Redis modules that natively store and execute common models generated by Spark ML and TensorFlow algorithms.
If you have Hadoop clusters in research or an early-stage data lake and are considering strategic vision and goals, this session is for you. Phillip Radley explains how to run Hadoop as a shared service, providing an enterprise-wide data platform hosting hundreds of projects securely and predictably.
When building your data stack, the architecture could be your biggest challenge. Yet it could also be the best predictor for success. With so many elements to consider and no proven playbook, where do you begin to assemble best practices for a scalable data architecture? Ben Sharma offers lessons learned from the field to get you started.
Hellmar Becker and Jorn Eilander explore real-time collection and predictive analytics of flight radar data with IoT devices, NiFi, HBase, Spark, and Zeppelin.
Data exploration usually entails making endless one-use exploratory plots. Victor Zabalza shares a Python package based on Dask execution graphs and interactive visualization in Jupyter widgets built to overcome this drudge work. Victor offers an overview of the tool and explains how it was built and why it will become essential in the first steps of every data science project.
Wojciech Biela and ukasz Osipiuk offer an introduction to Presto, an open source distributed analytical SQL engine that enables users to run interactive queries over their datasets stored in various data sources, and explore its applications in various big data problems.
Collaborative filtering is great for recommendations, yet it suffers from the cold-start problem. New content with no views is ignored, and new users get poor recommendation. Aurlien Gron shares a solution: knowledge graphs. With a knowledge graph, you can truly understand your users' interests and make better, more relevant recommendations.
Real-time data analysis is becoming more and more important to Internet companies daily business. Qunar has been running Alluxio in production for over a year. Xueyan Li explores how stream processing on Alluxio has led to a 16x performance improvement on average and 300x improvement at service peak time on workloads at Qunar.
Any nontrivial streaming app requires that you consider a number of important topics, but questions like how to manage offsets or state often go unanswered. Mark Grover and Ted Malaska share practices that no one talks about when you start writing a streaming app but that you'll inevitably need to learn along the way.
In most organizations, data is spread across multiple data sources, such as Hadoop/cloud storage, RDBMS, and NoSQL. Tomer Shiran and Jacques Nadeau offer an overview of Apache Arrow, an open source in-memory columnar technology that enables users to combine multiple data sources and expose them as a virtual data lake to users of Spark, SQL-on-Hadoop, Python, and R.
The class of big data computations known as the distributed merge trees was built to aggregate user information across multiple data sources in the media domain. Vijay Srinivas Agneeswaran explores prototypes built on top of Apache HAWQ, Druid, and Kinetica, one of the open source GPU databases. Results show that Kinetica on a single G2.8x node outperformed clusters of HAWQ and Druid nodes.
Angie Ma offers a hands-on overview of implementing machine learning with Python, providing practical experience while covering the most commonly used libraries, including NumPy, pandas, and scikit-learn.
Big data and data science have great potential for accelerating business, but how do you reconcile the business opportunity with the sea of possible technologies? Data should serve the strategic imperatives of a businessthose aspirations that will define an organizations future vision. Scott Kurth and Edd Wilder-James explain how to create a modern data strategy that powers data-driven business.
Reynold Xin looks back at the history of data systems, from filesystems, databases, and big data systems (e.g., MapReduce) to "small data" systems (e.g., R and Python), covering the pros and cons of each, the abstractions they provide, and the engines underneath. Reynold then shares lessons learned from this evolution, explains how Spark is developed, and offers a peek into the future of Spark.
Security has been a large and growing aspect of distributed systems, specifically in the big data ecosystem, but it's an underappreciated topic within the Spark framework itself. Neelesh Srinivas Salian explains how detailed knowledge of the setup and an awareness of what to be looking out for in terms of problems and issues can help an organization move forward in the right way.
Structured Streaming is new in Apache Spark 2.0, and work is being done to integrate the machine-learning interfaces with this new streaming system. Holden Karau and Seth Hendrickson demonstrate how to do streaming machine learning using Structured Streaming and walk you through creating your own streaming model.
Spark is now the de facto engine for big data processing. Vincent Van Steenbergen walks you through two real-world applications that use Spark to build functional machine-learning pipelines (wine price prediction and malware analysis), discussing the architecture and implementation and sharing the good, the bad, and the ugly experiences he had along the way.
Harry Powell and Raffael Strassnig demonstrate how to model unobserved customer preferences over businesses by thinking about transactional data as a bipartite graph and then computing a new similarity metricthe expected degrees of separationto characterize the full graph.
Mireia Alos Palop and Natalino Busa share an implementation for classifying pictures based on Spark and Slider, developed during the 2016 Yelp Restaurant Photo Classification challenge. Spark processes data and trains the ML model, which consists of deep learning and ensemble classification methods, while picture scoring is exposed via an API that is persisted and scaled with Slider.
Herman van Hvell tot Westerflier offers a deep dive into Spark SQL's Catalyst optimizer, introducing the core concepts of Catalyst and demonstrating how new and upcoming features are implemented using Catalyst.
Much of Apache Sparks power comes from lazy evaluation along with intelligent pipelining, which can make debugging more challenging than on traditional distributed systems. Holden Karau explores how to debug Apache Spark applications, the different options for logging in Sparks variety of supported languages, and some common errors and how to detect them.
Nicolas Poggi evaluates the out-of-the-box support for Spark and compares the offerings, reliability, scalability, and price-performance from major PaaS providers, including Azure HDinsight, Amazon Web Services EMR, Google Dataproc, and Rackspace Cloud Big Data, with an on-premises commodity cluster as baseline.
Matthias Niehoff shares lessons learned working with Spark, Cassandra, and the Spark-Cassandra connector and best practices drawn from his work on multiple big and fast data projects, as well as challenges encountered along the way.
How do you take a platform designed for large-scale storage of unstructured key-value data and optimize it for the structured world of Spark? John Musser leads a deep dive into integrating Riak, the distributed key-value NoSQL database, with Spark, covering the challenges and solutions for integrating these tools and sharing lessons learned along the way.
Alison Lowndes leads a hands-on exploration of approaches to the challenging problem of detecting if an object of interest is present within an image and, if so, recognizing its precise location within the image. Along the way, Alison walks you through testing three different approaches to deploying a trained DNN for inference.
Deep learning is the state of the art in domains such as computer vision and natural language understanding. MXNet is a highly flexible and developer-friendly deep learning framework. Anima Anandkumar provides hands-on experience on how to use MXNet with preconfigured Deep Learning AMIs and CloudFormation Templates to help speed your development.
Todd Lipcon and Marcel Kornacker offer an introduction to using Impala and Kudu to power your real-time data-centric applications for use cases like time series analysis (fraud detection, stream market data), machine data analytics, and online reporting.
Apache Kylin is rapidly being adopted over the worldespecially in China. Luke Han explores how various industries use Apache Kylin, sharing why these companies choose Apache Kylin (a technology comparison), how they use Apache Kylin (their production deployment pattern), and most importantly the resulting business impact.
Trent Gray-Donald and Gil Vernik explain the challenges of current Hadoop and Apache Spark integration with object stores and discuss Stocator, an open source object store connector that overcomes these shortcomings by leveraging object store semantics. Compared to native Hadoop connectors, Stocator provides close to a 100% speed up for DFSIO on Hadoop and a 500% speed up for Terasort on Spark.
Marcel Kornacker and Mostafa Mokhtar help simplify the process of making good SQL-on-Hadoop decisions and cover top performance optimizations for Apache Impala (incubating), from schema design and memory optimization to query tuning.
Multiple challenges arise if distributed applications are provisioned in a containerized environment. Daniel Burer and Sascha Askani share a solution for distributed storage in cloud-native environments using Spark on Kubernetes.
Chris Fregly explores an often-overlooked area of machine learning and artificial intelligencethe real-time, end-user-facing "serving layer in hybrid-cloud and on-premises deployment environmentsand shares a production-ready environment to serve your notebook-based Spark ML and TensorFlow AI models with highly scalable and highly available robustness.
Cloudera Enterprise has made many focused optimizations in order leverage all of the cloud-native capabilities of AWS for the CDH platform. Andrei Savu and Philip Langdale take you through all the ins and outs of successfully running an end-to-end batch data analytic workflow in AWS.
As Intuit moves its SaaS platform from its own data centers to AWS, it will straddle both worlds for a period of time (and potentially indefinitely). Calum Murray looks at what straddling means to data and data systems.
Self-service data science is easier said than delivered, especially on Apache Hadoop. Most organizations struggle to balance the diverging needs of the data scientist, data engineer, operator, and architect. Matt Brandwein and Tristan Zajonc cover the underlying root causes of these challenges and introduce new capabilities being developed to make self-service data science a reality.
Big data's main challenge is technological evolution. Arturo Bayo shares a solution combining three approaches to deal with the dynamism of technology: multitenant architectures to allow the sharing of IT resources cost efficiently, modular big data components interacting with a Docker-based containers platform, and advanced analytics to predict infrastructure behavior to optimize its efficiency.
Crafting interactive data visualizations for the web is hardyou're stuck using proprietary tools or must become proficient in JavaScript libraries like D3. But what if creating a visualization was as easy as writing text? Amit Kapoor and Bargava Subramanian outline the grammar of interactive graphics and explain how to use declarative markdown-based tool Visdown to build them with ease.
Shannon Cutt welcomes you to Data 101.
Deep learning is white-hot at the moment, but why does it matter? Developers are usually the first to understand why some technologies cause more excitement than others. Edd Wilder-James relates this insider knowledge, providing a tour through the hottest emerging data technologies of 2017 to explain why theyre exciting in terms of both new capabilities and the new economies they bring.
The cloud is becoming pervasive, but it isnt always full of rainbows. Defining a strategy that works for your company or for your use cases is critical to ensuring success. Jim Scott explores different use cases that may be best run in the cloud versus on-premises, points out opportunities to optimize cost and operational benefits, and explains how to get the data moved between locations.
Big data and emerging technologies offer powerful benefits, but for an organization to use them to their full advantage, a change in organizational culture is required. Ellen Friedman offers practical guidance on how to adopt an organizational culture that supports digital transformation, using examples from a variety of business use cases.
Big data needs governance, not just for compliance but also for data scientists. Governance empowers data scientists to find, trust, and use data on their own, yet it can be overwhelming to know where to startespecially if your big data environment spans beyond your enterprise to the cloud. Mark Donsky shares a step-by-step approach to kick-start your big data governance initiatives.
The global populace is asking for the IT industry to be held responsible for the safe-guarding of individual data. Steve Touw examines some of the data privacy regulations that have arisen and covers design strategies to protect personally identifiable data while still enabling analytics.
Successful organizations are becoming increasingly Agile, and the autonomy and empowerment that Agile brings create new active modes of engagement. Data governance however is still very much a centralized task that only CDOs and data owners actively care about. Antonio Alvarez and Lidia Crespo outline a more engaging and active method of data governance: data citizenship.
Yves-Alexandre de Montjoye shows how metadata can work as a fingerprint, identifying people in a large-scale metadata database even though no private information was ever collected, shares a formula that can be used to estimate the privacy of a dataset if you know its spatial and temporal resolution, and offers an overview of OPAL, a project that enables safe big data use using modern CS tools.
Emma Deraze explores a collaborative project between Datakind, Global Witness, and Open Corporates to analyze open UK corporate ownership data and presents findings and insights into the challenges facing open official data, specifically in the context of an international setting, such as complex corporate networks.
Sharing economy platforms are poorly regulated because there is no evidence upon which to draft policies. Daniele Quercia and Giovanni Quattrone propose a means for gathering evidence by matching web data with official socio-economic data and use data analysis to envision regulations that are responsive to real-time demands, contributing to the emerging idea of algorithmic regulation.
Many businesses will have to address EU GDPR as they deploy big data projects. This is an opportunity to rethink data security and deploy a flexible policy framework adapted to big data and regulations. Eric Tilenius explains how consistent visibility and control at a granular level across data domains can address both security and GDPR compliance.
The use of big data and machine learning to detect and predict security threats is a growing trend, with interest from financial institutions, telecommunications providers, healthcare companies, and governments alike. Eddie Garcia explores how companies are using Apache Hadoop-based approaches to protect their organizations and explains how Apache Spot is tackling this challenge head-on.
As the processing capability of the modern platforms come to memory speed, securing big data using encryption usually hurts performance. Haifeng Chen shares proven ways to speed up data encryption in Hadoop and Spark, as well as the latest progress in open source, and demystifies using hardware acceleration technology to protect your data.
Fergal Toomey and Graham Ahearne outline the challenges facing network security in complex industries, sharing key lessons learned from their experiences safeguarding electronic trading environments to demonstrate the utility of machine learning and machine-time network data analytics.
Reiner Kappenberger explains how data encryption and tokenization can help you protect your Hadoop environment and outlines options for securing data and speeding Hadoop implementation, drawing on recent deployments in pharma, health insurance, retail, and telecoms to illustrate the impact to operations and other areas of the business.
Data science continues to generate excitement, and yet real-world results can often disappoint business stakeholders. Martin Goodson offers a personal perspective on the most common failure modes of data science projects and discusses current best practices.
Majken Sander explains how to create a hub and start exploiting open data. Majken discusses which data can be found from external sources and how open data can add value by enhancing existing company data to gain new insights. There is a dataset out there for your business to become even more data driven. Join Majken to find it.
Darren Cook explores the main types of machine-learning algorithms, describing the kinds of task each is suited to, the explainability, repeatability, scalability, training time, sensitivity to data issues, and downsides of each, and the types of answers you can hope to get from them.
Leading companies are integrating operations and analytics to make real-time adjustments to improve revenues, reduce costs, and mitigate risks. There are many aspects to digital transformation, but the timely delivery of actionable data is both a key enabler and an obstacle. Jack Norris explores how companies from Altitude Digital to Uber are transforming their businesses.
More organizations are becoming aware of the value of data and want to get started and scaled up as quickly as possible. But how? Is it possible to get something useful done in five weeks? Kim Nilsson shares her experiences, both good and bad, delivering over 80 five-week data science projects to over 50 organizations, as well as some concrete tips on how to become a data star organization.
Building a data lake involves more than installing and using Hadoop. The goal in most organizations is to build multiuse data infrastructure that is not subject to past constraints. Mark Madsen discusses hidden design assumptions, reviews design principles to apply when building multiuse data infrastructure, and provides a reference architecture.
Since its creation, DataKind has helped charities do some fantastic things with data science through volunteers from the data science community (that's you!). But charities often don't know what to do next. Duncan Ross and Emma Prest share lessons learned from DataKind's projects and outline a data maturity model for doing good with data.
In an era when we are bombarded with data and tasks to finish, our ability to focus our attention becomes critical. When 70% of our code is for DevOps purposes and 90% of our data is dark, the cloud is a welcome, secure, and efficient relief. Yuval Dvir refutes common misconceptions about the cloud and explains why it's not a matter of "if" but "when" you'll move to the cloud.
Natural language generation, the branch of AI that turns raw data into human-sounding narratives, is coming into its own in 2016. Robbie Allen explores the real-world advances in NLG over the past decade and then looks ahead to the next. Computers are already writing finance, sports, ecommerce, and business intelligence stories. Find out whatand howtheyll be writing by 2026.
Visualization and exploratory analytics require subsecond interactions with massive volumes of data, a goal that has remained illusive due to numerous inefficiencies across the stack. Barzan Mozafari offers an overview of Verdict, an open source middleware that guarantees subsecond visualization and analytics and works with Impala, Spark, Hive, and most other engines in the Hadoop ecosystem.
Sean Kandel offers an overview of an entirely new approach to visualizing metadata and data lineage, explaining how to track how different attributes of data are derived during the data preparation process and the associated linkages across different elements in the data.
Measurement Lab is the largest collection of open internet performance data on the planet, with over five petabytes of information about the quality of experience on the internet and more data generated every day. Irene Ros shares recent work to develop a data processing pipeline, API, and visualizations to make the data more accessible.
Robert Schroll demonstrates TensorFlow's capabilities through its Python interface and explores TFLearn, a high-level deep learning library built on TensorFlow.  Join in to learn how to use TFLearn and TensorFlow to build machine-learning models on real-world data.
Deep learning has shown significant promise in common knowledge extraction tasks. However, the reputation of neural networks for being black-box learners can retard adoption in enterprise businesses. Martin Goodson gives a tell-all account of an ultimately successful installation of a deep learning system in an enterprise environment.
O'Reilly recently launched Oriole, a new learning medium for online tutorials that combines Jupyter notebooks, video timelines, and Docker containers run on a Mesos cluster, based the pedagogical theory of computable content. Paco Nathan explores the system architecture, shares project experiences, and considers the impact of notebooks for sharing and learning across a data-centric organization.
Low cost, big impact: this is what data science can bring to your business. Iaki Puigdollers explores how the analytics department changed Social Point games, creating an even better gaming experience and business.
Valuing data can be a headache. The unique properties of data make it difficult to assess its overall value using traditional valuation approaches. John Akred discusses a number of alternative approaches to valuing data within an organization for specific purposes so that you can optimize decisions around its acquisition and management.
The EU's General Data Protection Regulation is an ambitious legal project to reinstate the rights of "data subjects" within an increasingly lucrative data ecosystem. Aurlie Pols explores the legal obligations on companies and their respective interpretations and looks at how scale and integrity will be safeguarded in the data we increasingly base decisions upon in the long term.
In 2007, a computer game company decided to jump ahead of competitors by capturing and using data created during online gaming, but it wasn't prepared to deal with the data management and process challenges stemming from distributed devices creating data. Mark Madsen shares a case study that explores the oversights, failures, and lessons the company learned along its journey.
Big data technology is mature, but its adoption by business is slow, due in part to challenges like a lack of resources or the need for a cultural change. Carme Artigas explains why an analytics center of excellence (ACoE), whether internal or outsourced, is an effective way to accelerate the adoption and shares an approach to implementing an ACoE.
Ben Spivey, Mark Donsky, and Mubashir Kazia walk you through securing a Hadoop cluster. Youll start with a cluster with no security and then add security features related to authentication, authorization, encryption of data at rest, encryption of data in transit, and complete data governance.
Vartika Singh, Jayant Shekhar, and Jeffrey Shmain walk you through various approaches available via the machine-learning algorithms available in Spark Framework (and more) to understand and decipher meaningful patterns in real-world data in order to derive value.
Public cloud usage for Hadoop workloads is accelerating. Consequently, Hadoop components have adapted to leverage cloud infrastructure. Andrei Savu, Vinithra Varadharajan, Matthew Jacobs, and Jennifer Wu explore best practices for Hadoop deployments in the public cloud and provide detailed guidance for deploying, configuring, and managing Hive, Spark, and Impala in the public cloud.
Ian Wrigley demonstrates how to use Kafka Connect and Kafka Streams to build real-world, real-time streaming data pipelinesusing Kafka Connect to ingest data from a relational database into Kafka topics as the data is being generated and then using Kafka Streams to process and enrich the data in real time before writing it out for further analysis.
What are the essential components of a data platform? John Akred and Stephen O'Sullivan explain how the various parts of the Hadoop, Spark, and big data ecosystems fit together in production to create a data platform supporting batch, interactive, and real-time analytical workloads.
Using Entity 360 as an example, Jonathan Seidman, Ted Malaska, Mark Grover, and Gwen Shapira explain how to architect a modern, real-time big data platform leveraging recent advancements in the open source software world, using components like Kafka, Impala, Kudu, Spark Streaming, and Spark SQL with Hadoop to enable new forms of data processing and analytics.
Apache Spark is written in Scala. Hence, many if not most data engineers adopting Spark are also adopting Scala, while most data scientists continue to use Python and R. Dean Wampler offers an overview of the core features of Scala you need to use Spark effectively, using hands-on exercises with the Spark APIs.
R is a top contender for statistics and machine learning, but Spark has emerged as the leader for in-memory distributed data analysis. Douglas Ashton, Kate Ross-Smith, and Mark Sellors introduce Spark, cover data manipulation with Spark as a backend to dplyr and machine learning via MLlib, and explore RStudio's sparklyr package, giving you the power of Spark without having to leave your R session.
James Malone explores using managed Spark and Hadoop solutions in public clouds alongside cloud products for storage, analysis, and message queues to meet enterprise requirements via the Spark and Hadoop ecosystem.
Twenty months into its big data journey, Cox Automotive is using a variety of tools and techniques to deliver actionable insights, transforming decision making within the automotive industry. Allison Nau shares lessons learned, including where to begin transforming a legacy business and industry to become more data driven, how to gain momentum, and how to deliver meaningful results at pace.
Transport for London (TfL) and WSO2 have been working together on broader integration projects focused on getting the most efficient use out of London transport. Roland Major and Sriskandarajah Suhothayan explain how TfL and WSO2 bring together a wide range of data from multiple disconnected systems to understand current and predicted transport network status.
Denis C. Bauer explores how genomic research has leapfrogged to the forefront of big data and cloud solutions, outlines how to deal with big (many samples) and wide (many features per sample) data and how to keep runtime constant by using instantaneously scalable microservices with AWS Lambda, and contrasts Spark- and Lambda-based parallelization.
Many large organizations want to develop data science capabilities, but the traditional complexity and legacy of such companies dont allow a fast and agile evolution toward data-driven decision making. EasyJet is working toward becoming completely data driven. Alberto Rey shares real-world examples on how easyJet is tackling the challenges of scaling up its analytics capabilities.
Sameer Tilak and Anand Iyer offer an overview of recent developments in the Spark ML library and common real-world usage patterns, focusing on how Kaiser Permanente uses Spark ML for predictive analytics on healthcare data. Sameer and Anand share lessons learned building and deploying distributed machine-learning pipelines at Kaiser Permanente.
Wael Elrifai leads a journey through the design and implementation of a predictive maintenance platform for Hitachi Rail. The industrial internet, the IoT, data science, and big data make for an exciting ride.
In 2013, ING announced its Think Forward strategy to empower people in life and in business using advanced analytics (AA). To ING Wholesale Banking's corporate and investment bankers, big data, AA, and AI were a "retail thing." Doron Reuter explains how a team of 15 convinced 15,000 bankers that ING become data driven beyond BI and shares some lessons learned so far on this three-year journey.
Quantitative finance has been a key feature in the financial industry for over 30 years. Big data, machine learning, and AI are increasingly being used today, but is the financial industry actually ready for AI? Aida Mehonic explores some of the most common applications of AI in finance and shares the typical challenges of data transformation and AI adoption that financial institutions face.
Colin White discusses Goldman Sachs's industrialization program, under which it is digitizing processes, rules, and data in order to decrease cost, reduce time to market, and manage the risk of repetitive business processes. Goldman Sachs is taking models seriously; the data that is generated offers real insights into how to optimize its business.
Big data for retail is a step-change innovation path to improving analytics and real-time capabilities to enable more effective and efficient business services, processes, and products. Fabio Oberto and Ivan Luciano Danesi explore a value-retention use case and outline a CRM solution that manages massive data to support business and create value through machine learning and predictive analytics.
Regtech is one of the fastest growing areas in financial world. Tanvi Singh showcases the use of data science tools and techniques in this space and offers a holistic view of how to do surveillance and monitoring using a man + machine approach.
Drawing on use cases from Trifacta customers, Olivier de Garrigues explains how to leverage data wrangling solutions in the insurance industry to streamline, strengthen, and improve data analytics initiatives on Hadoop.
The speed of a machine-learning algorithm can be crucial in problems that require retraining in real time. Mathew Salvaris and Miguel Gonzlez-Fierro introduce Microsoft's recently open sourced LightGBM library for decision trees, which outperforms other libraries in both speed and performance, and demo several applications using LightGBM.
Reliable prediction is the ability of a predictive model to explicitly measure the uncertainty involved in a prediction without feedback. Robin Senge shares two approaches to measure different types of uncertainty involved in a prediction.
Dafna Shahaf explains how to scale up analogy with crowdsourcing and machine learning.
Causal relationships empower us to understand the consequences of our actions and decide what to do next. This is why identifying causal effects has been at the heart of data science. Kay Brodersen offers an introduction to CausalImpact, a new analysis library developed at Google for identifying the causal effect of an intervention on a metric over time.
Identifying the relationships between time series metrics lets them be used for predictions, root cause diagnosis, and more. Ira Cohen shares accurate methods that work on a large scale (e.g., behavioral pattern similarity clustering algorithms) and strategies for reducing FPs and FNs, reducing computational resources, and distinguishing correlation and causation.
Built on Apache Spark, BigDL provides deep learning functionality parity with existing DL frameworkswith better performance. Ding Ding explains how BigDL helps make the big data platform a unified data analytics platform, enabling more accessible deep learning for big data users and data scientists.
There are sometimes occasions where the labels on data are insufficient. In such situations, semisupervised learning can be of great practical value. Yingsong Zhang explores illustrative examples of how to come up with creative solutions, derived from textbook approaches.
Aida Mehonic explains how ASI Data Science has trained a deep neural net on historical prices of liquid financial contracts. The neural net has already outperformed comparable strategies based on expert systems.
Alan Mosca discusses using ensembles in deep learning and tackles a benchmark problem in computer vision with Toupee, a library and toolkit for experimentation with deep learning and ensembles.
Cortexica had the first commercial implementation of a deep convolutional network in a GPU back in 2010. However, in the real world, running a CNN is not always a possibility. Eduard Vazquez discusses current challenges that commercial applications based on this technology are facing and how some of them can be tackled.
Program chairs Roger Magoulas, Doug Cutting, and Alistair Croll welcome you to the first day of keynotes.
Program chairs Roger Magoulas, Doug Cutting, and Alistair Croll welcome you to the second day of keynotes.
To handle real-time big data, you need to solve two difficult problems: how do you ingest that much data, and how will you process that much data? Jesse Anderson explores the latest real-time frameworksboth open source and managed cloud servicesdiscusses the leading cloud providers, and explains how to choose the right one for your company.
Real-time data analysis is becoming more and more important to Internet companies daily business. Qunar has been running Alluxio in production for over a year. Lei Xu explores how stream processing on Alluxio has led to a 16x performance improvement on average and 300x improvement at service peak time on workloads at Qunar.
Verdi March demystifies deep learning and shares his experience on how to gradually transition to deep learning. Using a specific example in computer vision, Verdi touches upon key differences in engineering traditional software versus deep learning-based software.
Shao Wei Ying explains how mobility intelligence derived from telco big data informs us about the state of our urban infrastructure, economic activities, and public safety.
Making recommendations for the food and beverage industry is tricky as they must take into consideration the user's context (location, time, day, etc.) in addition to the constraints of a regular recommendation algorithm. Arun Veettil explains how to incorporate user contextual information into recommendation algorithms and apply reinforcement learning to track continuously changing user behavior.
Application developers have long created complex schemas to handle storing with minor relationships in an RDBMS. This talk will show how to convert an existing (complicated schema) music database to HBase for transactional workloads, plus how to use Drill against HBase for real-time queries. HBase column families will also be discussed.
As the number of products on Lazada grows exponentially, helping customers find relevant, quality products is key to customer experience. Eugene Yan shares how Lazada ranks products on its website, covering how Lazada scales data pipelines to collect user-behavioral data, cleans and prepares data, creates simple features, builds models to meet key objectives, and measures outcomes.
With the rise of deep learning, natural language understanding techniques are becoming more effective and are not as reliant on costly annotated data. This leads to an explosion of possibilities of what businesses can do with language. Alyona Medelyan explains what the newest NLU tools can achieve today and presents their common use cases.
Chi-Yi Kuan, Weidong Zhang, and Yongzheng Zhang explain how LinkedIn has built a "voice of member" platform to analyze hundreds of millions of text documents. Chi-Yi, Weidong, and Yongzheng illustrate the critical components of this platform and showcase how LinkedIn leverages it to derive insights such as customer value propositions from an enormous amount of unstructured data.
When operating on billions of data events per day, modern AI and machine-learning programs require distributed clusters with tens to hundreds machines. Qirong Ho offers an introduction to high-efficiency AI and ML distributed systems developed as part of the Petuum open source project and explains how they can reduce capital and operational costs for businesses.
Can you imagine an intelligent software to assist in your decision making and drive actions? Flavio Clesio and Eiti Kimura offer a practical demonstration of using machine learning to create an intelligent monitoring application based on a distributed system data analysis using Apache Spark MLlib.
Ofer Ron examines the development of LivePerson's traffic targeting solution from a generic to a domain-specific implementation to demonstrate that a thorough understanding of the problem domain is essential to a good machine-learning-based product. Ofer then reviews the underlying architecture that makes this possible.
Jared Lander worked with the Minnesota Vikings to bring moneyball to football for the 2015 NFL draft. Join Jared as he dives further into football, using statistical modeling and R to analyze opponent play-calling, examine when the New York Giants will run or pass the ball, and discern quarterback Eli Manning's favorite receivers.
Ted Malaska and Mark Grover cover the top five things that prevent Spark developers from getting the most out of their Spark clusters. When these issues are addressed, it is not uncommon to see the same job running 10x or 100x faster with the same clusters and the same data, using just a different approach.
The success of Apache Spark is bringing developers to Scala. For big data, the JVM uses memory inefficiently, causing significant GC challenges. Spark's Project Tungsten fixes these problems with custom data layouts and code generation. Dean Wampler gives an overview of Spark, explaining ongoing improvements and what we should do to improve Scala and the JVM for big data.
Hybrid cloud architectures marry the flexibility to scale workloads on-demand in the public cloud with the ability to control mission-critical applications on-premises. Publish-subscribe message streams offer a natural paradigm for hybrid cloud use cases. Mathieu Dumoulin describes how to architect a real-time, global IoT analytics hybrid cloud application with a Kafka-based message stream system.
Alluxio is an open source memory-speed virtual distributed storage system. In the past year, the Alluxio open source community has grown to more than 300 developers. The project also experienced a tremendous improvement in performance and scalability and was extended with new features. Haoyuan Li offers an overview of Alluxio, covering its use cases, its community, and the value it brings.
Jorge Pablo Fernandez and Nicolette Bullivant explore Santander Bank's Spendlytics app, which helps customers track their spending by offering a listing of transactions, transactions aggregations, and real-time enrichment based on the categorization of transactions depending on market and brands. Along they way, they share the challenges encountered and lessons learned while implementing the app.
Todd Lipcon and Marcel Kornacker provide an introduction to using Impala + Kudu to power your real-time data-centric applications for use cases like time series analysis (fraud detection, stream market data), machine data analytics, and online reporting.
Jason Dai and Yiheng Wang share their experience building web-scale machine learning using Apache Sparkfocusing specifically on "war stories" (e.g., in-game purchase, fraud detection, and deep leaning)outline best practices to scale these learning algorithms, and discuss trade-offs in designing learning systems for the Spark framework.
If your organization has Hadoop clusters in research or as point solutions and you're wondering where you go from there, this session is for you. Phillip Radley explains how to run Hadoop as a service, providing an enterprise-wide data platform hosting hundreds of projects securely and predictably.
Mediacorp analyzes its online audience through a computationally and economically efficient cloud-based platform. The cornerstone of the platform is Apache Spark, a framework whose clean APIs and performance gains make it an ideal choice for data scientists. Andrea Gagliardi La Gala and Brandon Lee highlight the platforms architecture, benefits, and considerations for deploying it in production.
Alex Gutow and Henry Robinson explain how Apache Hadoop and Apache Impala (incubating) take advantage of the benefits of the cloud to provide the same great functionality, partner ecosystem, and flexibility of on-premises deployments combined with the flexibility and cost efficiency of the cloud.
Everyone is talking about data lakes. The intended use of a data lake is as a central storage facility for performing analytics. But, Jim Scott asks, why have a separate data lake when your entire (or most of your) infrastructure can run directly on top of your storage, minimizing or eliminating the need for data movement, separate processes and clusters, and ETL?
Building a data lake involves more than installing and using Hadoop. The goal in most organizations is to build multiuse data infrastructure that is not subject to past constraints. Mark Madsen discusses hidden design assumptions, reviews design principles to apply when building multiuse data infrastructure, and provides a reference architecture.
Big data and data science have great potential for accelerating business, but how do you reconcile the business opportunity with the sea of possible technologies? Data should serve the strategic imperatives of a businessthose aspirations that will define an organizations future vision. Scott Kurth and John Akred explain how to create a modern data strategy that powers data-driven business.
Big data and data science have great potential for accelerating business, but how do you reconcile the business opportunity with the sea of possible technologies? Data should serve the strategic imperatives of a businessthose aspirations that will define an organizations future vision. Scott Kurth and John Akred explain how to create a modern data strategy that powers data-driven business.
IHI has developed a common platform for remote monitoring and maintenance and has started leveraging Spark MLlib to get up speed developing applications for process improvement and product fault diagnosis. Yoshitaka Suzuki and Masaru Dobashi explain how IHI used PySpark and MLlib to improve its services and share best practices for application development and lessons for operating Spark on YARN.
Twitter generates billions and billions of events per day. Analyzing these events in real time presents a massive challenge. Maosong Fu offers an overview of the end-to-end real-time stack Twitter designed in order to meet this challenge, consisting of DistributedLog (the distributed and replicated messaging system) and Heron (the streaming system for real-time computation).
Apache Beam (incubating) defines a new data processing programming model evolved from more than a decade of experience building big data infrastructure within Google. Beam pipelines are portable across open source and private cloud runtimes. Dan Halperin covers the basics of Apache Beamits evolution, main concepts in the programming mode, and how it compares to similar systems.
Watermarks are a system for measuring progress and completeness in out-of-order streaming systems and are utilized to emit correct results in a timely manner. Given the trend toward out-of-order processing in existing streaming systems, watermarks are an increasingly important tool when designing streaming pipelines. Slava Chernyak explains watermarks and explores real-world applications.
Picking up where his talk at Strata + Hadoop World in London left off, Gopal GopalKrishnan shares lessons learned from using components of the big data ecosystem for insights from industrial sensor and time series data and explores use cases in predictive maintenance, energy optimization, process efficiency, production cost reduction, and quality improvement.
Takayuki Nishikawa and Ei Yamaguhi explain how Panasonic developed an integrated data analytics platform to analyze the increasing number of home appliances logs from its IoT products, achieving scalability for millions of households and a 10x improvement in processing time with Hadoop and Hive, in the process gaining more reliable knowledge about users lifestyles with Spark MLlib.
Translating streaming, real-time telecommunications data into actionable analytics products remains challenging. Boon Siew Seah explores SmartHubs past successes and failures building telco analytics products for its customers and shares the big data technologies behind its two API-based telco analytics products: Grid360 (geolocation analytics) and C360 (consumer insights).
Modern telecommunications are alphabet soups that produce massive amounts of diagnostic data. Ted Dunning offers an overview of a real-time, low-fidelity simulation of the edge protocols of such a system to help illustrate how modern big data tools can be used for telecom analytics. Ted demos the system and shows how several tools can produce useful analytical results and system understanding.
One challenge when dealing with manufacturing sensor data analysis is to formulate an efficient model of the underlying physical system. Rajesh Sampathkumar shares his experience working with sensor data at scale to model a real-world manufacturing subsystem with simple techniques, such as moving average analysis, and advanced ones, like VAR, applied to the problem of predictive maintenance.
Jennifer Marsman, Ranveer Chandra, and Wee Hyong Tok explore the various drone technologies that are currently available and explain how to acquire and analyze real-time signals from drones to design intelligent applications.
Aljoscha Krettek offers a very short introduction to stream processing before diving into writing code and demonstrating the features in Apache Flink that make truly robust stream processing possible. All of this will be done in the context of a real-time analytics application that we'll be modifying on the fly based on the topics we're working though.
Rebecca Tien Yu Lin and Mon-Fong Mike Jiang offer an overview of a Hadoop-based big data solution helping the semiconductor industry increase yield by monitoring the huge amount of tool logs and the data generated from the FDC system.
Mark Grover, Ted Malaska, and Jonathan Seidman explain how to architect a modern, real-time big data platform leveraging recent advancements in the open source software world and discuss how to use components like Kafka, Impala, Kudu, Spark Streaming, and Spark SQL with Hadoop to enable new forms of data processing and analytics.
Wolff Dobson walks you through training and deploying a machine-learning system using TensorFlow, a popular open source library, and demonstrates how to build machine-learning systems from simple classifiers to complex image-based models.
Mark Grover, Ted Malaska, and Jonathan Seidman explain how to architect a modern, real-time big data platform leveraging recent advancements in the open source software world and discuss how to use components like Kafka, Impala, Kudu, Spark Streaming, and Spark SQL with Hadoop to enable new forms of data processing and analytics.
Wolff Dobson walks you through training and deploying a machine-learning system using TensorFlow, a popular open source library, and demonstrates how to build machine-learning systems from simple classifiers to complex image-based models.
Stop copying and pasting your D3.js visualization code each time you start a new project and start writing intelligent visualization software. Michael Freeman demonstrates how to build modular, reusable charting code by leveraging foundational JavaScript principles (such as closures) and the reusability structure used internally by the D3.js library.
Stop copying and pasting your D3.js visualization code each time you start a new project and start writing intelligent visualization software. Michael Freeman demonstrates how to build modular, reusable charting code by leveraging foundational JavaScript principles (such as closures) and the reusability structure used internally by the D3.js library.
Julie Rodriguez and Piotr Kaczmarek provide a fresh take on data visualizations with an extensive set of case studies that contrast traditional uses of charts with new methods that provide more effective representations of the data to produce greater insights.
Mark Grover, Jonathan Seidman, and Ted Malaska, the authors of Hadoop Application Architectures, participate in an open Q&A session on considerations and recommendations for the architecture and design of applications using Hadoop. Come with questions about your use case and its big data architecture or just listen in on the conversation.
O'Reilly recently launched Oriole, a new learning medium for online tutorials that combines Jupyter notebooks, video timelines, and Docker containers run on a Mesos cluster, based the pedagogical theory of computable content. Paco Nathan explores the system architecture, shares project experiences, and considers the impact of notebooks for sharing and learning across a data-centric organization.
Matt Winkler and Jennifer Marsman explain how to easily extend your apps and services with bots to reach users where they arein messaging appscovering use cases and case studies, how to quickly get started building a bot, how to process input using linguistic analysis, and how to deploy and integrate bots with messaging apps.
The interconnected world presents unprecedented opportunities to gain new insights on behavior, both human and nonhuman alike. Likewise, it also poses unprecedented challenges on how organizations can act on these moments of opportunities in time. Michael O'Connell and San Zaw share real-world case studies demonstrating how real-time analytics solves these challenges.
When data is transformed into visualizations, the impact can sometimes be lost on the user. Drawing on her work with Doctors Without Borders, Vivian Peng explains how emotions help convey impact and move people to take action and demonstrates how we might design emotions into the data visualization experience.
To communicate high-value, data-driven insights, analysts need stories, but data storytelling is difficult. Lisa Collins demonstrates how to build a compelling story with the right blend of narrative, data, and visualization.
Implementing a data governance strategy that is agile enough to take on the new technical challenges of big data while being robust enough to meet corporate standards is a huge, emerging challenge. Clara Fletcher explores what next-generation data governance will look like and what the trends will be in this space.
Data already plays an important role as raw material for art, from algorithmic visualization and parametric architecture to works created entirely by autonomous machines. With data-driven art, data science now touches even the most human aspects of culture. Heather Dewey-Hagborg and Joerg Blumtritt share examples and discuss possible routes for future data art.
Data ethics covers more than just privacy. In a connected world where most people rely on data-driven services, opting out and locking data away is hardly an option. More important than keeping data private is ensuring fairness and preventing abuse. Joerg Blumtritt and Heather Dewey-Hagborg show how to deal with data in an ethical way that has sound economic value.
Apache Spark is written in Scala. Hence, manyif not mostdata engineers adopting Spark are also adopting Scala, while most data scientists continue to use Python and R. Dean Wampler offers an overview of the core features of Scala you need to use Spark effectively, using hands-on exercises with the Spark APIs.
Sean Owen and Juliet Hougland offer a practical overview of the basics of using Python data tools with a Hadoop cluster, covering HDFS connectivity and dealing with raw data files, running SQL queries with a SQL-on-Hadoop system like Apache Hive or Apache Impala (incubating), and using Apache Spark to write some more-complex analytical jobs.
Apache Spark is written in Scala. Hence, manyif not mostdata engineers adopting Spark are also adopting Scala, while most data scientists continue to use Python and R. Dean Wampler offers an overview of the core features of Scala you need to use Spark effectively, using hands-on exercises with the Spark APIs.
Sean Owen and Juliet Hougland offer a practical overview of the basics of using Python data tools with a Hadoop cluster, covering HDFS connectivity and dealing with raw data files, running SQL queries with a SQL-on-Hadoop system like Apache Hive or Apache Impala (incubating), and using Apache Spark to write some more-complex analytical jobs.
Spark is white-hot, but why does it matter? Some technologies cause more excitement than others, and at first the only people who understand why are the developers who use them. John Akred offers a tour through the hottest emerging data technologies of 2016 and explains why theyre exciting, in the context of the new capabilities and economies they bring.
YARN includes security features such as SSL encryption, Kerberos-based authentication, and HDFS encryption. Nitin Khandelwal and Abhishek Modi share the challenges they faced in enabling these features for ephemeral clusters running in the cloud with multitenancy support as well as performance numbers for different encryption algorithms available.
Wolff Dobson answers your questions about TensorFlow.
Nir Lotan describes a new, free software tool based on existing deep learning frameworks that enables the fast and easy creation of deep learning models and incorporates extensive optimizations that provide high performance on standard CPUs.
If your design only focuses on the processing layer to get speed and power, you may be leaving a significant amount of optimization untapped. Ted Malaska describes a set of storage design patterns and schemas implemented on HBase, Kudu, Kafka, SolR, HDFS, and S3 that, by carefully tailoring how data is stored, can reduce processing and access times by two to three orders of magnitude.
Creating big data solutions that can process data at terabyte scale and produce spatial-temporal real-time insights at speed demands a well-thought-through system architecture. Chandras Sekhar Saripaka details the production architecture at DataSpark that works through terabytes of spatial-temporal telco data each day in PaaS mode and showcases how DataSpark operates in SaaS mode.
Hao Hao and Alex Leblang explore the architecture of Apache Sentry and RecordService (RS) for Hadoop in the cloud, which provides unified, fine-grained authorization via role- and attribute-based access controls, walking you through using Apache Sentry and RS to protect sensitive data on a multitenant cloud across the Hadoop ecosystem.
Holden Karau and Seth Hendrickson demonstrate how to do streaming machine learning using Spark's new Structured Streaming and walk you through creating your own streaming model.
With enterprise adoption of Apache Spark come enterprise security requirements and the need to meet enterprise security standards. Vinay Shukla walks you through enterprise security requirements, provides a deep dive into Spark security features, and shows how Spark meets these enterprise security requirements.
Deep learning has made a huge impact on predictive analytics and is here to stay, so you'd better get up to speed with the neural net craze. Mateusz Dymczyk explains why all the top companies are using deep learning, what it's all about, and how you can start experimenting and implementing deep learning solutions in your business in only a few easy steps.
Adam Gibson offers a brief overview of deep reinforcement learning on Spark, exploring how to run large-scale training on Spark and the implications on deep reinforcement learning targeting the doom environment.
Ever wondered how Google Translate works so well, how the autocaptioning works on YouTube, or how to mine the sentiments of tweets on Twitter? Whats the underlying theme? They all use deep learning. Bargava Subramanian and Amit Kapoor explore artificial neural networks and deep learning for natural language processing to get you started.
The real power and value proposition of Apache Spark is in building a unified use case that combines ETL, batch analytics, real-time stream analysis, machine learning, graph processing, and visualizations. Through hands-on examples, Sameer Farooqui explores various Wikipedia datasets to illustrate a variety of ideal programming paradigms.
The real power and value proposition of Apache Spark is in building a unified use case that combines ETL, batch analytics, real-time stream analysis, machine learning, graph processing, and visualizations. Through hands-on examples, Sameer Farooqui explores various Wikipedia datasets to illustrate a variety of ideal programming paradigms.
In 2007, a computer game company decided to jump ahead of competitors by capturing and using data created during online gaming, but it wasn't prepared to deal with the data management and process challenges stemming from distributed devices creating data. Mark Madsen shares a case study that explores the oversights, failures, and lessons the company learned along its journey.
Huge amounts of data are generated every minute by nearly every company.. .which largely goes unused. Historically, so-called data exhaust has been collected for the purpose of manual analysis in the case of a fault or failure. Cameron Turner explains why companies are increasingly looking to their data exhaust as a valuable asset to influence their revenue and profit through machine learning.
Embedding operational analytics with the IoT enables organizations to act on insights in real time. Devin Deen and Dnyanesh Prabhu walk you through examples from Sky TV and NZ Bustwo businesses that iteratively developed their analytic capabilities integrating the IoT on Hadoop, allowing people and process changes to keep pace with technical enablement.
Combining the power of the IoT and big data analytics opens the doors to a wide range of opportunities for organizations to solve new challenges that create an impact on the world that we live in. Frank Saeuberlich and Karthik Thirumalai explain why data management, data integration, and multigenre analytics are foundational to driving business value from IoT initiatives.
Santander was one of the last big banks in the UK to start using Hadoop and other big data technologies. However, the maturity of the technology made it possible to create a customer-facing data product in production in less than a year and a fully adopted production analytics platform in less than two. Antonio Alvarez shares what other late entrants can learn from this experience.
The unique properties of data make it difficult to assess its overall value using traditional valuation approaches. John Akred discusses a number of alternative approaches to valuing data within an organization for specific purposes so that you can optimize decisions around its acquisition and management.
Creating better models is a critical component to building a good data science product. It is relatively easy to build a first-cut machine-learning model, but what does it take to build a reasonably good or state-of-the-art model? Ensemble modelswhich help exploit the power of computing in searching the solution space. Bargava Subramanian discusses various strategies to build ensemble models.
The opportunity to harness data to impact business is ripe, and as a result, every industry, every organization, and every department is going through a huge change, whether they realize it or not. John Kreisa shares use cases from across Asia and Europe of businesses that are successfully leveraging new platform technologies to transform their organizations using data.
For decades, business intelligence companies have strived to make their products easier to use in the hope that they could finally reach the mythical subject-matter expertthat wondrous individual who would change the course of the company if only she had access to the data she needed. Drawing on his real-world experience, Chris Neumann asks, "What if the subject-matter expert doesnt exist?"
Shopback, a company that gives cash back to customers for successful transactions covering various lifestyles, crawls 25 million products from multiple ecommerce websites to provide a smooth customer experience. Qiaoliang Xiang walks you through how to crawl and update products, how to scale it using big data tools, and how to design a modularized system.
Anusua Trivedi proposes a method to apply a pretrained deep convolution neural network (DCNN) on images to improve prediction accuracy. This approach improves prediction accuracy on domain-specific image datasets compared to state-of-the-art machine-learning approaches.
Marketing has become ever more data driven. While there are thousands of marketing applications available, it is challenging to get an end-to-end line of sight and fully understand customers. Franz Aman explains how bringing the data from the various applications and data sources together in a data lake changes everything.
Vartika Singh and Jayant Shekhar offers a hands-on tutorial that exposes you to techniques for building and tuning machine-learning apps using Spark ML libraries, building pipelines, tuning parameters, and graph processing with GraphX.
Tyler Akidau, Slava Chernyak, and Dan Halperin offer a guided walkthrough of Apache Beam (incubating)the most sophisticated and portable stream processing model on the planetcovering the basics of robust stream processing (windowing, watermarks, and triggers) with the option to execute exercises on top of the runner of your choice (Flink, Spark, or Google Cloud Dataflow).
Vartika Singh and Jayant Shekhar offers a hands-on tutorial that exposes you to techniques for building and tuning machine-learning apps using Spark ML libraries, building pipelines, tuning parameters, and graph processing with GraphX.
Tyler Akidau, Slava Chernyak, and Dan Halperin offer a guided walkthrough of Apache Beam (incubating)the most sophisticated and portable stream processing model on the planetcovering the basics of robust stream processing (windowing, watermarks, and triggers) with the option to execute exercises on top of the runner of your choice (Flink, Spark, or Google Cloud Dataflow).
The future of big data is AI, and the future is here. With machine learning and deep learning, with robotics and heuristics, with fuzzy logic and AI, the convergence is changing myriad industries like healthcare, banking, insurance, and gaming. Raju Chellam explains why its time to step back and consider how big data with HPC and AI can make a key difference in your management.
Join Lean Analytics author, Harvard lecturer, and Strata chair Alistair Croll for a look at how to think critically about data, based on his Harvard Business School course.
Powerful artificial intelligence is emerging as the underlying technology for future intelligent applications. Building new applications requires considerable analysis of data performed on powerful computers. Graham Williams explains why the cloud offers a cost-effective platform for developers and demonstrates this capability with actual applications.
Ted Dunning explains how a stream-first approach simplifies and speeds development of applications, resulting in real-time applications that have significant impact. Along the way, Ted contrasts a stream-first approach with existing approaches that start with an application that dictates specialized data structures, ETL activities, data silos, and processing delays.
In the past decade, enterprises have made massive investments in IT to keep pace with increasing data volumes and velocity. Similarly, the tool set to solve use cases for developers, data scientists, and analysts is ever-expanding. Grant Salisbury and Zhiwei Jiang explore how enterprises can create more value from IT investments and how big data can improve decisions across the enterprise.
Join in to meet four experts who will share their views of the people, processes, and technologies that are driving information transformation around the world, including machine learning, big data, the cloud, and distributed computing. Find out why the role of chief data officer is at the center of driving tangible business value from data across the enterprise.
Minh Chau Nguyen and Hee Sun Won demonstrate how all metadata from system, service, and user can be managed in one unified platform across many geographically distributed data centers by extending the overall architecture of the Hadoop ecosystem so that multiple tenants and authorized third parties can securely access and modify the metadata in runtime via a so-called metadatabase.
Mathieu Dumoulin offers an overview of stream processing and explains how to simplify a seemingly complex real-time enterprise streaming architecture using an open source business rules engine and Apache Kafka API streaming. Mathieu then illustrates this architecture with a demo based on a successful production use case for Busan, South Korea's Smart City initiative.
Raghunath Nambiar explores the architectural components of building large-scale big data systems with Cisco UCS, as well as use cases, lessons learned, best practice guidelines, and Cisco Validated Designs.
Organizations used to store information in separate silos. As a result, searching for the data you needed was a difficult affair. KC Wong explores big data analytics (BDA) platforms that can produce what you need in a much shorter timeframe and are even intelligent enough to present exactly what you need for greater efficiency, productivity, and profits.
Garbage in, garbage outthis truism has become significantly more impactful for big data as companies have moved away from traditional schema-based approaches to more flexible and dynamic file system approaches. Steve Jones explains how to add governance, schema evolution, and the industrialization required to deliver true enterprise-grade big data solutions.
Mediatrac, a big data technology platform focused on data connectivity, object profiling, and knowledge discovery, allows businesses and startups to build advanced analytic solutions on top of it. Imron Zuhri shares several data connectivity use cases and explains how to leverage distributed computing to tackle massive entity recognition and resolution problems.
Big data makes big promises, but when business users don't pay attention, your work and insights are wasted. Patrick Nord explains how to use a methodology developed by data scientists and designers to help you efficiently and effectively communicate your transformative insightseven to unreceptive executive teams.
Raymond Chan dives into the trials and tribulations of DataKind SG, a data science consulting social good organization operating in the digitally underserved but rapidly developing frontier of Southeast Asia.
Maojin Jiang demonstrates how Spark and Hadoop enable data scientists to help companies reduce costs, increase profits, improve products, retain customers, and identify new opportunities. Through in-class simulations and exercises, Maojin walks you through applying data science methods to real-world challenges in different industries, offering preparation for data scientist roles in the field.
Maojin Jiang demonstrates how Spark and Hadoop enable data scientists to help companies reduce costs, increase profits, improve products, retain customers, and identify new opportunities. Through in-class simulations and exercises, Maojin walks you through applying data science methods to real-world challenges in different industries, offering preparation for data scientist roles in the field.
Maojin Jiang demonstrates how Spark and Hadoop enable data scientists to help companies reduce costs, increase profits, improve products, retain customers, and identify new opportunities. Through in-class simulations and exercises, Maojin walks you through applying data science methods to real-world challenges in different industries, offering preparation for data scientist roles in the field.
The real power and value proposition of Apache Spark is in building a unified use case that combines ETL, batch analytics, real-time stream analysis, machine learning, graph processing, and visualizations. Andy Huang employs hands-on exercises using various Wikipedia datasets to illustrate the variety of ideal programming paradigms Spark makes possible.
The real power and value proposition of Apache Spark is in building a unified use case that combines ETL, batch analytics, real-time stream analysis, machine learning, graph processing, and visualizations. Andy Huang employs hands-on exercises using various Wikipedia datasets to illustrate the variety of ideal programming paradigms Spark makes possible.
The real power and value proposition of Apache Spark is in building a unified use case that combines ETL, batch analytics, real-time stream analysis, machine learning, graph processing, and visualizations. Andy Huang employs hands-on exercises using various Wikipedia datasets to illustrate the variety of ideal programming paradigms Spark makes possible.
Program chairs Ben Lorica, Doug Cutting, and Alistair Croll welcome you to the first day of keynotes.
Program chairs Ben Lorica, Doug Cutting, and Alistair Croll welcome you to the second day of keynotes.
Mike Olson explores the latest trends on how organizations are using big data to drive board-level business decisions. Mike will be joined by Dave Gledhill, who will share how DBS is using big data and customer 360 to improve customer experience and drive ATM network and customer call center operations efficiency.
Julie Rodriguez discusses the idea of visualizing data to render its individual characteristics while simultaneously encapsulating and projecting out the personality of data.
Machine learning and artificial intelligence show great promise, but, really, machine learning and deep learning are already here and being used everywhere around you.  Find out how Google uses large-scale machine learning in many of its products, and how TensorFlow and ML can help your business (and even help you make art and music).
Susan Etlinger lays out the market opportunities, challenges, and use cases for image intelligence and offers recommendations for organizations that wish to unlock the predictive potential of visual content.
Since its inception, big data solutions have best been known for their ability to master the complexity of the volume, variety, and velocity of data. But as we enter the era of data democratization, theres a new set of concerns to consider. Amr Awadallah discusses the new dynamics of big data and explains how a renewed approach focused on where, who, and why can lead to cutting-edge solutions.
OCBC has long been recognized as a leader in traditional analytics within Southeast Asia, investing over $100M to build its capability. Donald MacDonald shares how OCBC determined that what had worked in the past may not succeed in the future and how it built support for its next-generation platform.
Most consumer-facing personalization today is rudimentary and coarsely targeted at best, and designers dont give users cues for how they are meant to interact with and interpret personalized experiences and interfaces. Sara Watson makes the case for personalization signals that give context to personalization and expose levers of control to users.
M. C. Srivas covers the technologies underpinning the big data architecture at Uber and explores some of the real-time problems Uber needs to solve to make ride sharing as smooth and ubiquitous as running water, explaining how they are related to real-time big data analytics.
Elevating the intelligence of the whole ecosystem of things is imperative to ensure the proliferation of IoT devices doesn't result in disparate, detached machinery. Isaac Jacob explains how Fusionex intends to achieve this by leveraging robust, intuitive BDA solutions with real-time capabilities.
Automation technologies such as machine learning and robotics play an increasingly important role in everyday life, and their potential effect on the workplace has become a cause for concern. Ferry Grijpink explores which jobs will or won't be replaced by machines.
Zia Zaman shares seven predictions about the future of insurance.
Vijay Narayanan explains how rapid advances in algorithms, the cloud, the Internet of Things, and data are driving unimaginable breakthroughs in every human endeavor, across agriculture, healthcare, education, travel, smart nations, and more.
Today, we are able to collect, manage, and query data with very advanced data processing frameworks like Apache Hadoop in both on-premises and cloud deployments, yet turning data into trustworthy information is one of the toughest challenges facing businesses. Prakash Nanduri explains how to deal with the challenge of data swamps of #deplorable data.
There is a symbiotic relationship between predictive modeling and Big Data. Performance gets better with more data and predictive models demonstrate like few other techniques the value of Big Data. However, there is a surprising paradox: when you need models most, even all the data is not enough or just not suitable. So in the days and age of Big Data there remains an art to predictive modeling.
In this debate, two teams of the world's best data scientists will debate the following proposition: "If you can't code, you can't be a data scientist."
An increasingly common task for data science is the measurement and attribution of experimental impact. Using examples from healthcare.gov, Microsoft advertising, and Bing experimentation, we will explore the strengths, weaknesses, and pitfalls of techniques for dealing with impact and attribution in scenarios/data in which control experiments were not possible or otherwise not performed.
A talk about how the largest professional social network in the world is digitally mapping the global economy to connect talent with opportunity at massive scale.
The IPython Notebook is an open-source, web-based interactive computing environment. The Notebook enables users to author documents that combine live code, descriptive text, mathematical equations, images, videos, and arbitrary HTML. This talk will describe how IPython is evolving to support a wide range of programming languages relevant in data science, including Python, Julia, and R.
Just in the US, we make over ~40 billion queries every month. From the time we wake up,  search engines are one of the top activities we do online, this talk will show some examples on how this data can be used from funny things like determining which city wakes up earlier to more complex scenarios like finding adverse drug interactions.
What does AI mean in 2014, and where is it headed? Every day brings news of purported breakthroughs, and some of the new applications are certainly impressive, but the field has witnessed boom/bust cycles before. What are the challenges that lie ahead this time? This talk will provide an overview of the state of the field, as well as a critical framework for thinking about the years ahead.
We debunk some popular approaches and attitudes we have encountered over the course of more than 50 real world Big Data implementations. We will describe each anti-pattern and its appeal--but also why they fail, and how to do it right.
Microsoft Translator currently supports 100+ languages. We constantly improve the translation quality, add new scenarios, all with a constant team size. This session describes a production scale ML architecture using MS Translator as a case study. You will learn the mental model to approach your ML problem and concrete Dos and Donts for the various components of the ML system architecture.
Karen Moon, Co-founder and CEO, Trendalytics
H2O presents the worlds fastest Distributed Parallel GBM.  GBM is a ML algorithm used to win many recent Kaggle competitions, and is well known for it's high quality results.
Strata Program Chairs, Roger Magoulas, Doug Cutting, and Alistair Croll, welcome you to the first day of keynotes.
Mike Olson, CSO and Chairman, Cloudera
If you want to know what's coming next in big data, just ask yourself, "what would Google do?
Miriah Meyer, Assistant Professor of Computer Science, University of Utah
This talk introduces how Intel is  working with scientists and physicians to help improve research, treatment, and drug development for Parkinsons Disease using data science and enabling the Parkinson's research community to build upon an open platform for big data analytics.
Data is an evolving story. Its not a static snapshot of a point in time insight. With data from internal and external sources constantly updating, we are evolving from rear-view mirror dashboard views into an era of interactive Storytelling.
Amanda Cox, Graphics Operator, The New York Times
Spark represents the next-step function leap in what is possible with Hadoop, but what does that mean for business analysts that are swimming in multi-structured data? This presentation discusses the new workflow required so that business analysts can work with massive volumes of multi-structured data to find new insights today, instead of continually having to wait for IT to make big data small.
There are two essential skills for the data scientist: engineering and statistics. A great many data scientists are very strong engineers but feel like impostors when it comes to statistics. In this talk John will argue that the ability to program a computer gives you special access to the deepest and most fundamental ideas in statistics.
Strata Program Chairs, Roger Magoulas, Doug Cutting, and Alistair Croll, welcome you to the second day of keynotes.
In this presentation Eli Collins, Clouderas Chief Technologist, will discuss how we might both reap the benefits of data while avoiding its perils.
This keynote will share insights from the worlds largest repository of consumer emotions and present the challenges and opportunities that this data presents for machine learning as well as data mining and visualization.
Software and the rise of cloud services have given rise to revolutionary new economies  creating new markets for everything from self-published books, music and videos to mobile apps.  Only a few years ago, it would have been hard to imagine developers authoring a million apps for smartphones.  But thats history.
Karen Moon will discuss the characteristics of unstructured data that makes identifying and synthesizing fashion trends particularly challenging and how getting it right can be a competitive advantage.
In this presentation, George L. Legendre, principal of IJP Architects and faculty at Harvard graduate School of Design, will show how the mathematical equations of pasta define the ultimate taxonomy of the genre.
The world is a rapidly changing place, where time flies and technological innovations batter us fast and furiously. Hadoop is just nine years old; and just five years ago had nowhere near the audience, ecosystem, or impact it has now . . .
Becoming an organization that can make agile decisions from agile data requires agile analytics
Shankar Vedantam, NPR Science Desk, NPR
In this session you'll see an application that builds on in-place existing technologies like Hadoop to deliver understandable results.  You'll hear a story where analytics at rest was applied to unstructured data using a simple SQL-like development environment, a&findings were promoted to the frontier of the business to score, in real time, monetizable intent, assess reputations & more. .
Julia Angwin discusses how much she has spent trying to protect her privacy, and raises the question of whether we want to live in a society where only the rich can buy their way out of ubiquitous surveillance.
Bob Mankoff, The New Yorker's cartoon editor, will analyze the lessons we learn from crowdsourced humor. Along the way, he'll explore how cartoons work (and sometimes don't); how he makes decisions about what cartoons to include; and what crowds can tell us about a good joke.
There is a symbiotic relationship between predictive modeling and Big Data. Performance gets better with more data and predictive models demonstrate like few other techniques the value of Big Data. However, there is a surprising paradox: when you need models most, even all the data is not enough or just not suitable. So in the days and age of Big Data there remains an art to predictive modeling.
In this debate, two teams of the world's best data scientists will debate the following proposition: "If you can't code, you can't be a data scientist."
An increasingly common task for data science is the measurement and attribution of experimental impact. Using examples from healthcare.gov, Microsoft advertising, and Bing experimentation, we will explore the strengths, weaknesses, and pitfalls of techniques for dealing with impact and attribution in scenarios/data in which control experiments were not possible or otherwise not performed.
A talk about how the largest professional social network in the world is digitally mapping the global economy to connect talent with opportunity at massive scale.
The IPython Notebook is an open-source, web-based interactive computing environment. The Notebook enables users to author documents that combine live code, descriptive text, mathematical equations, images, videos, and arbitrary HTML. This talk will describe how IPython is evolving to support a wide range of programming languages relevant in data science, including Python, Julia, and R.
Just in the US, we make over ~40 billion queries every month. From the time we wake up,  search engines are one of the top activities we do online, this talk will show some examples on how this data can be used from funny things like determining which city wakes up earlier to more complex scenarios like finding adverse drug interactions.
What does AI mean in 2014, and where is it headed? Every day brings news of purported breakthroughs, and some of the new applications are certainly impressive, but the field has witnessed boom/bust cycles before. What are the challenges that lie ahead this time? This talk will provide an overview of the state of the field, as well as a critical framework for thinking about the years ahead.
We debunk some popular approaches and attitudes we have encountered over the course of more than 50 real world Big Data implementations. We will describe each anti-pattern and its appeal--but also why they fail, and how to do it right.
Microsoft Translator currently supports 100+ languages. We constantly improve the translation quality, add new scenarios, all with a constant team size. This session describes a production scale ML architecture using MS Translator as a case study. You will learn the mental model to approach your ML problem and concrete Dos and Donts for the various components of the ML system architecture.
Karen Moon, Co-founder and CEO, Trendalytics
H2O presents the worlds fastest Distributed Parallel GBM.  GBM is a ML algorithm used to win many recent Kaggle competitions, and is well known for it's high quality results.
Strata Program Chairs, Roger Magoulas, Doug Cutting, and Alistair Croll, welcome you to the first day of keynotes.
Mike Olson, CSO and Chairman, Cloudera
If you want to know what's coming next in big data, just ask yourself, "what would Google do?
Miriah Meyer, Assistant Professor of Computer Science, University of Utah
This talk introduces how Intel is  working with scientists and physicians to help improve research, treatment, and drug development for Parkinsons Disease using data science and enabling the Parkinson's research community to build upon an open platform for big data analytics.
Data is an evolving story. Its not a static snapshot of a point in time insight. With data from internal and external sources constantly updating, we are evolving from rear-view mirror dashboard views into an era of interactive Storytelling.
Amanda Cox, Graphics Operator, The New York Times
Spark represents the next-step function leap in what is possible with Hadoop, but what does that mean for business analysts that are swimming in multi-structured data? This presentation discusses the new workflow required so that business analysts can work with massive volumes of multi-structured data to find new insights today, instead of continually having to wait for IT to make big data small.
There are two essential skills for the data scientist: engineering and statistics. A great many data scientists are very strong engineers but feel like impostors when it comes to statistics. In this talk John will argue that the ability to program a computer gives you special access to the deepest and most fundamental ideas in statistics.
Strata Program Chairs, Roger Magoulas, Doug Cutting, and Alistair Croll, welcome you to the second day of keynotes.
In this presentation Eli Collins, Clouderas Chief Technologist, will discuss how we might both reap the benefits of data while avoiding its perils.
This keynote will share insights from the worlds largest repository of consumer emotions and present the challenges and opportunities that this data presents for machine learning as well as data mining and visualization.
Software and the rise of cloud services have given rise to revolutionary new economies  creating new markets for everything from self-published books, music and videos to mobile apps.  Only a few years ago, it would have been hard to imagine developers authoring a million apps for smartphones.  But thats history.
Karen Moon will discuss the characteristics of unstructured data that makes identifying and synthesizing fashion trends particularly challenging and how getting it right can be a competitive advantage.
In this presentation, George L. Legendre, principal of IJP Architects and faculty at Harvard graduate School of Design, will show how the mathematical equations of pasta define the ultimate taxonomy of the genre.
The world is a rapidly changing place, where time flies and technological innovations batter us fast and furiously. Hadoop is just nine years old; and just five years ago had nowhere near the audience, ecosystem, or impact it has now . . .
Becoming an organization that can make agile decisions from agile data requires agile analytics
Shankar Vedantam, NPR Science Desk, NPR
In this session you'll see an application that builds on in-place existing technologies like Hadoop to deliver understandable results.  You'll hear a story where analytics at rest was applied to unstructured data using a simple SQL-like development environment, a&findings were promoted to the frontier of the business to score, in real time, monetizable intent, assess reputations & more. .
Julia Angwin discusses how much she has spent trying to protect her privacy, and raises the question of whether we want to live in a society where only the rich can buy their way out of ubiquitous surveillance.
Bob Mankoff, The New Yorker's cartoon editor, will analyze the lessons we learn from crowdsourced humor. Along the way, he'll explore how cartoons work (and sometimes don't); how he makes decisions about what cartoons to include; and what crowds can tell us about a good joke.
The command line, although invented decades ago, remains an amazing environment for doing data science. By combining small, yet powerful, command-line tools you can quickly obtain, scrub, explore, visualize, and model your data. In this hands-on tutorial you will gain a solid understanding of how to leverage the power of the command line and integrate it into your existing data science workflow.
This tutorial will help you get a jump start on HBase development. Well start with a quick overview of HBase, the HBase data model, and architecture, and then well dive directly into code to help you understand how to build HBase applications. We will also offer guidelines for good schema design, and will cover a few advanced concepts such as using HBase for transactions.
Interaction and visual design are exacting exercises. Designing for data -- especially in messy and massive forms -- brings a new set of challenges. How can we help people of varying backgrounds effectively transform and understand data at scale?
Bob Mankoff, The New Yorker's cartoon editor, will analyze the lessons we learn from crowdsourced humor. Along the way, he'll explore how cartoons work (and sometimes don't); how he makes decisions about what cartoons to include; and what crowds can tell us about a good joke.
Data scientists wear many hats -- how do you train a ready-for-prime-time data scientist in twelve weeks? We'll share some of the choices and models we used to create the Metis Data Science Bootcamp and select its first cohort of students.
Leave the over-structured, complex Data Warehouse behind. Dive into the pure, sparkling waters of the Data Lake! I suggest you enjoy the Instagram, but beware the hidden depths. The Data Lake is a misleading metaphor; it will become a watery grave for context, governance, and value. In reality, today's intricate information ecosystem demands a careful blend of architectures and technologies.
There is a wave of challengers in the database world focused on the scaling costs of traditional RDBMSs. These potential giant killers have capitalized on explosive data growth and disruptive technologies like distributed computing (e.g., Hadoop and NoSQL). Well discuss the new breed of database buyers, the redefinition of  enterprise, and apply lessons from past database wars.
Recent studies show the vast majority of Hadoop projects are stuck in development, with very few ever reaching production status. And those programs that do convert from pilot to production often view Hadoop as little more than an ETL tool. This session looks at why Hadoop implementations often stall out in the development phase and what companies can do to make Hadoop production ready.
Nathan Shetterley, Josh Patterson, and their team, set out to change the visual identity of the world's largest IT consulting firm in the world. From grass roots public visualization to a global visual literacy curriculum, see how they made Accenture more focused on data visualization. In addition, they will share insights into the business value of data visualization to their firm and clients.
An important skill of today's data scientists is data communication. Mapping and other types of data visualization have been sufficient to try and demonstrate the trends and patterns these professions find in data. However, there is an important shift happening in the way we consume data that means as a community, we need to think about our ability to turn data into stories.
IDEO's Hybrid team brings all the design tools from IDEO's product design process to work with clients on data oriented projects. The team will share elements of their process and case studies to show how incorporating human-centered techniques from design can improve data as an input to decision making.
Text adds clarity to visualizations and helps authors communicate. There are many text elements to consider when making a chart: axis titles, category and data labels, gridline labels, legends, citations, and annotations, to name a few. This talk will dive into the specifics of typography and text placement in information design.
Whether you're lining up for an Apple Watch, using the heads-up display of Google Glass, or sporting one of the hundreds of activity and sleep trackers, it's clear that wearable technology is exploding.  No longer bulky and cumbersome gadgets, today's wearables are fashionably... data-chic.
Shoving 1MM rows of query results into a chart or graph returns illegible results and kills interactivity. Smarter designs, however, will achieve data visibility. Furthermore, running on GPUs turns static designs into interactive tools. We will show how Graphistry does this in production with (a) new client/cloud GPU infrastructure and (b) GPU-accelerated languages like Superconductor.
Nanocubes is an open source project that can be used to visually explore large spatiotemporal datasets at interactive rates using a web browser.
The command line, although invented decades ago, remains an amazing environment for doing data science. By combining small, yet powerful, command-line tools you can quickly obtain, scrub, explore, visualize, and model your data. In this hands-on tutorial you will gain a solid understanding of how to leverage the power of the command line and integrate it into your existing data science workflow.
This tutorial will help you get a jump start on HBase development. Well start with a quick overview of HBase, the HBase data model, and architecture, and then well dive directly into code to help you understand how to build HBase applications. We will also offer guidelines for good schema design, and will cover a few advanced concepts such as using HBase for transactions.
Interaction and visual design are exacting exercises. Designing for data -- especially in messy and massive forms -- brings a new set of challenges. How can we help people of varying backgrounds effectively transform and understand data at scale?
Bob Mankoff, The New Yorker's cartoon editor, will analyze the lessons we learn from crowdsourced humor. Along the way, he'll explore how cartoons work (and sometimes don't); how he makes decisions about what cartoons to include; and what crowds can tell us about a good joke.
Data scientists wear many hats -- how do you train a ready-for-prime-time data scientist in twelve weeks? We'll share some of the choices and models we used to create the Metis Data Science Bootcamp and select its first cohort of students.
Leave the over-structured, complex Data Warehouse behind. Dive into the pure, sparkling waters of the Data Lake! I suggest you enjoy the Instagram, but beware the hidden depths. The Data Lake is a misleading metaphor; it will become a watery grave for context, governance, and value. In reality, today's intricate information ecosystem demands a careful blend of architectures and technologies.
There is a wave of challengers in the database world focused on the scaling costs of traditional RDBMSs. These potential giant killers have capitalized on explosive data growth and disruptive technologies like distributed computing (e.g., Hadoop and NoSQL). Well discuss the new breed of database buyers, the redefinition of  enterprise, and apply lessons from past database wars.
Recent studies show the vast majority of Hadoop projects are stuck in development, with very few ever reaching production status. And those programs that do convert from pilot to production often view Hadoop as little more than an ETL tool. This session looks at why Hadoop implementations often stall out in the development phase and what companies can do to make Hadoop production ready.
Nathan Shetterley, Josh Patterson, and their team, set out to change the visual identity of the world's largest IT consulting firm in the world. From grass roots public visualization to a global visual literacy curriculum, see how they made Accenture more focused on data visualization. In addition, they will share insights into the business value of data visualization to their firm and clients.
An important skill of today's data scientists is data communication. Mapping and other types of data visualization have been sufficient to try and demonstrate the trends and patterns these professions find in data. However, there is an important shift happening in the way we consume data that means as a community, we need to think about our ability to turn data into stories.
IDEO's Hybrid team brings all the design tools from IDEO's product design process to work with clients on data oriented projects. The team will share elements of their process and case studies to show how incorporating human-centered techniques from design can improve data as an input to decision making.
Text adds clarity to visualizations and helps authors communicate. There are many text elements to consider when making a chart: axis titles, category and data labels, gridline labels, legends, citations, and annotations, to name a few. This talk will dive into the specifics of typography and text placement in information design.
Whether you're lining up for an Apple Watch, using the heads-up display of Google Glass, or sporting one of the hundreds of activity and sleep trackers, it's clear that wearable technology is exploding.  No longer bulky and cumbersome gadgets, today's wearables are fashionably... data-chic.
Shoving 1MM rows of query results into a chart or graph returns illegible results and kills interactivity. Smarter designs, however, will achieve data visibility. Furthermore, running on GPUs turns static designs into interactive tools. We will show how Graphistry does this in production with (a) new client/cloud GPU infrastructure and (b) GPU-accelerated languages like Superconductor.
Nanocubes is an open source project that can be used to visually explore large spatiotemporal datasets at interactive rates using a web browser.
Technologists focused on privacy and civil liberties will run through the material in their book. The workshop will cover how to think about privacy, privacy protection properties that a system can have and the architectures that implement them, related issues in information security, and privacy issues in data collection.
This tutorial focuses on hands-on data science skills from prototyping to production. Using GraphLab tools, we walk through multiple case studies such as fraud detection, social network analysis, and building personalized recommendation services.
Goldman Sachs is a leading global investment banking, securities and investment management firm that provides a wide range of financial services. Goldman executes 100's of millions of financial transactions per day, across nearly every market in the world. Learn how Goldman is harnessing knowledge, data and compute power to maintain and increase its competitive edge.
Transamerica is a financial services company moving to a more customer centric model using Big Data. Our approach to this effort spans our Insurance, Annuity, and Retirement divisions.  We went from a simple proof of concept to establishing Hadoop as a viable element of our enterprise data strategy. We cover core components of our solution and focus on lessons learned from our experience.
American Express is transforming for the digital age! Learn how we unleashed Big Data into our ecosystem and built on the strength of our core capabilities to remain relevant in a rapidly changing environment. New commerce opportunities and innovative products are being delivered, and the chance to provide actionable insights, social analysis, and predictive modeling is growing exponentially.
The accumulation, access and analysis of customer data (the original Big Data) are ingrained for L.L.Bean, which has been doing customer modeling since the 1960s. In line with todays omnichannel imperative, however, the retailer has embraced a new Big Data-driven culturedemocratizing data access and toolsin order to sustain its customer-centric philosophy.
CERN, home to the Large Hadron Collider (LHC) is at the forefront of science and technology. Come to this session to learn how projects at CERN are leveraging In-memory data management and Hadoop to derive real-time insights from sensor data helping to manage the technical infrastructure of the Large Hadron Collider (LHC).
FICO has been delivering analytic solutions, such as their renowned credit scores, for nearly 60 years. Big data technologies like Hadoop promise FICO analysts the ability to build models much faster, and with greater accuracy than before, but this new generation of tools challenge them to think differently.
LinkedIn processes enormous amounts of events each day. In this talk, you will learn the background of the data challenges that LinkedIn faced, how the teams came together to construct the solution, and the underlying stack structure powering this solution including an interactive analytics infrastructure and a self-serve data visualization frontend solution at fast scale.
In this panel discussion, individuals representing key stakeholders across the healthcare ecosystem will share the ways they're applying Hadoop to solve big data challenges that will ultimately improve the quality of patient care while driving better healthcare affordability.
Medicine is undergoing a renaissance made possible by analyzing and creating insights from this huge and growing number of genomes. This session will showcase how ETL and MapReduce can be applied in a clinical session.
Automated image processing improves efficiency for a diverse range of applications from defect detection in manufacturing to tumor detection in medical images. Well go beyond traditional approaches to image processing, which fail for large image datasets, by leveraging Hadoop for processing a vast number of arbitrarily large images.
Netflix continues evolve its big data architecture in the cloud with performance enhancements and updated OSS offerings. We will share our experiences and selections in file formats, interactive query engines, and instance types. Genie emerges with updates to support YARN applications and we will unveil a new performance visualization tool, Inviso.
Kaiser Permanente is dedicated to improving the quality of healthcare, and big data presents numerous opportunities to drive this mission forward.
Technologists focused on privacy and civil liberties will run through the material in their book. The workshop will cover how to think about privacy, privacy protection properties that a system can have and the architectures that implement them, related issues in information security, and privacy issues in data collection.
This tutorial focuses on hands-on data science skills from prototyping to production. Using GraphLab tools, we walk through multiple case studies such as fraud detection, social network analysis, and building personalized recommendation services.
Goldman Sachs is a leading global investment banking, securities and investment management firm that provides a wide range of financial services. Goldman executes 100's of millions of financial transactions per day, across nearly every market in the world. Learn how Goldman is harnessing knowledge, data and compute power to maintain and increase its competitive edge.
Transamerica is a financial services company moving to a more customer centric model using Big Data. Our approach to this effort spans our Insurance, Annuity, and Retirement divisions.  We went from a simple proof of concept to establishing Hadoop as a viable element of our enterprise data strategy. We cover core components of our solution and focus on lessons learned from our experience.
American Express is transforming for the digital age! Learn how we unleashed Big Data into our ecosystem and built on the strength of our core capabilities to remain relevant in a rapidly changing environment. New commerce opportunities and innovative products are being delivered, and the chance to provide actionable insights, social analysis, and predictive modeling is growing exponentially.
The accumulation, access and analysis of customer data (the original Big Data) are ingrained for L.L.Bean, which has been doing customer modeling since the 1960s. In line with todays omnichannel imperative, however, the retailer has embraced a new Big Data-driven culturedemocratizing data access and toolsin order to sustain its customer-centric philosophy.
CERN, home to the Large Hadron Collider (LHC) is at the forefront of science and technology. Come to this session to learn how projects at CERN are leveraging In-memory data management and Hadoop to derive real-time insights from sensor data helping to manage the technical infrastructure of the Large Hadron Collider (LHC).
FICO has been delivering analytic solutions, such as their renowned credit scores, for nearly 60 years. Big data technologies like Hadoop promise FICO analysts the ability to build models much faster, and with greater accuracy than before, but this new generation of tools challenge them to think differently.
LinkedIn processes enormous amounts of events each day. In this talk, you will learn the background of the data challenges that LinkedIn faced, how the teams came together to construct the solution, and the underlying stack structure powering this solution including an interactive analytics infrastructure and a self-serve data visualization frontend solution at fast scale.
In this panel discussion, individuals representing key stakeholders across the healthcare ecosystem will share the ways they're applying Hadoop to solve big data challenges that will ultimately improve the quality of patient care while driving better healthcare affordability.
Medicine is undergoing a renaissance made possible by analyzing and creating insights from this huge and growing number of genomes. This session will showcase how ETL and MapReduce can be applied in a clinical session.
Automated image processing improves efficiency for a diverse range of applications from defect detection in manufacturing to tumor detection in medical images. Well go beyond traditional approaches to image processing, which fail for large image datasets, by leveraging Hadoop for processing a vast number of arbitrarily large images.
Netflix continues evolve its big data architecture in the cloud with performance enhancements and updated OSS offerings. We will share our experiences and selections in file formats, interactive query engines, and instance types. Genie emerges with updates to support YARN applications and we will unveil a new performance visualization tool, Inviso.
Kaiser Permanente is dedicated to improving the quality of healthcare, and big data presents numerous opportunities to drive this mission forward.
Spark Camp, organized by the creators of the Apache Spark project at Databricks, will be a day long hands-on introduction to the Spark platform including Spark Core, the Spark Shell, Spark Streaming, Spark SQL, MLlib, and more.
Find out how to run real-time analytics over raw data without requiring a manual ETL process targeted at an RDBMS. This talk describes Impalas approach to on-the-fly data transformation and its support for nested data; examples demonstrate how this can be used to query raw data feeds in formats such as text, JSON and XML, at a performance level commonly associated with specialized engines.
Hyde shows how to quickly build a SQL interface to a NoSQL system using Optiq. He shows how to add rules and operators to Optiq to push down processing to the source system, and how to automatically build materialized data sets in memory for blazing-fast interactive analysis.
When people think of big data processing, they think of Apache Hadoop, but that doesn't mean traditional databases don't play a role. In most cases users will still draw from data stored in RDBMS systems. Apache Sqoop can be used to unlock that data and transfer it to Hadoop, enabling users with information stored in existing SQL tables to use new analytic tools.
The past year has seen the advent of various "low latency" solutions for querying big data such as Shark, Impala, and Presto. The Hive team at Yahoo has spent the past several months benchmarking several versions of Hive (and Tez), with several permutations of file-formats, compression, and query engine features, at various data sizes. In this talk, we present our tests, the results, and findings.
We will discuss the basics of scaling, common mistakes and misconceptions, how different technology decisions affect performance, and how to identify and scale around the bottlenecks in a Storm deployment.
Apache Samza is a framework for processing high-volume real-time event streams. In this session we will walk through our experiences of putting Samza into production at LinkedIn, discuss how it compares to other stream processing tools, and share the lessons we learnt about dealing with real-time data at scale.
This talk examines sources of latency in HBase, detailing steps along the read and write paths. We'll examine the entire request lifecycle, from client to server and back again. We'll also look at the different factors that impact latency, including GC, cache misses, and system failures. Finally, the talk will highlight some of the work done in 0.96+ to improve the reliability of HBase.
Today, there are hundreds of production Apache HBase clusters running either entity-centric or event-based applications. Gathered from known clusters and a survey conducted by Cloudera's development, product, and services teams from their experiences with the nearly 20,000 HBase nodes under management, this talk categorizes these the gamut of use-case into a compact set of application archetypes.
Are you taking advantage of all of Hadoops features to operate a stable and effective cluster?  Inspired by real-world support cases, this talk discusses best practices and new features to help improve incident response and daily operations.  Chances are that youll walk away from this talk with some new ideas to implement in your own clusters.
This talk will cover resource management using YARN - the new resource management platform introduced in Hadoop 2.0. It will cover how it achieves effective cluster utilization, fair sharing of resources, and allow different type of applications to utilize the cluster. We will go over the architecture, recent improvements, and things coming down the pipeline.
This presentation will show you how to get your Big Data into Apache HBase as fast as possible. Those 40 minutes will save you hours of debugging and tuning, with the added bonus of having a better understanding of how HBase works. You will learn things like the write path, bulk loading, HFiles, and more.
Impala provides the ability to easily analyze large, distributed data sets. This talk will cover the impyla package, which aims to make data science easier with Impala by integrating with Python. The impyla package currently supports programmatically interacting with Impala, running distributed machine learning in Impala, and compiling Python UDFs into assembly instructions via LLVM.
Spark Camp, organized by the creators of the Apache Spark project at Databricks, will be a day long hands-on introduction to the Spark platform including Spark Core, the Spark Shell, Spark Streaming, Spark SQL, MLlib, and more.
Find out how to run real-time analytics over raw data without requiring a manual ETL process targeted at an RDBMS. This talk describes Impalas approach to on-the-fly data transformation and its support for nested data; examples demonstrate how this can be used to query raw data feeds in formats such as text, JSON and XML, at a performance level commonly associated with specialized engines.
Hyde shows how to quickly build a SQL interface to a NoSQL system using Optiq. He shows how to add rules and operators to Optiq to push down processing to the source system, and how to automatically build materialized data sets in memory for blazing-fast interactive analysis.
When people think of big data processing, they think of Apache Hadoop, but that doesn't mean traditional databases don't play a role. In most cases users will still draw from data stored in RDBMS systems. Apache Sqoop can be used to unlock that data and transfer it to Hadoop, enabling users with information stored in existing SQL tables to use new analytic tools.
The past year has seen the advent of various "low latency" solutions for querying big data such as Shark, Impala, and Presto. The Hive team at Yahoo has spent the past several months benchmarking several versions of Hive (and Tez), with several permutations of file-formats, compression, and query engine features, at various data sizes. In this talk, we present our tests, the results, and findings.
We will discuss the basics of scaling, common mistakes and misconceptions, how different technology decisions affect performance, and how to identify and scale around the bottlenecks in a Storm deployment.
Apache Samza is a framework for processing high-volume real-time event streams. In this session we will walk through our experiences of putting Samza into production at LinkedIn, discuss how it compares to other stream processing tools, and share the lessons we learnt about dealing with real-time data at scale.
This talk examines sources of latency in HBase, detailing steps along the read and write paths. We'll examine the entire request lifecycle, from client to server and back again. We'll also look at the different factors that impact latency, including GC, cache misses, and system failures. Finally, the talk will highlight some of the work done in 0.96+ to improve the reliability of HBase.
Today, there are hundreds of production Apache HBase clusters running either entity-centric or event-based applications. Gathered from known clusters and a survey conducted by Cloudera's development, product, and services teams from their experiences with the nearly 20,000 HBase nodes under management, this talk categorizes these the gamut of use-case into a compact set of application archetypes.
Are you taking advantage of all of Hadoops features to operate a stable and effective cluster?  Inspired by real-world support cases, this talk discusses best practices and new features to help improve incident response and daily operations.  Chances are that youll walk away from this talk with some new ideas to implement in your own clusters.
This talk will cover resource management using YARN - the new resource management platform introduced in Hadoop 2.0. It will cover how it achieves effective cluster utilization, fair sharing of resources, and allow different type of applications to utilize the cluster. We will go over the architecture, recent improvements, and things coming down the pipeline.
This presentation will show you how to get your Big Data into Apache HBase as fast as possible. Those 40 minutes will save you hours of debugging and tuning, with the added bonus of having a better understanding of how HBase works. You will learn things like the write path, bulk loading, HFiles, and more.
Impala provides the ability to easily analyze large, distributed data sets. This talk will cover the impyla package, which aims to make data science easier with Impala by integrating with Python. The impyla package currently supports programmatically interacting with Impala, running distributed machine learning in Impala, and compiling Python UDFs into assembly instructions via LLVM.
All-Day: For business strategists, marketers, product managers, and entrepreneurs, Data-Driven Business looks at how to use data to make better business decisions faster. Packed with case studies, panels, and eye-opening presentations, this fast-paced day focuses on how to solve today's thorniest business problems with Big Data. It's the missing MBA for a data-driven, always-on business world.
The explosion of internal data sources, external public data sources and feeds from the Internet of Things is causing a tsunami of diverse data sources for enterprises. Top-down data-integration tools and data scientist tools wont scale to meet the demands of the modern enterprise. Learn how a scalable data curation platform can help enterprises connect and enrich their data to leverage it all.
Data transformation  traditionally the domain of IT specialists  is emerging as a critical, widespread problem in data analytics. In this session we discuss the advantages of using a domain-specific language for data transformation tasks. We illustrate these issues with Wrangle, a DSL designed for interactive data transformation.
Organizations often showcase the virtues of their data platforms, but rarely share the challenges and decisions faced along the way. Our session describes how we architected our analytics stack around Druid, an open source distributed data store, and how we overcame the challenges around scaling the system, balancing features with cost, and making performance consistent.
Leveraging our experience from working on some of the largest-scale high-growth applications at Facebook and other companies, including building the most popular data analysis tool Scuba, this talk outlines 10 lessons learned, along with best practices towards extracting the most value out of data, while avoiding common pitfalls.
An introduction to Tachyon, a memory centric storage system started from UC Berkeley. It enables different frameworks to share data at memory-speed. It is also a major component of Berkeley Data Analytics Stack (BDAS). The project is open source and is deployed at multiple companies. It has more than 30 contributors from over 10 institutions, including Yahoo, Intel, Redhat, Alibaba etc.
Apache Spark is a popular new paradigm for computation on Hadoop. It's particularly effective for iterative algorithms relevant to data science like clustering, which can be used to detect anomalies in data. Curious? Get a taste of Spark MLlib, Scala and k-means clustering in this walkthrough of anomaly detection as applied to network intrusion, using the KDD Cup '99 data set.
What is the lambda architecture, and how do you put it to use for your streaming data?  Flip Kromer and Q Ethan McCallum will explain how this works, using a live-updating recommendation engine as the supporting example.
In this talk Michael will describe Spark SQL, the newest component of the Apache Spark stack.  A key feature of Spark SQL is the ability to blur the lines between relational tables and RDDs, making it easy for developers to intermix SQL commands that query structured data with complex analytics in imperative or functional languages.
We will demonstrate how to combine visual tools with Spark to apply three specific techniques to visually explore big data using a) summarize and visualize, b) sample and visualize, and c) model and visualize. We will use a real big dataset, such as Wikipedia traffic logs, to demonstrate these techniques in a live demo.
Open Source Real Time BI using Storm, Hadoop, Titan, Druid & D3
In the last two years we've seen the introduction of several open-source SQL engines for Hadoop.  There have been numerous marketing claims around SQL-on-Hadoop performance but what should you believe?  How do these different engines compare on functionality?  This talk will compare and contrast Hive, Impala, and Presto all from an non-vendor, unsponsored, independent point of view.
The widespread adoption of web-based maps provides a familiar set of interactions for exploring large data spaces. Building on these techniques, Tile-based visual analytics provides interactive visualization of billions of points of data or more. This session provides an overview of technical challenges and promise using applications created with the open source Aperture Tiles framework on GitHub.
All-Day: For business strategists, marketers, product managers, and entrepreneurs, Data-Driven Business looks at how to use data to make better business decisions faster. Packed with case studies, panels, and eye-opening presentations, this fast-paced day focuses on how to solve today's thorniest business problems with Big Data. It's the missing MBA for a data-driven, always-on business world.
The explosion of internal data sources, external public data sources and feeds from the Internet of Things is causing a tsunami of diverse data sources for enterprises. Top-down data-integration tools and data scientist tools wont scale to meet the demands of the modern enterprise. Learn how a scalable data curation platform can help enterprises connect and enrich their data to leverage it all.
Data transformation  traditionally the domain of IT specialists  is emerging as a critical, widespread problem in data analytics. In this session we discuss the advantages of using a domain-specific language for data transformation tasks. We illustrate these issues with Wrangle, a DSL designed for interactive data transformation.
Organizations often showcase the virtues of their data platforms, but rarely share the challenges and decisions faced along the way. Our session describes how we architected our analytics stack around Druid, an open source distributed data store, and how we overcame the challenges around scaling the system, balancing features with cost, and making performance consistent.
Leveraging our experience from working on some of the largest-scale high-growth applications at Facebook and other companies, including building the most popular data analysis tool Scuba, this talk outlines 10 lessons learned, along with best practices towards extracting the most value out of data, while avoiding common pitfalls.
An introduction to Tachyon, a memory centric storage system started from UC Berkeley. It enables different frameworks to share data at memory-speed. It is also a major component of Berkeley Data Analytics Stack (BDAS). The project is open source and is deployed at multiple companies. It has more than 30 contributors from over 10 institutions, including Yahoo, Intel, Redhat, Alibaba etc.
Apache Spark is a popular new paradigm for computation on Hadoop. It's particularly effective for iterative algorithms relevant to data science like clustering, which can be used to detect anomalies in data. Curious? Get a taste of Spark MLlib, Scala and k-means clustering in this walkthrough of anomaly detection as applied to network intrusion, using the KDD Cup '99 data set.
What is the lambda architecture, and how do you put it to use for your streaming data?  Flip Kromer and Q Ethan McCallum will explain how this works, using a live-updating recommendation engine as the supporting example.
In this talk Michael will describe Spark SQL, the newest component of the Apache Spark stack.  A key feature of Spark SQL is the ability to blur the lines between relational tables and RDDs, making it easy for developers to intermix SQL commands that query structured data with complex analytics in imperative or functional languages.
We will demonstrate how to combine visual tools with Spark to apply three specific techniques to visually explore big data using a) summarize and visualize, b) sample and visualize, and c) model and visualize. We will use a real big dataset, such as Wikipedia traffic logs, to demonstrate these techniques in a live demo.
Open Source Real Time BI using Storm, Hadoop, Titan, Druid & D3
In the last two years we've seen the introduction of several open-source SQL engines for Hadoop.  There have been numerous marketing claims around SQL-on-Hadoop performance but what should you believe?  How do these different engines compare on functionality?  This talk will compare and contrast Hive, Impala, and Presto all from an non-vendor, unsponsored, independent point of view.
The widespread adoption of web-based maps provides a familiar set of interactions for exploring large data spaces. Building on these techniques, Tile-based visual analytics provides interactive visualization of billions of points of data or more. This session provides an overview of technical challenges and promise using applications created with the open source Aperture Tiles framework on GitHub.
Are you looking for a deeper understanding of how to integrate components in the Apache Hadoop ecosystem to implement data management and processing solutions? Then this tutorial is for you. We'll provide a clickstream analytics example illustrating how to architect solutions with Apache Hadoop along with providing best practices and recommendations for using Hadoop and related tools.
What are the essential components of a data platform? This tutorial will explain how the various parts of the Hadoop and big data ecosystem fit together in production to create a data platform supporting batch, interactive and realtime analytical workloads.
An open data in government love story / case study - how a team of techies overcame political and procedural hurdles to change the financial marketplace.
Bad press, FTC consent decrees, and White House reports have all put a spotlight on bad data practices. Data scientists and designers have become increasingly aware of how privacy principles should guide their work. So, the geeks have met the wonks. Now, its time for the wonks to meet the geeks and use data analytics to keep pace with burgeoning data volumes, velocities, and innovations.
While the inexorable march of technology does threaten historical notions of privacy, privacy IS very much alive  a shifting, vital conversation society has with itself and its machines. This talk explores the principles of transparency, unlinkability, and intervenability to build a foundation for a design ethos for technologists.
The story of using predictive analytics for homelessness prevention in New York City. SumAll.org is currently piloting this approach with the citys department of homeless services. Predicting at-risk families in a timely manner and micro-targeting social services is a game-changer. SumAll.org is a data analytics nonprofit, dedicated to leveraging the power of data for social innovation.
This session examines the risks of over-reliance on big data and the need to bring in Thick Dataqualitative methods used by ethnographers.
Open government data on healthcare, finance, education, energy, and other areas has become a major business resource. Joel Gurin, author of Open Data Now and director of the Open Data 500 study, will show how both startups and established companies are putting open data to work. He'll cover Open Data and Big Data, business models for open-data companies, and lessons from a range of case studies.
Last year, Douglas Merrill, CEO of ZestFinance and former Google CIO, discussed how success in big data analysis requires not just machines and algorithms, but also human analysis, or data artists". Building on this notion, Mike Armstrong, CMO of ZestFinance, will discuss how companies can find, identify, and correct data inaccuracies.
This session will cover the value that linking algorithms bring to identity risk management, and how to apply linking algorithms, data and super compute capability to the challenge of identity risk management and identity fraud. We will also look at patterns of identity fraud, namely those (stolen) identities that have come back from the dead and how to differ those from real, live identities.
People living in the Information Age are faced with a conundrum. They wish to be connected on a series of global, interconnected networks but they also wish to protect their privacy and to be left alonesometimes.
As in a game of chess, successful use of machine learning techniques against adaptive adversaries, such as spammers and intruders, requires designing the learning algorithms having anticipated the opponents response to those algorithms. In this talk, we present techniques to design robust machine learning algorithms for adversarial environments and provide clarifying attack-defense examples.
The Internet is a warzone. Any business with a digital presence needs to protect itself from threats that exist in cyberspace. In this presentation, well show you how to build a real-time anomaly detection system using Sqrrl Enterprise and Apache Spark GraphX to monitor and surface advanced persistent threats and malicious actor attacks.
In this session Guavus Chief Technology Officer, Roy Singh, will present a framework using an operational intelligence platform based on Apache Spark, for providing a pipeline for anomaly detection, causality analysis, anomaly prediction, and actionable alerts.
Are you looking for a deeper understanding of how to integrate components in the Apache Hadoop ecosystem to implement data management and processing solutions? Then this tutorial is for you. We'll provide a clickstream analytics example illustrating how to architect solutions with Apache Hadoop along with providing best practices and recommendations for using Hadoop and related tools.
What are the essential components of a data platform? This tutorial will explain how the various parts of the Hadoop and big data ecosystem fit together in production to create a data platform supporting batch, interactive and realtime analytical workloads.
An open data in government love story / case study - how a team of techies overcame political and procedural hurdles to change the financial marketplace.
Bad press, FTC consent decrees, and White House reports have all put a spotlight on bad data practices. Data scientists and designers have become increasingly aware of how privacy principles should guide their work. So, the geeks have met the wonks. Now, its time for the wonks to meet the geeks and use data analytics to keep pace with burgeoning data volumes, velocities, and innovations.
While the inexorable march of technology does threaten historical notions of privacy, privacy IS very much alive  a shifting, vital conversation society has with itself and its machines. This talk explores the principles of transparency, unlinkability, and intervenability to build a foundation for a design ethos for technologists.
The story of using predictive analytics for homelessness prevention in New York City. SumAll.org is currently piloting this approach with the citys department of homeless services. Predicting at-risk families in a timely manner and micro-targeting social services is a game-changer. SumAll.org is a data analytics nonprofit, dedicated to leveraging the power of data for social innovation.
This session examines the risks of over-reliance on big data and the need to bring in Thick Dataqualitative methods used by ethnographers.
Open government data on healthcare, finance, education, energy, and other areas has become a major business resource. Joel Gurin, author of Open Data Now and director of the Open Data 500 study, will show how both startups and established companies are putting open data to work. He'll cover Open Data and Big Data, business models for open-data companies, and lessons from a range of case studies.
Last year, Douglas Merrill, CEO of ZestFinance and former Google CIO, discussed how success in big data analysis requires not just machines and algorithms, but also human analysis, or data artists". Building on this notion, Mike Armstrong, CMO of ZestFinance, will discuss how companies can find, identify, and correct data inaccuracies.
This session will cover the value that linking algorithms bring to identity risk management, and how to apply linking algorithms, data and super compute capability to the challenge of identity risk management and identity fraud. We will also look at patterns of identity fraud, namely those (stolen) identities that have come back from the dead and how to differ those from real, live identities.
People living in the Information Age are faced with a conundrum. They wish to be connected on a series of global, interconnected networks but they also wish to protect their privacy and to be left alonesometimes.
As in a game of chess, successful use of machine learning techniques against adaptive adversaries, such as spammers and intruders, requires designing the learning algorithms having anticipated the opponents response to those algorithms. In this talk, we present techniques to design robust machine learning algorithms for adversarial environments and provide clarifying attack-defense examples.
The Internet is a warzone. Any business with a digital presence needs to protect itself from threats that exist in cyberspace. In this presentation, well show you how to build a real-time anomaly detection system using Sqrrl Enterprise and Apache Spark GraphX to monitor and surface advanced persistent threats and malicious actor attacks.
In this session Guavus Chief Technology Officer, Roy Singh, will present a framework using an operational intelligence platform based on Apache Spark, for providing a pipeline for anomaly detection, causality analysis, anomaly prediction, and actionable alerts.
Python has become an increasingly important part of the data engineer and analytic tool landscape. Pydata at Strata provides in-depth coverage of the tools and techniques gaining traction with the data audience, including iPython Notebook, NumPy/matplotlib for visualization, SciPy, scikit-learn, and how to scale Python performance, including how to handle large, distributed data sets.
Uber has created an AI city simulation framework to optimize its dispatching system, minimize user wait times, and maximize driver partner earnings. Based on agent-based and swarm intelligence models, this framework generates plausible optimizations across many interacting, dynamic, non-linear parameters on a city-by-city basis.
How far can we take open data--and where can it take us? Brett Goldstein, who helped pioneer Chicagos cutting-edge efforts in open data and analytics as CIO and CDO, will speak on how these act as a force multiplier on government efforts and can lead to smarter and more inclusive policy-making, while enhancing the governments ability to anticipate and react to the needs of the public.
GeoSpatial BigData and types are special "animals" when it comes to storage, discovery and processing. This session will explore the various non-traditional ways to stream, extract, batch and visualize GeoSpatial Information for deeper geo-insight, such as "Where are the 3 nearest facilities to each of my customers based on current traffic conditions...nationwide ?"
Aadhaar, India's Unique Identity Project, is the largest biometric identity system in the world with more than 600 million people. Its strength lies in its design simplicity, sound strategy, and technology backbone issuing 1 million identity numbers and doing 600 trillion biometric matches every day! Pramod Varma, who is the Chief Architect of Aadhaar, shares his experience from this project.
This session will help data scientists support healthcare leaders to harmonize health data with Open Source community data commons approaches. This enhances the value of mandated EMR adoption beyond Meaningful Use requirements by creating evidence-based community health intelligence at the pace and point of change, the everyday lives and activities of community members.
At Etsy, we run dozens of experiments simultaneously and we have terabytes of data generated by the tens of millions of members of our community. We've worked hard to establish a product development process informed by -- and often driven by -- data.  In this talk, Nell will discuss the tensions that arise in a data-driven product culture.
Industrial systems produce large volumes of real-time data that can be analyzed using Big Data technologies in the data center environments. In many cases, such data needs to be analyzed at the edge before leaving industrial machines or systems that control them. This is possible if machines have intelligence to process data and make decisions. GE will share such use cases and experience.
The trend towards cloud architectures we've seen over the last few years isn't sustainable. With tens of billions more Internet connected devices arriving over the next few yearsfar faster than any predicted increase in bandwidth to outside worlddata is increasingly going to become a local problem, rather than a cloud problem.
A lot of stationary, big data begins its life as small data in rapid motion - think logs, sensors, social data. The pressure is on architects, infra devops, and app developers to harness real-time data, and expose it to the right data processing paradigm. Learn how on AWS, services like Amazon Kinesis, Redshift, and Elastic MapReduce can be composed to deliver a smarter big data infrastructure.
After babysitting Hadoop clusters for many years and knowing the limitations really well we had the chance to design and implement the cloud infrastructure for a large connected home platform from scratch. Well show how weve built that backend with Crate Data and Twitter Storm and why this is a perfect match for this workload.
Born as a solution built for RMS (the Australian Government agency managing and regulating the use of roads in New South Wales), this Internet of Things application for smarter transportation services provides a real-time data hub for transportation sensor networks, network information and traveler information, offering actionable insight into network performance, congestion, and incidents.
Rio Tinto is one of the worlds leading mining companies. Our current technology focus is around using innovation to realize our vision of the Mine of The Future. Join us as we explore how natural resource companies are using Big Data techniques to visualize resource deposits, enable fully autonomous rail road systems and global system monitoring.
Python has become an increasingly important part of the data engineer and analytic tool landscape. Pydata at Strata provides in-depth coverage of the tools and techniques gaining traction with the data audience, including iPython Notebook, NumPy/matplotlib for visualization, SciPy, scikit-learn, and how to scale Python performance, including how to handle large, distributed data sets.
Uber has created an AI city simulation framework to optimize its dispatching system, minimize user wait times, and maximize driver partner earnings. Based on agent-based and swarm intelligence models, this framework generates plausible optimizations across many interacting, dynamic, non-linear parameters on a city-by-city basis.
How far can we take open data--and where can it take us? Brett Goldstein, who helped pioneer Chicagos cutting-edge efforts in open data and analytics as CIO and CDO, will speak on how these act as a force multiplier on government efforts and can lead to smarter and more inclusive policy-making, while enhancing the governments ability to anticipate and react to the needs of the public.
GeoSpatial BigData and types are special "animals" when it comes to storage, discovery and processing. This session will explore the various non-traditional ways to stream, extract, batch and visualize GeoSpatial Information for deeper geo-insight, such as "Where are the 3 nearest facilities to each of my customers based on current traffic conditions...nationwide ?"
Aadhaar, India's Unique Identity Project, is the largest biometric identity system in the world with more than 600 million people. Its strength lies in its design simplicity, sound strategy, and technology backbone issuing 1 million identity numbers and doing 600 trillion biometric matches every day! Pramod Varma, who is the Chief Architect of Aadhaar, shares his experience from this project.
This session will help data scientists support healthcare leaders to harmonize health data with Open Source community data commons approaches. This enhances the value of mandated EMR adoption beyond Meaningful Use requirements by creating evidence-based community health intelligence at the pace and point of change, the everyday lives and activities of community members.
At Etsy, we run dozens of experiments simultaneously and we have terabytes of data generated by the tens of millions of members of our community. We've worked hard to establish a product development process informed by -- and often driven by -- data.  In this talk, Nell will discuss the tensions that arise in a data-driven product culture.
Industrial systems produce large volumes of real-time data that can be analyzed using Big Data technologies in the data center environments. In many cases, such data needs to be analyzed at the edge before leaving industrial machines or systems that control them. This is possible if machines have intelligence to process data and make decisions. GE will share such use cases and experience.
The trend towards cloud architectures we've seen over the last few years isn't sustainable. With tens of billions more Internet connected devices arriving over the next few yearsfar faster than any predicted increase in bandwidth to outside worlddata is increasingly going to become a local problem, rather than a cloud problem.
A lot of stationary, big data begins its life as small data in rapid motion - think logs, sensors, social data. The pressure is on architects, infra devops, and app developers to harness real-time data, and expose it to the right data processing paradigm. Learn how on AWS, services like Amazon Kinesis, Redshift, and Elastic MapReduce can be composed to deliver a smarter big data infrastructure.
After babysitting Hadoop clusters for many years and knowing the limitations really well we had the chance to design and implement the cloud infrastructure for a large connected home platform from scratch. Well show how weve built that backend with Crate Data and Twitter Storm and why this is a perfect match for this workload.
Born as a solution built for RMS (the Australian Government agency managing and regulating the use of roads in New South Wales), this Internet of Things application for smarter transportation services provides a real-time data hub for transportation sensor networks, network information and traveler information, offering actionable insight into network performance, congestion, and incidents.
Rio Tinto is one of the worlds leading mining companies. Our current technology focus is around using innovation to realize our vision of the Mine of The Future. Join us as we explore how natural resource companies are using Big Data techniques to visualize resource deposits, enable fully autonomous rail road systems and global system monitoring.
All-Day: Strata's regular data science track has great talks with real world experience from leading edge speakers. But we didn't just stop therewe added the Hardcore Data Science day to give you a chance to go even deeper. The Hardcore day will add new techniques and technologies to your data science toolbox, shared by leading data science practitioners from startups, industry, consulting...
Business problems dont reveal themselves neatly as data problems. The data community is obsessed with tools and techniques, but the real challenge is understanding how to solve problems with data. How do we bridge the gap? In this talk, we will teach you a methodology for figuring out the right problems to solve and making sure that the work stays smart.
This talk highlights William's success, challenges, and experiences creating a data driven operations model into Ciscos engineering services organization. William highlights the role of data, the need for scale and security, the opportunity for new technology to accelerate business, the role of IT to help guide/partner, and the mind shift and cultural changes along the journey.
Over two years of running A/B testing at Pinterest on millions of users each day, Andrea learned about the nuances that can make or break an experimentation platform. Andrea will discuss how her approach to testing has adjusted over time to avoid critical errors at all levels, from organizational to analytical.
This discussion touches on the human response to analysis results, especially when they do not support long held beliefs and how this effects organizational change. This discussion also focuses on Predictive Analytics best practices, team skills, and a review of what it takes to build a sustainable Predictive Analytics program.
In this session, Kleiner Perkins Caufield & Byers General Partner Michael Abbott speaks with Geoff Guerdat of the Gilt Groupe, Will Moss of Airbnb, and Emil Ong of Lookout, to unbox their respective companies and examine the technology, architecture, and innovations theyve harnessed to deliver superior products and services.
In this session, Kleiner Perkins Caufield & Byers General Partner Michael Abbott speaks with Michael Stoppelman of Yelp and Siva Subramanian of Box to unbox their respective companies and examine the technology, architecture, and innovations theyve harnessed to deliver superior products and services.
To anticipate who will succeed and invest wisely, investors spend a lot of time trying to understand the longer-term trends within an industry. In this panel discussion, well consider the big trends in Big Data, asking top-tier VCs to look over the horizon discuss the visions they have two or more years in the future.
For a long time Internet retailers have been trying to move items they sell closer to customers. Flash sale site Gilt.com takes it to the extreme: we apply machine learning to predict customers' cravings for fashion products in different geographic regions without purchase history to draw from.
The Big Data market is busy, with sky-high valuations and a rapid pace of innovation. This panel of data-focused Venture Capitalists will look at how they think about investing in and around the Big Data spacefrom the kind of deals theyre after, to how they like to work with entrepreneurs and founders.
PDFs are the bane of data science, a jail from which machine-readable data struggles to escape. We'll explain how at Edmunds.com we freed data from diverse auto manufacturer PDFs, applied NLP and entity recognition, and integrated the results into the expert-driven process of defining vehicle models.
Panel discussion, moderated by Liza Kindred, Founder, Third Wave Fashion. Panelists include David Whittemore, founder of Clotheshorse; Gina Mancusco, founder of Love That Fit; and Rasmus Thofte, head of North America at Virtusize.
A look at how we use public governmental data to answer questions about our customers and their behavior; data used by marketing, space management, and product managers. Other government data is used to support the company's sales forecast, which items to purchase, predict amount to be purchased, and determining which items to phase out. All Data driven management - made even easier with Hadoop.
All-Day: Strata's regular data science track has great talks with real world experience from leading edge speakers. But we didn't just stop therewe added the Hardcore Data Science day to give you a chance to go even deeper. The Hardcore day will add new techniques and technologies to your data science toolbox, shared by leading data science practitioners from startups, industry, consulting...
Business problems dont reveal themselves neatly as data problems. The data community is obsessed with tools and techniques, but the real challenge is understanding how to solve problems with data. How do we bridge the gap? In this talk, we will teach you a methodology for figuring out the right problems to solve and making sure that the work stays smart.
This talk highlights William's success, challenges, and experiences creating a data driven operations model into Ciscos engineering services organization. William highlights the role of data, the need for scale and security, the opportunity for new technology to accelerate business, the role of IT to help guide/partner, and the mind shift and cultural changes along the journey.
Over two years of running A/B testing at Pinterest on millions of users each day, Andrea learned about the nuances that can make or break an experimentation platform. Andrea will discuss how her approach to testing has adjusted over time to avoid critical errors at all levels, from organizational to analytical.
This discussion touches on the human response to analysis results, especially when they do not support long held beliefs and how this effects organizational change. This discussion also focuses on Predictive Analytics best practices, team skills, and a review of what it takes to build a sustainable Predictive Analytics program.
In this session, Kleiner Perkins Caufield & Byers General Partner Michael Abbott speaks with Geoff Guerdat of the Gilt Groupe, Will Moss of Airbnb, and Emil Ong of Lookout, to unbox their respective companies and examine the technology, architecture, and innovations theyve harnessed to deliver superior products and services.
In this session, Kleiner Perkins Caufield & Byers General Partner Michael Abbott speaks with Michael Stoppelman of Yelp and Siva Subramanian of Box to unbox their respective companies and examine the technology, architecture, and innovations theyve harnessed to deliver superior products and services.
To anticipate who will succeed and invest wisely, investors spend a lot of time trying to understand the longer-term trends within an industry. In this panel discussion, well consider the big trends in Big Data, asking top-tier VCs to look over the horizon discuss the visions they have two or more years in the future.
For a long time Internet retailers have been trying to move items they sell closer to customers. Flash sale site Gilt.com takes it to the extreme: we apply machine learning to predict customers' cravings for fashion products in different geographic regions without purchase history to draw from.
The Big Data market is busy, with sky-high valuations and a rapid pace of innovation. This panel of data-focused Venture Capitalists will look at how they think about investing in and around the Big Data spacefrom the kind of deals theyre after, to how they like to work with entrepreneurs and founders.
PDFs are the bane of data science, a jail from which machine-readable data struggles to escape. We'll explain how at Edmunds.com we freed data from diverse auto manufacturer PDFs, applied NLP and entity recognition, and integrated the results into the expert-driven process of defining vehicle models.
Panel discussion, moderated by Liza Kindred, Founder, Third Wave Fashion. Panelists include David Whittemore, founder of Clotheshorse; Gina Mancusco, founder of Love That Fit; and Rasmus Thofte, head of North America at Virtusize.
A look at how we use public governmental data to answer questions about our customers and their behavior; data used by marketing, space management, and product managers. Other government data is used to support the company's sales forecast, which items to purchase, predict amount to be purchased, and determining which items to phase out. All Data driven management - made even easier with Hadoop.
All-Day: Strata's regular data science track has great talks with real world experience from leading edge speakers. But we didn't just stop therewe added the Hardcore Data Science day to give you a chance to go even deeper. The Hardcore day will add new techniques and technologies to your data science toolbox, shared by leading data science practitioners from startups, industry, consulting...
Business problems dont reveal themselves neatly as data problems. The data community is obsessed with tools and techniques, but the real challenge is understanding how to solve problems with data. How do we bridge the gap? In this talk, we will teach you a methodology for figuring out the right problems to solve and making sure that the work stays smart.
This talk highlights William's success, challenges, and experiences creating a data driven operations model into Ciscos engineering services organization. William highlights the role of data, the need for scale and security, the opportunity for new technology to accelerate business, the role of IT to help guide/partner, and the mind shift and cultural changes along the journey.
Over two years of running A/B testing at Pinterest on millions of users each day, Andrea learned about the nuances that can make or break an experimentation platform. Andrea will discuss how her approach to testing has adjusted over time to avoid critical errors at all levels, from organizational to analytical.
This discussion touches on the human response to analysis results, especially when they do not support long held beliefs and how this effects organizational change. This discussion also focuses on Predictive Analytics best practices, team skills, and a review of what it takes to build a sustainable Predictive Analytics program.
In this session, Kleiner Perkins Caufield & Byers General Partner Michael Abbott speaks with Geoff Guerdat of the Gilt Groupe, Will Moss of Airbnb, and Emil Ong of Lookout, to unbox their respective companies and examine the technology, architecture, and innovations theyve harnessed to deliver superior products and services.
In this session, Kleiner Perkins Caufield & Byers General Partner Michael Abbott speaks with Michael Stoppelman of Yelp and Siva Subramanian of Box to unbox their respective companies and examine the technology, architecture, and innovations theyve harnessed to deliver superior products and services.
To anticipate who will succeed and invest wisely, investors spend a lot of time trying to understand the longer-term trends within an industry. In this panel discussion, well consider the big trends in Big Data, asking top-tier VCs to look over the horizon discuss the visions they have two or more years in the future.
For a long time Internet retailers have been trying to move items they sell closer to customers. Flash sale site Gilt.com takes it to the extreme: we apply machine learning to predict customers' cravings for fashion products in different geographic regions without purchase history to draw from.
The Big Data market is busy, with sky-high valuations and a rapid pace of innovation. This panel of data-focused Venture Capitalists will look at how they think about investing in and around the Big Data spacefrom the kind of deals theyre after, to how they like to work with entrepreneurs and founders.
PDFs are the bane of data science, a jail from which machine-readable data struggles to escape. We'll explain how at Edmunds.com we freed data from diverse auto manufacturer PDFs, applied NLP and entity recognition, and integrated the results into the expert-driven process of defining vehicle models.
Panel discussion, moderated by Liza Kindred, Founder, Third Wave Fashion. Panelists include David Whittemore, founder of Clotheshorse; Gina Mancusco, founder of Love That Fit; and Rasmus Thofte, head of North America at Virtusize.
A look at how we use public governmental data to answer questions about our customers and their behavior; data used by marketing, space management, and product managers. Other government data is used to support the company's sales forecast, which items to purchase, predict amount to be purchased, and determining which items to phase out. All Data driven management - made even easier with Hadoop.
D3.js has a very steep learning curve. However, there are three main concepts that, once you get your head around them, will make the climb much easier. Focusing on these three main concepts, we will walk through many examples to teach the fundamental building blocks of D3.js.
Advanced math for business people: just enough math to take advantage of new classes of open source frameworks. Many take college math up to calculus, but never learn how to approach sparse matrices, complex graphs, or supply chain optimizations. This tutorial ties these pieces together into a conceptual whole, with use cases and simple Python code, as a new approach to computational thinking.
This session will outline Intels vision of an E2E Data Analytics Architecture for IoT as well as how we are enabling companies to elevate and transform the way they interact with their customers.
Up to 90% of your data is coming in new forms, in greater size, and at increasing speed. This multi-structured data requires a new workflow, putting the power of Hadoop and Spark into the hands of business analysts. In this session, we will share how Fortune 500 analysts have transformed their workflow by gaining insights into their business once never possible.
Todays unstructured data is raw and complex, but everyone agrees it can provide context and hidden insights when it is easily accessed during the business intelligence lifecycle. . .
This session will cover how MemSQLs hybrid transactional and analytic data processing capabilities and Apache Spark integration enable businesses to build real-time platforms for applications like operational analytics, position monitoring, and anomaly detection.
Learn how you can architect Amazon Kinesis and Amazon Elastic MapReduce together to create a highly scalable real-time analytics solution which can ingest and process terabytes of data per hour from hundreds of thousands of different concurrent sources.
The cloud is an amazing game changer for Data Science. This talk will show with demos and real world customer examples the magic that every data scientist can now perform in the cloud...
This session will describe the kinds of tools and solutions available in the market to tap into text sources.  Two use cases will be discussed and short demos used to illustrate a tools and a solution approach.
Visualizations can be easy on the eyes, until you need to view data at scale.  In this session, James Dixon, CTO and Co-Founder of Pentaho will talk about ways of presenting large scale datasets.  Using data from the City of Chicago, James will present practical examples that help distill large amounts of data in ways that are easier for users to comprehend.
Leveraging Hadoop data, served to users with advanced visualization in MicroStrategy, Netflix delivers effective, responsive insights quickly. This puts advanced analytics in the hands of business users who make the decisions that help the online entertainment network to outperform their rivals by serving consumers the content they want, how they want it.
It's been twenty years since Red Hat first launched Linux. Since then Red Hat has fueled the rapid adoption of open source technologies. As Big Data transitions into enterprise mode, Red Hat is again poised to facilitate the innovation and communities needed to empower multiple data stakeholders across your organization so you can truly open the possibilities of your data.
By reducing friction from deploying models and comparing competing models, data scientists can focus on high-value efforts. At Vast we've experimented with tools and strategies for this while shipping a suite of data products for consumers and agents in the midst of some of lifes biggest purchases.  I'll share best practices and lessons learned, and help you free up time for the fun stuff.
D3.js has a very steep learning curve. However, there are three main concepts that, once you get your head around them, will make the climb much easier. Focusing on these three main concepts, we will walk through many examples to teach the fundamental building blocks of D3.js.
Advanced math for business people: just enough math to take advantage of new classes of open source frameworks. Many take college math up to calculus, but never learn how to approach sparse matrices, complex graphs, or supply chain optimizations. This tutorial ties these pieces together into a conceptual whole, with use cases and simple Python code, as a new approach to computational thinking.
This session will outline Intels vision of an E2E Data Analytics Architecture for IoT as well as how we are enabling companies to elevate and transform the way they interact with their customers.
Up to 90% of your data is coming in new forms, in greater size, and at increasing speed. This multi-structured data requires a new workflow, putting the power of Hadoop and Spark into the hands of business analysts. In this session, we will share how Fortune 500 analysts have transformed their workflow by gaining insights into their business once never possible.
Todays unstructured data is raw and complex, but everyone agrees it can provide context and hidden insights when it is easily accessed during the business intelligence lifecycle. . .
This session will cover how MemSQLs hybrid transactional and analytic data processing capabilities and Apache Spark integration enable businesses to build real-time platforms for applications like operational analytics, position monitoring, and anomaly detection.
Learn how you can architect Amazon Kinesis and Amazon Elastic MapReduce together to create a highly scalable real-time analytics solution which can ingest and process terabytes of data per hour from hundreds of thousands of different concurrent sources.
The cloud is an amazing game changer for Data Science. This talk will show with demos and real world customer examples the magic that every data scientist can now perform in the cloud...
This session will describe the kinds of tools and solutions available in the market to tap into text sources.  Two use cases will be discussed and short demos used to illustrate a tools and a solution approach.
Visualizations can be easy on the eyes, until you need to view data at scale.  In this session, James Dixon, CTO and Co-Founder of Pentaho will talk about ways of presenting large scale datasets.  Using data from the City of Chicago, James will present practical examples that help distill large amounts of data in ways that are easier for users to comprehend.
Leveraging Hadoop data, served to users with advanced visualization in MicroStrategy, Netflix delivers effective, responsive insights quickly. This puts advanced analytics in the hands of business users who make the decisions that help the online entertainment network to outperform their rivals by serving consumers the content they want, how they want it.
It's been twenty years since Red Hat first launched Linux. Since then Red Hat has fueled the rapid adoption of open source technologies. As Big Data transitions into enterprise mode, Red Hat is again poised to facilitate the innovation and communities needed to empower multiple data stakeholders across your organization so you can truly open the possibilities of your data.
By reducing friction from deploying models and comparing competing models, data scientists can focus on high-value efforts. At Vast we've experimented with tools and strategies for this while shipping a suite of data products for consumers and agents in the midst of some of lifes biggest purchases.  I'll share best practices and lessons learned, and help you free up time for the fun stuff.
Big Data is reaching beyond the Internet and into the machines that drive our world. Visit Industrial Internet day to gain insights from the way that power plants, factories, cars, and airplanes make use of sensors and software intelligence to improve operations and help managers make good decisions.
In this session, you will learn why its powered by Spark, hear key business use cases from customers across various industries using it and gain understanding of the five fundamentals of speeding disparate data analysis.
Shifting workloads from the enterprise data warehouse (EDW) to Hadoop reduces costs, enables you to keep that data longer, and frees up EDW capacity for fast analytics. Check out our live demo and learn a proven framework for offloading workloads from the EDW to Hadoop: Identify & prioritize what to offload; Shift workloads to Hadoop; Optimize & secure your environment; and Visualize new insights.
Big Data and Analytics is still a young space but novel new methods are on the way. Prominent among them is graph analytics. Actian will show radical and innovative graph analytic capabilities, from its investment in SPARQL City.  Founded by database legend Barry Zane, SPARQL City and Actian are committed to delivering the industrys highest performing in memory graph analysis engine.
In this talk Arun Murthy will share the very latest innovation from the community aimed at accelerating the interactive and realtime capabilities of enterprise Hadoop.
SQL is the natural language for querying data, but data lives in many places.  We discuss the importance of SQL not only on Hadoop, but on relational databases, and noSQL stores.  Additionally, we dive deep into the architecture of Big Data SQL, which can access all of these sources in a single query.
Join us for a panel discussion that includes customers, industry experts and partners who are ready to explore the latest advances in Hadoop, from affordability and appliances, to Apache Spark, simplification and security.
Predictive modeling is as much art as it is science. The art is in matching your business questions to available data, and then pairing that data with the appropriate statistical techniques. Next comes model refinement, comparison and interpretation.  Well demonstrate how SAS and Hadoop work together to turn raw data into valuable information  and how you can visualize it for better decisions.
In this session we talk about how to design, build and manage large scale enterprise Big Data deployments, with its high disk IO apps to in-memory solutions, for both on-premise as well as multi-tenant cloud environments taking holistic view of all the components including compute, network and the software stack
Agile data transformation uses Hadoops schema-on-read capability to manipulate raw data as needed for business purposes. Transforming data can be a barrier to data access and agility consuming up to 80% of business analyst time. Hear directly from LinkedIn, Autodesk, MarketShare, and Orange about how predictive interactions make agile data transformation a reality on Hadoop.
Learn how Western Union uses Hadoop with Informatica to parse and integrate Omniture web log files, XML data, and relational transactions data to meet their current and future data analysis needs.
Many big data instances overlook human created content, we will discuss the business value and technology that can be used to tap into the power and showcase real life use cases. This content makes up the majority of the information produced by most organizations. Despite this fact, it has been under-used, under-valued and under-analyzed because of legacy technology limitations.
Big Data is reaching beyond the Internet and into the machines that drive our world. Visit Industrial Internet day to gain insights from the way that power plants, factories, cars, and airplanes make use of sensors and software intelligence to improve operations and help managers make good decisions.
In this session, you will learn why its powered by Spark, hear key business use cases from customers across various industries using it and gain understanding of the five fundamentals of speeding disparate data analysis.
Shifting workloads from the enterprise data warehouse (EDW) to Hadoop reduces costs, enables you to keep that data longer, and frees up EDW capacity for fast analytics. Check out our live demo and learn a proven framework for offloading workloads from the EDW to Hadoop: Identify & prioritize what to offload; Shift workloads to Hadoop; Optimize & secure your environment; and Visualize new insights.
Big Data and Analytics is still a young space but novel new methods are on the way. Prominent among them is graph analytics. Actian will show radical and innovative graph analytic capabilities, from its investment in SPARQL City.  Founded by database legend Barry Zane, SPARQL City and Actian are committed to delivering the industrys highest performing in memory graph analysis engine.
In this talk Arun Murthy will share the very latest innovation from the community aimed at accelerating the interactive and realtime capabilities of enterprise Hadoop.
SQL is the natural language for querying data, but data lives in many places.  We discuss the importance of SQL not only on Hadoop, but on relational databases, and noSQL stores.  Additionally, we dive deep into the architecture of Big Data SQL, which can access all of these sources in a single query.
Join us for a panel discussion that includes customers, industry experts and partners who are ready to explore the latest advances in Hadoop, from affordability and appliances, to Apache Spark, simplification and security.
Predictive modeling is as much art as it is science. The art is in matching your business questions to available data, and then pairing that data with the appropriate statistical techniques. Next comes model refinement, comparison and interpretation.  Well demonstrate how SAS and Hadoop work together to turn raw data into valuable information  and how you can visualize it for better decisions.
In this session we talk about how to design, build and manage large scale enterprise Big Data deployments, with its high disk IO apps to in-memory solutions, for both on-premise as well as multi-tenant cloud environments taking holistic view of all the components including compute, network and the software stack
Agile data transformation uses Hadoops schema-on-read capability to manipulate raw data as needed for business purposes. Transforming data can be a barrier to data access and agility consuming up to 80% of business analyst time. Hear directly from LinkedIn, Autodesk, MarketShare, and Orange about how predictive interactions make agile data transformation a reality on Hadoop.
Learn how Western Union uses Hadoop with Informatica to parse and integrate Omniture web log files, XML data, and relational transactions data to meet their current and future data analysis needs.
Many big data instances overlook human created content, we will discuss the business value and technology that can be used to tap into the power and showcase real life use cases. This content makes up the majority of the information produced by most organizations. Despite this fact, it has been under-used, under-valued and under-analyzed because of legacy technology limitations.
From advanced visualization, collaboration, reproducibility to data manipulation, R Day at Strata covers a raft of current topics that analysts and R users need to pay attention to. The R Day tutorials come from leading luminaries and R committers, the folks keeping the R ecosystem apace of the challenges facing analysts and others who work with data.
Learn the critical success factors for organizational success with Hadoop and building the right team and skill sets for high performance Hadoop success from a veteran of three successful Hadoop projects.
In this session you will hear from big data experts with real world experience on the architectural patterns and platform integrations used to solve real business problems with data.
This session will examine the distribution and storage of data in HDFS across multiple datacenters in a single coordinated, Paxos-based file system over a WAN. Efficient use of compute resources in a globally distributed HDFS cluster is also discussed.
Join TIBCO Software,an industry leader in infrastructure and analytics software,for a thought leadership discussion to learn how your organization can redefine its data strategy.Transition from a company of Big Data to Fast Data and convert your customers into fans whileachievinga competitive advantage.
This session provides a brief overview of  Couchbase Server, a document database and its underlying distributed architecture.
Deriving value from data depends on how well companies capture and manage that data. Learn how to create a centralized processing pool where data can be captured, cleansed, linked and structured in a consistent way. Use the scalability and flexibility of Hadoop to create a powerful processing and refinement engine to drive usable information across enterprise data bases and data marts.
Studies are showing the vast majority of Big Data projects involve 2 or more data platforms. Moving data is costly and must be carefully considered. . .
Developing Big Data applications for real-world business processes can be complex: method of processing, variety of systems, # of data sources. Large web companies have implemented a generic, scalable, fault-tolerant data processing architecture: LAMDA. Well explore this evolving architecture, design principles, layers/components, & use cases/lessons learned from real-world implementations.
Join us to learn how to leverage new Distributed R open source technology from the HP Labs and HP Vertica. Distributed R platform introduces new easy to use distributed programming model and infrastructure for the R language. Distributed R includes out-of-the-box open source parallel R algorithms that can scale for terabytes of data.
Data continues to drive innovation, yet its how we interpret and use that data that becomes imperative to success. Using a design approach called Natural Analytics, technology can leverage the way our human curiosity searches and processes information.
If the allure of Big Data is that you can throw it all in the data lake and process it cheaply and quickly, then the catch is how do you know what's in there and how do you govern it?A Big Data lake needs data governance to create trusted data, ensureconsistency, and secure information appropriately.  This session will discuss how to start putting a Big Data governance framework in place.
From advanced visualization, collaboration, reproducibility to data manipulation, R Day at Strata covers a raft of current topics that analysts and R users need to pay attention to. The R Day tutorials come from leading luminaries and R committers, the folks keeping the R ecosystem apace of the challenges facing analysts and others who work with data.
Learn the critical success factors for organizational success with Hadoop and building the right team and skill sets for high performance Hadoop success from a veteran of three successful Hadoop projects.
In this session you will hear from big data experts with real world experience on the architectural patterns and platform integrations used to solve real business problems with data.
This session will examine the distribution and storage of data in HDFS across multiple datacenters in a single coordinated, Paxos-based file system over a WAN. Efficient use of compute resources in a globally distributed HDFS cluster is also discussed.
Join TIBCO Software,an industry leader in infrastructure and analytics software,for a thought leadership discussion to learn how your organization can redefine its data strategy.Transition from a company of Big Data to Fast Data and convert your customers into fans whileachievinga competitive advantage.
This session provides a brief overview of  Couchbase Server, a document database and its underlying distributed architecture.
Deriving value from data depends on how well companies capture and manage that data. Learn how to create a centralized processing pool where data can be captured, cleansed, linked and structured in a consistent way. Use the scalability and flexibility of Hadoop to create a powerful processing and refinement engine to drive usable information across enterprise data bases and data marts.
Studies are showing the vast majority of Big Data projects involve 2 or more data platforms. Moving data is costly and must be carefully considered. . .
Developing Big Data applications for real-world business processes can be complex: method of processing, variety of systems, # of data sources. Large web companies have implemented a generic, scalable, fault-tolerant data processing architecture: LAMDA. Well explore this evolving architecture, design principles, layers/components, & use cases/lessons learned from real-world implementations.
Join us to learn how to leverage new Distributed R open source technology from the HP Labs and HP Vertica. Distributed R platform introduces new easy to use distributed programming model and infrastructure for the R language. Distributed R includes out-of-the-box open source parallel R algorithms that can scale for terabytes of data.
Data continues to drive innovation, yet its how we interpret and use that data that becomes imperative to success. Using a design approach called Natural Analytics, technology can leverage the way our human curiosity searches and processes information.
If the allure of Big Data is that you can throw it all in the data lake and process it cheaply and quickly, then the catch is how do you know what's in there and how do you govern it?A Big Data lake needs data governance to create trusted data, ensureconsistency, and secure information appropriately.  This session will discuss how to start putting a Big Data governance framework in place.
Companies are deploying Hadoop data lakes to provide unprecedented access to data for data science and analytics. However, the advantages of frictionless ingest, flexible schema on read, and lack of data governance, turn into increasingly insurmountable challenges to enable true data self-service, and create a barrier to the enterprise adoption of Hadoop.
Apache Cassandra has proven to be one of the best solutions for storing and retrieving time series data. Add in Apache Spark and Kafka, you have an amazing time series solution. We will talk data models, go through deployment and code to build a functional, real-time application. Languages used: Java, Scala
Companies are deploying Hadoop data lakes to provide unprecedented access to data for data science and analytics. However, the advantages of frictionless ingest, flexible schema on read, and lack of data governance, turn into increasingly insurmountable challenges to enable true data self-service, and create a barrier to the enterprise adoption of Hadoop.
Apache Cassandra has proven to be one of the best solutions for storing and retrieving time series data. Add in Apache Spark and Kafka, you have an amazing time series solution. We will talk data models, go through deployment and code to build a functional, real-time application. Languages used: Java, Scala
Bootstrapping is a statistical technique that resamples data many times overan effective method for determining confidence in A/B test results but an expensive procedure in a world of big data. Emily Sommer explains how Etsy implemented the Bag of Little Bootstraps, a clever take on bootstrapping that involves examining many smaller subsets of one's data.
Data-driven decision making is still contentious, with decision makers skeptical that the data knows more than they do. Often they're right; if data is not communicated with a good understanding of the uncertainty, the findings can be meaningless. Abigail Lebrecht uses Bayesian and frequentist techniques to highlight bad data communication in business and the media and shows how to get it right.
Can our real-time distributed data systems help predict whether high-resolution audio is the future of digital music? What about content curation? Paul Shannon and Alan Hannaway explore the future of music services through data and explain why 7digital believes well-curated, high-resolution listening experiences are the future of digital music services.
A data-driven culture empowers companies to deliver greater value to their customers, yet many organizations still struggle to break down cultural barriers and drive data-driven innovation across their products. Lucian Lita, Mita Mahadevan, Shalin Mantri, and Gabrielle Gianelli explore Intuit's, Uber's, and Etsy's A/B platforms, which enable experimentation and engender a data-driven mentality.
For centuries, the decisions made in a company were the responsibility of the top managers. But when firms harness AI and big data, algorithms can make millions more decisions in the same time, and probably better ones. Kenneth Cukier explores how this affects the ways that companies are organized and how they compete and set strategy (as opposed to just execution).
In 2014, Times Higher Education made the decision to move from being a traditional publisher to being a data business. As part of the move, it needed to bring the creation of the World University Rankings in-house and build a set of data products from scratch. Duncan Ross and Francine Bennett explain how the transition was made and highlight the challenges and lessons learned.
With over 90% of the worlds trade transported over the oceans, data on ship activity is critical to decision makers across industries. But despite the huge stakes at sea, ship activity remains a mystery: the data is massive, fragmented, and extremely unreliable when taken as is. Tal Guttman explores how data science can shed light on this critically important but opaque world.
Data is all sales and marketing. The reality of data work is pain. Most data projects fail and are horrible experiences to work on. Phil Harvey explains that data is just too hardthe world needs to talk about real challenges so that we can start tackling them to deliver data projects that work. This is DataOps; there will be tears before bedtime.
At first glance A/B testing is a simple matter: take a few numbers, put them into an online calculator, and read off the statistical significance. But in fact it's a complex topic with amazing opportunities (and pitfalls) for organizations. Marton Trencseni offers a deep dive into A/B testing to provide attendees the information needed to improve their organizations' experimentation cultures.
Nick Turner offers an insightful view on how technology is delivering self-service analytics through visualization and enabling business users to quickly explore their data at scale.
As Intuit evolved QuickBooks, Payroll, Payments, and other product offerings into a SaaS business and an open cloud platform, it quickly became apparent that business analytics could no longer be treated as an afterthought but had to be part of the platform architecture as a first-class concern. Calum Murray outlines key design considerations when architecting analytics into your SaaS platform.
Bootstrapping is a statistical technique that resamples data many times overan effective method for determining confidence in A/B test results but an expensive procedure in a world of big data. Emily Sommer explains how Etsy implemented the Bag of Little Bootstraps, a clever take on bootstrapping that involves examining many smaller subsets of one's data.
Data-driven decision making is still contentious, with decision makers skeptical that the data knows more than they do. Often they're right; if data is not communicated with a good understanding of the uncertainty, the findings can be meaningless. Abigail Lebrecht uses Bayesian and frequentist techniques to highlight bad data communication in business and the media and shows how to get it right.
Can our real-time distributed data systems help predict whether high-resolution audio is the future of digital music? What about content curation? Paul Shannon and Alan Hannaway explore the future of music services through data and explain why 7digital believes well-curated, high-resolution listening experiences are the future of digital music services.
A data-driven culture empowers companies to deliver greater value to their customers, yet many organizations still struggle to break down cultural barriers and drive data-driven innovation across their products. Lucian Lita, Mita Mahadevan, Shalin Mantri, and Gabrielle Gianelli explore Intuit's, Uber's, and Etsy's A/B platforms, which enable experimentation and engender a data-driven mentality.
For centuries, the decisions made in a company were the responsibility of the top managers. But when firms harness AI and big data, algorithms can make millions more decisions in the same time, and probably better ones. Kenneth Cukier explores how this affects the ways that companies are organized and how they compete and set strategy (as opposed to just execution).
In 2014, Times Higher Education made the decision to move from being a traditional publisher to being a data business. As part of the move, it needed to bring the creation of the World University Rankings in-house and build a set of data products from scratch. Duncan Ross and Francine Bennett explain how the transition was made and highlight the challenges and lessons learned.
With over 90% of the worlds trade transported over the oceans, data on ship activity is critical to decision makers across industries. But despite the huge stakes at sea, ship activity remains a mystery: the data is massive, fragmented, and extremely unreliable when taken as is. Tal Guttman explores how data science can shed light on this critically important but opaque world.
Data is all sales and marketing. The reality of data work is pain. Most data projects fail and are horrible experiences to work on. Phil Harvey explains that data is just too hardthe world needs to talk about real challenges so that we can start tackling them to deliver data projects that work. This is DataOps; there will be tears before bedtime.
At first glance A/B testing is a simple matter: take a few numbers, put them into an online calculator, and read off the statistical significance. But in fact it's a complex topic with amazing opportunities (and pitfalls) for organizations. Marton Trencseni offers a deep dive into A/B testing to provide attendees the information needed to improve their organizations' experimentation cultures.
Nick Turner offers an insightful view on how technology is delivering self-service analytics through visualization and enabling business users to quickly explore their data at scale.
As Intuit evolved QuickBooks, Payroll, Payments, and other product offerings into a SaaS business and an open cloud platform, it quickly became apparent that business analytics could no longer be treated as an afterthought but had to be part of the platform architecture as a first-class concern. Calum Murray outlines key design considerations when architecting analytics into your SaaS platform.
The real power and value proposition of Apache Spark is in building a unified use case that combines ETL, batch analytics, real-time stream analysis, machine learning, graph processing, and visualizations. Through hands-on examples, Sameer Farooqui explores various Wikipedia datasets to illustrate a variety of ideal programming paradigms.
Deep learning is taking data science by storm, due to the combination of stable distributed computing technologies, increasing amounts of data, and available computing resources. Andy Petrella and Melanie Warrick show how to implement a Spark-ready version of the long short-term memory (LSTM) neural network, widely used in the hardest natural language processing and understanding problems.
David Talby and Claudiu Branzan offer a live demo of an end-to-end system that makes nontrivial clinical inferences from free-text patient records. Infrastructure components include Kafka, Spark Streaming, Spark, Titan, and Elasticsearch; data science components include custom UIMA annotators, curated taxonomies, machine-learned dynamic ontologies, and real-time inferencing.
The generalized low-rank model is a new machine-learning approach for reconstructing missing values and identifying important features in heterogeneous data. Through a series of examples, Jo-fai Chow demonstrates how to fit low-rank models in a parallelized framework and how to use these models to make better predictions.
The advent of next-generation DNA sequencing technologies is revolutionizing life sciences research by routinely generating extremely large datasets. Tom White explains how big data tools developed to handle large-scale Internet data (like Hadoop) help scientists effectively manage this new scale of data and also enable addressing a host of questions that were previously out of reach.
A polyglot is a person who knows and is able to use several languages. There are a plethora of programming languages and computing environments available for working with data, and some data science projects require using multiple languages together. Jeroen Janssens discusses three approaches to become a polyglot data scientist.
Thomas Wiecki explores the prevalence of backtest overfitting and debunks several common myths in quantitative finance based on empirical findings. Thomas demonstrates how he trained a machine-learning classifier on Quantopian's huge and unique dataset of over 800,000 trading algorithms to predict if an algorithm is overfit and how its future performance will likely unfold.
Andy Petrella and Dean Wampler explore what it means to do data science today and why Scala succeeds at coping with large and fast data where older languages fail. Andy and Dean then discuss the current ongoing projects in advanced data science that use Scala as the main language, including Splash, mic-cut problem, OptiML, needle (DL), ADAM, and more.
Marcel Kornacker explains how nested data structures can increase analytic productivity, using the well-known TPC-H schema to demonstrate how to simplify analytic workloads with nested schemas.
Dave McCrory explores the concept of data gravitythe effect that as data accumulates, there is a greater likelihood that additional services and applications will be attracted to this data, essentially having the same effect gravity has on objects around a planetand discusses how the giant cycle of expansion and use of data and services in the cloud is created and what to do about it.
Applying a data-driven approach to the recruitment process has long been an aspirational goal for many organizations. In recent years, through the use of data science, it has become a genuine reality. Gary Willis explains how data science and, more importantly, an intelligent approach to interview design have enabled companies to start identifying unconscious bias in their recruitment process.
The real power and value proposition of Apache Spark is in building a unified use case that combines ETL, batch analytics, real-time stream analysis, machine learning, graph processing, and visualizations. Through hands-on examples, Sameer Farooqui explores various Wikipedia datasets to illustrate a variety of ideal programming paradigms.
Deep learning is taking data science by storm, due to the combination of stable distributed computing technologies, increasing amounts of data, and available computing resources. Andy Petrella and Melanie Warrick show how to implement a Spark-ready version of the long short-term memory (LSTM) neural network, widely used in the hardest natural language processing and understanding problems.
David Talby and Claudiu Branzan offer a live demo of an end-to-end system that makes nontrivial clinical inferences from free-text patient records. Infrastructure components include Kafka, Spark Streaming, Spark, Titan, and Elasticsearch; data science components include custom UIMA annotators, curated taxonomies, machine-learned dynamic ontologies, and real-time inferencing.
The generalized low-rank model is a new machine-learning approach for reconstructing missing values and identifying important features in heterogeneous data. Through a series of examples, Jo-fai Chow demonstrates how to fit low-rank models in a parallelized framework and how to use these models to make better predictions.
The advent of next-generation DNA sequencing technologies is revolutionizing life sciences research by routinely generating extremely large datasets. Tom White explains how big data tools developed to handle large-scale Internet data (like Hadoop) help scientists effectively manage this new scale of data and also enable addressing a host of questions that were previously out of reach.
A polyglot is a person who knows and is able to use several languages. There are a plethora of programming languages and computing environments available for working with data, and some data science projects require using multiple languages together. Jeroen Janssens discusses three approaches to become a polyglot data scientist.
Thomas Wiecki explores the prevalence of backtest overfitting and debunks several common myths in quantitative finance based on empirical findings. Thomas demonstrates how he trained a machine-learning classifier on Quantopian's huge and unique dataset of over 800,000 trading algorithms to predict if an algorithm is overfit and how its future performance will likely unfold.
Andy Petrella and Dean Wampler explore what it means to do data science today and why Scala succeeds at coping with large and fast data where older languages fail. Andy and Dean then discuss the current ongoing projects in advanced data science that use Scala as the main language, including Splash, mic-cut problem, OptiML, needle (DL), ADAM, and more.
Marcel Kornacker explains how nested data structures can increase analytic productivity, using the well-known TPC-H schema to demonstrate how to simplify analytic workloads with nested schemas.
Dave McCrory explores the concept of data gravitythe effect that as data accumulates, there is a greater likelihood that additional services and applications will be attracted to this data, essentially having the same effect gravity has on objects around a planetand discusses how the giant cycle of expansion and use of data and services in the cloud is created and what to do about it.
Applying a data-driven approach to the recruitment process has long been an aspirational goal for many organizations. In recent years, through the use of data science, it has become a genuine reality. Gary Willis explains how data science and, more importantly, an intelligent approach to interview design have enabled companies to start identifying unconscious bias in their recruitment process.
Jayesh Seshadri, Justin Hancock, Mark Samson, and Wellington Chevreuil offer a full-day deep dive into all phases of successfully managing Hadoop clustersfrom installation to configuration management, service monitoring, troubleshooting, and support integrationwith an emphasis on production systems.
Ben Lorica hosts a conversation with Hadoop cofounder Doug Cutting and Tom White, an early user and committer of Apache Hadoop.
Big data provides an unprecedented opportunity to really understand and engage with your customers, but only if you have the keys to unlock the value in the data. Through examples from the Royal Bank of Scotland, Dan Jermyn and Connor Carreras explain how to use data wrangling to harness the power of data stored on Hadoop and deliver personalized interactions to increase customer satisfaction.
Fergal Toomey and Pierre Lacave demonstrate how to effectively use Spark and Hadoop to reliably analyze data in high-speed trading environments across multiple machines in real time.
Value at risk (VaR) is a widely used risk measure. VaR is not simply additive, which provides unique challenges to report VaR at any aggregate level, as traditional database aggregation functions don't work. Deenar Toraskar explains how the Hive complex data types and user-defined functions can be used very effectively to provide simple, fast, and flexible VaR aggregation.
Risk data aggregation and risk reporting (RDARR) is critical to compliance in financial services. Big data expert Ben Sharma explores multiple use cases to demonstrate how organizations in the financial services industry are building big data lakes that deliver the necessary components for risk data aggregation and risk reporting.
Stuart Russell and Jaan Tallinn explore and debate the future of artificial intelligence in a panel discussion moderated by Marc Warner.
Steven Noels explains how to prime the Hadoop ecosystem for real-time data analysis and actionability, examining ways to evolve from batch processing to real-time stream-based processing.
Experience tells us a decision is only as good as the information it is based on. The same is true for driving. The better a vehicle knows its surroundings, the better it can support the driver. Information makes vehicles safer, more efficient, and more comfortable. Thomas Beer and Felix Werkmeister explain how Continental exploits big data technologies for building information-driven vehicles.
Dirk Gorissen demonstrates how to use machine learning to detect land mines from a drone-mounted ground-penetrating radar sensor.
Apache Eagle is an open source monitoring solution to instantly identify access to sensitive data, recognize malicious activities, and take action. Arun Karthick Manoharan, Edward Zhang, and Chaitali Gupta explain how Eagle helps secure a Hadoop cluster using policy-based and machine-learning user-profile-based detection and alerting.
Privacy is no longer "a social norm," but this may not survive as the Internet of Things grows. Big data is all very well when it is harvested in the background. But it's a very different matter altogether when your things tattle on you behind your back. Alasdair Allan explains how the rush to connect devices to the Internet has led to sloppy privacy and security and why that can't continue.
Jayesh Seshadri, Justin Hancock, Mark Samson, and Wellington Chevreuil offer a full-day deep dive into all phases of successfully managing Hadoop clustersfrom installation to configuration management, service monitoring, troubleshooting, and support integrationwith an emphasis on production systems.
Ben Lorica hosts a conversation with Hadoop cofounder Doug Cutting and Tom White, an early user and committer of Apache Hadoop.
Big data provides an unprecedented opportunity to really understand and engage with your customers, but only if you have the keys to unlock the value in the data. Through examples from the Royal Bank of Scotland, Dan Jermyn and Connor Carreras explain how to use data wrangling to harness the power of data stored on Hadoop and deliver personalized interactions to increase customer satisfaction.
Fergal Toomey and Pierre Lacave demonstrate how to effectively use Spark and Hadoop to reliably analyze data in high-speed trading environments across multiple machines in real time.
Value at risk (VaR) is a widely used risk measure. VaR is not simply additive, which provides unique challenges to report VaR at any aggregate level, as traditional database aggregation functions don't work. Deenar Toraskar explains how the Hive complex data types and user-defined functions can be used very effectively to provide simple, fast, and flexible VaR aggregation.
Risk data aggregation and risk reporting (RDARR) is critical to compliance in financial services. Big data expert Ben Sharma explores multiple use cases to demonstrate how organizations in the financial services industry are building big data lakes that deliver the necessary components for risk data aggregation and risk reporting.
Stuart Russell and Jaan Tallinn explore and debate the future of artificial intelligence in a panel discussion moderated by Marc Warner.
Steven Noels explains how to prime the Hadoop ecosystem for real-time data analysis and actionability, examining ways to evolve from batch processing to real-time stream-based processing.
Experience tells us a decision is only as good as the information it is based on. The same is true for driving. The better a vehicle knows its surroundings, the better it can support the driver. Information makes vehicles safer, more efficient, and more comfortable. Thomas Beer and Felix Werkmeister explain how Continental exploits big data technologies for building information-driven vehicles.
Dirk Gorissen demonstrates how to use machine learning to detect land mines from a drone-mounted ground-penetrating radar sensor.
Apache Eagle is an open source monitoring solution to instantly identify access to sensitive data, recognize malicious activities, and take action. Arun Karthick Manoharan, Edward Zhang, and Chaitali Gupta explain how Eagle helps secure a Hadoop cluster using policy-based and machine-learning user-profile-based detection and alerting.
Privacy is no longer "a social norm," but this may not survive as the Internet of Things grows. Big data is all very well when it is harvested in the background. But it's a very different matter altogether when your things tattle on you behind your back. Alasdair Allan explains how the rush to connect devices to the Internet has led to sloppy privacy and security and why that can't continue.
Jayant Shekhar, Vartika Singh, and Krishna Sankar explore techniques for building machine-learning apps using Spark ML as well as the principles of graph processing with Spark GraphX.
What are the essential components of a data platform? John Akred and Stephen O'Sullivan explain how the various parts of the Hadoop and big data ecosystems fit together in production to create a data platform supporting batch, interactive, and real-time analytical workloads.
In the last few years, auto makers and others have introduced devices to connect cars to the Internet and gather data about the vehicles activity, and auto insurers and local governments are just starting to require these devices. Charles Givre gives an overview of the security risks as well as the potential privacy invasions associated with this unique type of data collection.
Kafka provides the low latency, high throughput, high availability, and scale that financial services firms require. But can it also provide complete reliability? Gwen Shapira and Jeff Holoman explain how developers and operation teams can work together to build a bulletproof data pipeline with Kafka and pinpoint all the places where data can be lost if you're not careful.
Dataiku and Bioserenity have built a system for an at-home, real-time EEG and, in the process, created an open source stack for handling the data from connected devices. Eric Kramer offers an overview of the tools Dataiku and Bioserenity use to handle large amounts of time series data and explains how they created a real-time web app that processes petabytes of data generated by connected devices.
For decades, industrial manufacturing has dealt with large volumes of sensor data and handled a variety of data from the various manufacturing operations management (MOM) systems in production, quality, maintenance, and inventory. Gopal GopalKrishnan and Hoa Tram offer lessons learned from applying big data ecosystem tools to oil and gas, energy, utilities, metals, and mining use cases.
Apache NiFi has seen it all. (It worked for the NSA after all.) What it brings to the Hadoop ecosystem is a series of data flow and ingest patterns, a GUI, and a lot of security and record-level data provenance. Simon Elliston Ball offers an overview of Apache NiFi and explores its innovations around content and provenance repositories.
Apache Kafka lies at the heart of the largest data pipelines, handling trillions of messages and petabytes of data every day. Gwen Shapira and Todd Palino explain the right approach for getting the most out of Kafka, exploring how to monitor, optimize, and troubleshoot performance of your data pipelines from producer to consumer and from development to production.
Exactly-once semantics is a highly desirable property for streaming analytics. Ideally, all applications process events once and never twice, but making such guarantees in general either induces significant overhead or introduces other inconveniences, such as stalling. Flavio Junqueira explores what's possible and reasonable for streaming analytics to achieve when targeting exactly-once semantics.
Drawing on important real-world use cases, Kenneth Knowles delves into the details of the language- and runner-independent semantics developed for triggers in Apache Beam, demonstrating how the semantics support the use cases as well as all of the above variability in streaming systems. Kenneth then describes some of the particular implementations of those semantics in Google Cloud Dataflow.
Neha Narkhede offers an overview of Kafka Streams, a new stream processing library natively integrated with Apache Kafka. It has a very low barrier to entry, easy operationalization, and a natural DSL for writing stream processing applications. As such, it is the most convenient yet scalable option to analyze, transform, or otherwise process data that is backed by Kafka.
Data stream processing is emerging as a new paradigm for the data infrastructure. Streaming promises to unify and simplify many existing applications while simultaneously enabling new applications on both real-time and historical data. Stephan Ewen and Kostas Tzoumas introduce the data streaming paradigm and show how to build a set of simple but representative applications using Apache Flink.
ICT systems are growing in size and complexity. Monitoring and orchestration mechanisms need to evolve and provide richer capabilities to help handle them. Ignacio Manuel Mulas Viela and Nicolas Seyvet analyze a stream of telemetry/logs in real time by following the Kappa architecture paradigm, using machine-learning algorithms to spot unexpected behaviors from an in-production cloud system.
Jayant Shekhar, Vartika Singh, and Krishna Sankar explore techniques for building machine-learning apps using Spark ML as well as the principles of graph processing with Spark GraphX.
What are the essential components of a data platform? John Akred and Stephen O'Sullivan explain how the various parts of the Hadoop and big data ecosystems fit together in production to create a data platform supporting batch, interactive, and real-time analytical workloads.
In the last few years, auto makers and others have introduced devices to connect cars to the Internet and gather data about the vehicles activity, and auto insurers and local governments are just starting to require these devices. Charles Givre gives an overview of the security risks as well as the potential privacy invasions associated with this unique type of data collection.
Kafka provides the low latency, high throughput, high availability, and scale that financial services firms require. But can it also provide complete reliability? Gwen Shapira and Jeff Holoman explain how developers and operation teams can work together to build a bulletproof data pipeline with Kafka and pinpoint all the places where data can be lost if you're not careful.
Dataiku and Bioserenity have built a system for an at-home, real-time EEG and, in the process, created an open source stack for handling the data from connected devices. Eric Kramer offers an overview of the tools Dataiku and Bioserenity use to handle large amounts of time series data and explains how they created a real-time web app that processes petabytes of data generated by connected devices.
For decades, industrial manufacturing has dealt with large volumes of sensor data and handled a variety of data from the various manufacturing operations management (MOM) systems in production, quality, maintenance, and inventory. Gopal GopalKrishnan and Hoa Tram offer lessons learned from applying big data ecosystem tools to oil and gas, energy, utilities, metals, and mining use cases.
Apache NiFi has seen it all. (It worked for the NSA after all.) What it brings to the Hadoop ecosystem is a series of data flow and ingest patterns, a GUI, and a lot of security and record-level data provenance. Simon Elliston Ball offers an overview of Apache NiFi and explores its innovations around content and provenance repositories.
Apache Kafka lies at the heart of the largest data pipelines, handling trillions of messages and petabytes of data every day. Gwen Shapira and Todd Palino explain the right approach for getting the most out of Kafka, exploring how to monitor, optimize, and troubleshoot performance of your data pipelines from producer to consumer and from development to production.
Exactly-once semantics is a highly desirable property for streaming analytics. Ideally, all applications process events once and never twice, but making such guarantees in general either induces significant overhead or introduces other inconveniences, such as stalling. Flavio Junqueira explores what's possible and reasonable for streaming analytics to achieve when targeting exactly-once semantics.
Drawing on important real-world use cases, Kenneth Knowles delves into the details of the language- and runner-independent semantics developed for triggers in Apache Beam, demonstrating how the semantics support the use cases as well as all of the above variability in streaming systems. Kenneth then describes some of the particular implementations of those semantics in Google Cloud Dataflow.
Neha Narkhede offers an overview of Kafka Streams, a new stream processing library natively integrated with Apache Kafka. It has a very low barrier to entry, easy operationalization, and a natural DSL for writing stream processing applications. As such, it is the most convenient yet scalable option to analyze, transform, or otherwise process data that is backed by Kafka.
Data stream processing is emerging as a new paradigm for the data infrastructure. Streaming promises to unify and simplify many existing applications while simultaneously enabling new applications on both real-time and historical data. Stephan Ewen and Kostas Tzoumas introduce the data streaming paradigm and show how to build a set of simple but representative applications using Apache Flink.
ICT systems are growing in size and complexity. Monitoring and orchestration mechanisms need to evolve and provide richer capabilities to help handle them. Ignacio Manuel Mulas Viela and Nicolas Seyvet analyze a stream of telemetry/logs in real time by following the Kappa architecture paradigm, using machine-learning algorithms to spot unexpected behaviors from an in-production cloud system.
Jonathan Seidman, Mark Grover, Gwen Shapira, and Ted Malaska walk attendees through an end-to-end case study of building a fraud detection system, providing a concrete example of how to architect and implement real-time systems.
Aimee Gott, Mark Sellors, and Richard Pugh explore techniques for optimizing your workflow in R when working with big data, including how to efficiently extract data from a database, techniques for visualization and analysis, and how all of this can be incorporated into a single, reproducible report, directly from R.
Spark 2.0 is a major milestone for the project. It achieves major advances in performance and introduces new initiatives to unify streaming processing with the Sparks SQL engine. Tathagata Das explores these exciting new developments in Spark 2.0 as well as some other major initiatives that are coming in the future.
So youve successfully tackled big data. Now let Vida Ha and Prakash Chockalingam help you take it real time and conquer fast data. Vida and Prakash cover the most common uses cases for streaming, important streaming design patterns, and the best practices for implementing them to achieve maximum throughput and performance of your system using Spark Streaming.
Hadoop is used to run large-scale jobs over hundreds of machines. Considering the complexity of Hadoop jobs, it's no wonder that Hadoop jobs running slower than expected remains a perennial source of grief for developers. Bikas Saha draws on his experience debugging and analyzing Hadoop jobs to describe the approaches and tools that can solve this difficult problem.
Telecom operators need to find operational anomalies in their networks very quickly. Spark plus a streaming architecture can solve these problems very nicely. Ted Dunning presents a practical architecture as well as some detailed algorithms for detecting anomalies in event streams. These algorithms are simple and quite general and can be applied across a wide variety of situations.
Holden Karau walks attendees through a number of common mistakes that can keep your Spark programs from scaling and examines solutions and general techniques useful for moving beyond a proof of concept to production, covering topics like when to use DataFrames, tuning, and working with key skew.
As Spark is used more and more frequently for production workloads with stringent security requirements, fully locking down Spark applications has become critical. Kostas Sakellis explores the various facets of securing your Spark application.
Tathagata Das explains how Spark 2.x develops the next evolution of Spark Streaming by extending DataFrames and Datasets in Spark to handle streaming data. Streaming Datasets provides a single programming abstraction for batch and streaming data and also brings support for event-time-based processing, out-of-order data, sessionization, and tight integration with nonstreaming data sources.
Reactive Streams is an API designed to connect reactive systems with back-pressure. Luc Bourlier explains why, with Spark Streaming now supporting back-pressure, Reactive Streams is the right tool to connect Spark Streaming in a reactive application.
Ted Malaska leads an introduction to basic Spark concepts such as DAGs, RDDs, transformations, actions, and executors, designed for Java and Scala developers. You'll learn how your mindset must evolve beyond Java or Scala code that runs in a single JVM as you explore JVM locality, memory utilization, network/CPU usage, optimization of DAGs pipelines, and serialization conservation.
Heron has been in production at Twitter for nearly two years and is widely used by several teams for diverse use cases. Karthik Ramasamy describes Heron in detail, covering a few use cases in-depth and sharing the operating experiences and challenges of running Heron at scale.
Spark has been growing in deployments for the past year. The increasing amount of data being analyzed and processed through the framework is massive and continues to push the boundaries of the engine. Drawing on his experiences across 150+ production deployments, Neelesh Srinivas Salian explores common issues observed in a cluster environment setup with Apache Spark.
Jonathan Seidman, Mark Grover, Gwen Shapira, and Ted Malaska walk attendees through an end-to-end case study of building a fraud detection system, providing a concrete example of how to architect and implement real-time systems.
Aimee Gott, Mark Sellors, and Richard Pugh explore techniques for optimizing your workflow in R when working with big data, including how to efficiently extract data from a database, techniques for visualization and analysis, and how all of this can be incorporated into a single, reproducible report, directly from R.
Spark 2.0 is a major milestone for the project. It achieves major advances in performance and introduces new initiatives to unify streaming processing with the Sparks SQL engine. Tathagata Das explores these exciting new developments in Spark 2.0 as well as some other major initiatives that are coming in the future.
So youve successfully tackled big data. Now let Vida Ha and Prakash Chockalingam help you take it real time and conquer fast data. Vida and Prakash cover the most common uses cases for streaming, important streaming design patterns, and the best practices for implementing them to achieve maximum throughput and performance of your system using Spark Streaming.
Hadoop is used to run large-scale jobs over hundreds of machines. Considering the complexity of Hadoop jobs, it's no wonder that Hadoop jobs running slower than expected remains a perennial source of grief for developers. Bikas Saha draws on his experience debugging and analyzing Hadoop jobs to describe the approaches and tools that can solve this difficult problem.
Telecom operators need to find operational anomalies in their networks very quickly. Spark plus a streaming architecture can solve these problems very nicely. Ted Dunning presents a practical architecture as well as some detailed algorithms for detecting anomalies in event streams. These algorithms are simple and quite general and can be applied across a wide variety of situations.
Holden Karau walks attendees through a number of common mistakes that can keep your Spark programs from scaling and examines solutions and general techniques useful for moving beyond a proof of concept to production, covering topics like when to use DataFrames, tuning, and working with key skew.
As Spark is used more and more frequently for production workloads with stringent security requirements, fully locking down Spark applications has become critical. Kostas Sakellis explores the various facets of securing your Spark application.
Tathagata Das explains how Spark 2.x develops the next evolution of Spark Streaming by extending DataFrames and Datasets in Spark to handle streaming data. Streaming Datasets provides a single programming abstraction for batch and streaming data and also brings support for event-time-based processing, out-of-order data, sessionization, and tight integration with nonstreaming data sources.
Reactive Streams is an API designed to connect reactive systems with back-pressure. Luc Bourlier explains why, with Spark Streaming now supporting back-pressure, Reactive Streams is the right tool to connect Spark Streaming in a reactive application.
Ted Malaska leads an introduction to basic Spark concepts such as DAGs, RDDs, transformations, actions, and executors, designed for Java and Scala developers. You'll learn how your mindset must evolve beyond Java or Scala code that runs in a single JVM as you explore JVM locality, memory utilization, network/CPU usage, optimization of DAGs pipelines, and serialization conservation.
Heron has been in production at Twitter for nearly two years and is widely used by several teams for diverse use cases. Karthik Ramasamy describes Heron in detail, covering a few use cases in-depth and sharing the operating experiences and challenges of running Heron at scale.
Spark has been growing in deployments for the past year. The increasing amount of data being analyzed and processed through the framework is massive and continues to push the boundaries of the engine. Drawing on his experiences across 150+ production deployments, Neelesh Srinivas Salian explores common issues observed in a cluster environment setup with Apache Spark.
Ian Wrigley leads a hands-on workshop on leveraging the capabilities of Apache Kafka to collect, manage, and process stream data for both big data projects and general-purpose enterprise data integration, covering key architectural concepts, developer APIs, use cases, and how to write applications that publish data to, and subscribe to data from, Kafka. No prior knowledge of Kafka is required.
We as an industry are collecting more data every year. IoT, web, and mobile applications send torrents of bits to our data centers that have to be processed and stored, even as users expect an always-on experienceleaving little room for error. Patrick McFadin explores how successful companies do this every day using the powerful Team Apache: Apache Kafka, Spark, and Cassandra.
H2O is an in-memory, big-data, big-math machine-learning platform. Cliff Click offers a technical talk focused on the insides of H2O. Cliff explains how you can write simple, single-threaded Java code and have H2O autoparallelize and auto-scale-out to hundreds of nodes and thousands of cores.
TensorFlow is an open source software library for numerical computation with a focus on machine learning. Its flexible architecture makes it great for research and production deployment. Sherry Moore offers a high-level introduction to TensorFlow and explains how to use it to train machine-learning models to make your next application smarter.
Hear why big SQL is the future of analytics. Experts at Yahoo, Knewton, FullStack Analytics, and Looker discuss their respective data architectures, the trials and tribulations of running analytics in-cluster, and examples of the real business value gained from putting their data in the hands of employees across their companies.
In pursuit of speed and efficiency, big data processing is continuing its logical evolution toward columnar execution. Julien Le Dem offers a glimpse into the future of column-oriented data processing with Arrow and Parquet.
Watermarks are a system for measuring progress and completeness in out-of-order stream processing systems and are used to emit correct results in a timely way. Given the trend toward out-of-order processing in current streaming systems, understanding watermarks is an increasingly important skill. Slava Chernyak explains watermarks and demonstrates how to apply them using real-world cases.
Application messaging isnt new. Solutions like message queues have been around for a long time, but newer solutions like Kafka have emerged as high-performance, high-scalability alternatives that integrate well with Hadoop. Should distributed messaging systems like Kafka be considered replacements for legacy technologies? Jim Scott answers that question by delving into architectural trade-offs.
Tyler Akidau offers a whirlwind tour of the conceptual building blocks of massive-scale data processing systems over the last decade, comparing and contrasting systems at Google with popular open source systems in use today.
Xavier Laut shares his experience and relates the challenges scaling Metamarkets's real-time processing to over 3 million events per second. Built entirely on open source, the stack performs streaming joins using Kafka and Samza and feeds into Druid, serving 1 million interactive queries per day.
Neeraja Rentachintala discusses the latest integrations between Apache Drill and Spark technologies. Together, the combination allows Spark users to leverage Drills flexible schema and dynamic schema discovery capabilities to query and work with complex data directly using familiar Spark programming paradigms.
Modern data is often messy and does not fit into the old schema-on-write or even the newer schema-on-read paradigms. Some data effectively has no schema at all. Tomer Shiran explores how to analyze such data with Drill, covering Drills internal architecture and explaining how type introspection can be used to query JSON and JSON-structured datasuch as data in MongoDBwithout requiring a schema.
Hardware accelerated solutions are ready to meet challenges in data collection, exploration, and visualization. Simply stated, data analytics and high-performance computing evolution must go hand in hand. Kanu Gulati provides an overview of the advances in hardware acceleration and discusses specific real-world use cases of HPC applications that are enabling innovation in analytics.
Ian Wrigley leads a hands-on workshop on leveraging the capabilities of Apache Kafka to collect, manage, and process stream data for both big data projects and general-purpose enterprise data integration, covering key architectural concepts, developer APIs, use cases, and how to write applications that publish data to, and subscribe to data from, Kafka. No prior knowledge of Kafka is required.
We as an industry are collecting more data every year. IoT, web, and mobile applications send torrents of bits to our data centers that have to be processed and stored, even as users expect an always-on experienceleaving little room for error. Patrick McFadin explores how successful companies do this every day using the powerful Team Apache: Apache Kafka, Spark, and Cassandra.
H2O is an in-memory, big-data, big-math machine-learning platform. Cliff Click offers a technical talk focused on the insides of H2O. Cliff explains how you can write simple, single-threaded Java code and have H2O autoparallelize and auto-scale-out to hundreds of nodes and thousands of cores.
TensorFlow is an open source software library for numerical computation with a focus on machine learning. Its flexible architecture makes it great for research and production deployment. Sherry Moore offers a high-level introduction to TensorFlow and explains how to use it to train machine-learning models to make your next application smarter.
Hear why big SQL is the future of analytics. Experts at Yahoo, Knewton, FullStack Analytics, and Looker discuss their respective data architectures, the trials and tribulations of running analytics in-cluster, and examples of the real business value gained from putting their data in the hands of employees across their companies.
In pursuit of speed and efficiency, big data processing is continuing its logical evolution toward columnar execution. Julien Le Dem offers a glimpse into the future of column-oriented data processing with Arrow and Parquet.
Watermarks are a system for measuring progress and completeness in out-of-order stream processing systems and are used to emit correct results in a timely way. Given the trend toward out-of-order processing in current streaming systems, understanding watermarks is an increasingly important skill. Slava Chernyak explains watermarks and demonstrates how to apply them using real-world cases.
Application messaging isnt new. Solutions like message queues have been around for a long time, but newer solutions like Kafka have emerged as high-performance, high-scalability alternatives that integrate well with Hadoop. Should distributed messaging systems like Kafka be considered replacements for legacy technologies? Jim Scott answers that question by delving into architectural trade-offs.
Tyler Akidau offers a whirlwind tour of the conceptual building blocks of massive-scale data processing systems over the last decade, comparing and contrasting systems at Google with popular open source systems in use today.
Xavier Laut shares his experience and relates the challenges scaling Metamarkets's real-time processing to over 3 million events per second. Built entirely on open source, the stack performs streaming joins using Kafka and Samza and feeds into Druid, serving 1 million interactive queries per day.
Neeraja Rentachintala discusses the latest integrations between Apache Drill and Spark technologies. Together, the combination allows Spark users to leverage Drills flexible schema and dynamic schema discovery capabilities to query and work with complex data directly using familiar Spark programming paradigms.
Modern data is often messy and does not fit into the old schema-on-write or even the newer schema-on-read paradigms. Some data effectively has no schema at all. Tomer Shiran explores how to analyze such data with Drill, covering Drills internal architecture and explaining how type introspection can be used to query JSON and JSON-structured datasuch as data in MongoDBwithout requiring a schema.
Hardware accelerated solutions are ready to meet challenges in data collection, exploration, and visualization. Simply stated, data analytics and high-performance computing evolution must go hand in hand. Kanu Gulati provides an overview of the advances in hardware acceleration and discusses specific real-world use cases of HPC applications that are enabling innovation in analytics.
Todd Lipcon investigates the trade-offs between real-time transactional access and fast analytic performance from the perspective of storage engine internals and offers an overview of Kudu, the new addition to the open source Hadoop ecosystem that fills the gap described above, complementing HDFS and HBase to provide a new option to achieve fast scans and fast random access from a single API.
Ruhollah Farchtchi explores best practices for building systems that support ad hoc queries over real-time data and offers an overview of Kudu, a new storage layer for Hadoop that is specifically designed for use cases that require fast analytics on rapidly changing data with a simultaneous combination of sequential and random reads and writes.
Which venues have similar visiting patterns? How can we detect when a user is on vacation? Can we predict which venues will be favorited by users by examining their friends' preferences? Natalino Busa explains how these predictive analytics tasks can be accomplished by using Spark SQL, Spark ML, and just a few lines of Scala code.
Carl Steinbach offers an overview of Dali, LinkedIn's collection of libraries, services, and development tools that are united by the common goal of providing a dataset API for Hadoop.
Jennifer Wu outlines concepts for successfully running Hadoop in the cloud, provides guidance on selecting cloud storage, covers real-world examples of Hadoop deployment patterns in public clouds, and demos Cloudera Director provisioning on AWS.
Developers of big data applications face a unique challenge testing their software against a diverse ecosystem of data platforms that can be complex and resource intensive to deploy. Chad Metcalf and Seshadri Mahalingam explain why Docker offers a simpler model for systems by encapsulating complex dependencies and making deployment onto servers dynamic and lightweight.
Hadoop is supremely flexible, but with that flexibility comes integration challenges. Alex Leblang introduces RecordService, a new service that eliminates the need for components to support individual file formats, handle security, perform auditing, and implement sophisticated I/O scheduling and other common processing at the bottom of any computation.
How do you connect a Hadoop cluster to an enterprise directory with 100,000+ users and centralized role and access management? Hellmar Becker and Frank Albers present ING's approach to aligning Hadoop authentication and role management with INGs policies and architecture, discuss challenges they met on the way, and outline the solutions they found.
The global cyberthreat landscape is a constantly evolving environment. Oliver Newbury outlines BTs cybersecurity strategy and offers an overview of BT Assure Cyberthe big data solution it has built to protect its own data and help others protect themselveswhich sorts through masses of data, stitches together subtle clues, and produces useful, actionable information for cybersecurity.
As companies seek to expand their global data footprint, many new challenges arise. How can data be shared across countries? How can a company go about managing all of the policies and regulations specific to each country that customers reside in? Clara Fletcher explores best practices and lessons learned in international data management and security.
Currently, multitenancy in Hadoop is limited to organizations running separate Hadoop clusters, and the secure sharing of resources is achieved using virtualization or containers. Jim Dowling describes how HopsWorks enables organizations to securely share a single Hadoop cluster using projects and a new metadata layer that enables protection domains while still allowing data sharing.
Todd Lipcon investigates the trade-offs between real-time transactional access and fast analytic performance from the perspective of storage engine internals and offers an overview of Kudu, the new addition to the open source Hadoop ecosystem that fills the gap described above, complementing HDFS and HBase to provide a new option to achieve fast scans and fast random access from a single API.
Ruhollah Farchtchi explores best practices for building systems that support ad hoc queries over real-time data and offers an overview of Kudu, a new storage layer for Hadoop that is specifically designed for use cases that require fast analytics on rapidly changing data with a simultaneous combination of sequential and random reads and writes.
Which venues have similar visiting patterns? How can we detect when a user is on vacation? Can we predict which venues will be favorited by users by examining their friends' preferences? Natalino Busa explains how these predictive analytics tasks can be accomplished by using Spark SQL, Spark ML, and just a few lines of Scala code.
Carl Steinbach offers an overview of Dali, LinkedIn's collection of libraries, services, and development tools that are united by the common goal of providing a dataset API for Hadoop.
Jennifer Wu outlines concepts for successfully running Hadoop in the cloud, provides guidance on selecting cloud storage, covers real-world examples of Hadoop deployment patterns in public clouds, and demos Cloudera Director provisioning on AWS.
Developers of big data applications face a unique challenge testing their software against a diverse ecosystem of data platforms that can be complex and resource intensive to deploy. Chad Metcalf and Seshadri Mahalingam explain why Docker offers a simpler model for systems by encapsulating complex dependencies and making deployment onto servers dynamic and lightweight.
Hadoop is supremely flexible, but with that flexibility comes integration challenges. Alex Leblang introduces RecordService, a new service that eliminates the need for components to support individual file formats, handle security, perform auditing, and implement sophisticated I/O scheduling and other common processing at the bottom of any computation.
How do you connect a Hadoop cluster to an enterprise directory with 100,000+ users and centralized role and access management? Hellmar Becker and Frank Albers present ING's approach to aligning Hadoop authentication and role management with INGs policies and architecture, discuss challenges they met on the way, and outline the solutions they found.
The global cyberthreat landscape is a constantly evolving environment. Oliver Newbury outlines BTs cybersecurity strategy and offers an overview of BT Assure Cyberthe big data solution it has built to protect its own data and help others protect themselveswhich sorts through masses of data, stitches together subtle clues, and produces useful, actionable information for cybersecurity.
As companies seek to expand their global data footprint, many new challenges arise. How can data be shared across countries? How can a company go about managing all of the policies and regulations specific to each country that customers reside in? Clara Fletcher explores best practices and lessons learned in international data management and security.
Currently, multitenancy in Hadoop is limited to organizations running separate Hadoop clusters, and the secure sharing of resources is achieved using virtualization or containers. Jim Dowling describes how HopsWorks enables organizations to securely share a single Hadoop cluster using projects and a new metadata layer that enables protection domains while still allowing data sharing.
Visualizations are a key part of conveying any dataset. Brian Suda explains what good data visualizations are and how you can build them using D3, the most popular, easiest, and most extensible way to get your data online in an interactive way.
Visualizations are a key part of conveying any dataset. Brian Suda explains what good data visualizations are and how you can build them using D3, the most popular, easiest, and most extensible way to get your data online in an interactive way.
OReilly Media and DataStax have partnered to create a 2-day developer course for Apache Cassandra. Get trained as a Cassandra developer at Strata + Hadoop World in London, be recognized for your NoSQL expertise, and benefit from the skyrocketing demand for Cassandra developers.
OReilly Media and DataStax have partnered to create a 2-day developer course for Apache Cassandra. Get trained as a Cassandra developer at Strata + Hadoop World in London, be recognized for your NoSQL expertise, and benefit from the skyrocketing demand for Cassandra developers.
OReilly Media and DataStax have partnered to create a 2-day developer course for Apache Cassandra. Get trained as a Cassandra developer at Strata + Hadoop World in London, be recognized for your NoSQL expertise, and benefit from the skyrocketing demand for Cassandra developers.
The real power and value proposition of Apache Spark is in building a unified use case that combines ETL, batch analytics, real-time stream analysis, machine learning, graph processing, and visualizations. Stephane Rion employs hands-on exercises using explore various Wikipedia datasets to illustrate the variety of ideal programming paradigms Spark makes possible.
The Data Lab is an innovation center that delivers social and economic benefit to Scotland by bringing industry, the public sector, and academia together to exploit new opportunities from data. Brian Hills shares insights and lessons learned during the center's first 18 months, organized into three themes: collaborative innovation, nurturing skills and talent, and community building.
Mads Hjorth offers a glimpse of a world-class digital public administration, showcasing how data has transformed the Danish public administration and its services toward citizens and businesses, and issues a call for cross-border collaboration to effectively address central challenges using modern data technologies.
Being good is hard. Being evil is fun and gets you paid more. Once more Duncan Ross and Francine Bennett explore how to do high-impact evil with data and analysis. Make the maximum (negative) impact on your friends, your business, and the worldor use this talk to avoid ethical dilemmas, develop ways to deal responsibly with data, or even do good. But that would be perverse.
With the analytic and predictive power of big data comes the responsibility to respect and protect individual privacy. As citizens, we should hold organizations to account; as data practitioners, we must find intelligent ways to analyze data without violating privacy. Jason McFall discusses privacy risks and surveys leading privacy-preserving analysis techniques.
Who does your computer think I am? Today, every person is digitally represented in a multitude of IT systems, based on invisible algorithms that pervasively control pieces of our lives through decisions made based on our preferences, interests, and even future actions. Joerg Blumtritt and Majken Sander explore these judgments, discuss their consequences, and present possible solutions.
Mark Donsky and Chang She explore canonical case studies that demonstrate how leading banks, healthcare, and pharmaceutical organizations are tackling Hadoop governance challenges head-on. You'll learn how to ensure data doesn't get lost, help users find and trust the data they need, and protect yourself against a data breachall at Hadoop scale.
Anirudh Koul and Saqib Shaik explore cutting-edge advances at the intersection of vision, language, and deep learning that help the blind community "see" the physical world and explain how developers can utilize this state-of-the-art image-captioning and computer-vision technology in their own applications.
Daniele Quercia discusses the launch of Goodcitylife.orga global group of like-minded people who are passionate about building technologies whose focus is not necessarily to create a smart city but to give a good life to city dwellers.
With the rise of deep learning, natural language understanding techniques are becoming more effective and are not as reliant on costly annotated data. This leads to an explosion of possibilities of what businesses can do with language. Alyona Medelyan explains what the newest NLU tools can achieve today and presents their common use cases.
Efficient, accurate, and robust ETL (extract, transform, load) pipelines are essential components for building successful data products. Johannes Bauer discusses the fundamental requirements for ETL pipelines, highlighting major guiding principles as well as challenges and outlining selected elements of ETL pipeline implementations using advanced elements of Scala.
GPU-based databases, visualization layers, and analytic platforms have an immense advantage over their CPU-bound counterparts. Todd Mostak explains how data scientists and analysts can execute and visualize SQL queries on billions of rows of data in millisecondsup to 1,000x faster than legacy CPU systemsby leveraging the parallel processing power and memory bandwidth of GPUs.
The real power and value proposition of Apache Spark is in building a unified use case that combines ETL, batch analytics, real-time stream analysis, machine learning, graph processing, and visualizations. Stephane Rion employs hands-on exercises using explore various Wikipedia datasets to illustrate the variety of ideal programming paradigms Spark makes possible.
The Data Lab is an innovation center that delivers social and economic benefit to Scotland by bringing industry, the public sector, and academia together to exploit new opportunities from data. Brian Hills shares insights and lessons learned during the center's first 18 months, organized into three themes: collaborative innovation, nurturing skills and talent, and community building.
Mads Hjorth offers a glimpse of a world-class digital public administration, showcasing how data has transformed the Danish public administration and its services toward citizens and businesses, and issues a call for cross-border collaboration to effectively address central challenges using modern data technologies.
Being good is hard. Being evil is fun and gets you paid more. Once more Duncan Ross and Francine Bennett explore how to do high-impact evil with data and analysis. Make the maximum (negative) impact on your friends, your business, and the worldor use this talk to avoid ethical dilemmas, develop ways to deal responsibly with data, or even do good. But that would be perverse.
With the analytic and predictive power of big data comes the responsibility to respect and protect individual privacy. As citizens, we should hold organizations to account; as data practitioners, we must find intelligent ways to analyze data without violating privacy. Jason McFall discusses privacy risks and surveys leading privacy-preserving analysis techniques.
Who does your computer think I am? Today, every person is digitally represented in a multitude of IT systems, based on invisible algorithms that pervasively control pieces of our lives through decisions made based on our preferences, interests, and even future actions. Joerg Blumtritt and Majken Sander explore these judgments, discuss their consequences, and present possible solutions.
Mark Donsky and Chang She explore canonical case studies that demonstrate how leading banks, healthcare, and pharmaceutical organizations are tackling Hadoop governance challenges head-on. You'll learn how to ensure data doesn't get lost, help users find and trust the data they need, and protect yourself against a data breachall at Hadoop scale.
Anirudh Koul and Saqib Shaik explore cutting-edge advances at the intersection of vision, language, and deep learning that help the blind community "see" the physical world and explain how developers can utilize this state-of-the-art image-captioning and computer-vision technology in their own applications.
Daniele Quercia discusses the launch of Goodcitylife.orga global group of like-minded people who are passionate about building technologies whose focus is not necessarily to create a smart city but to give a good life to city dwellers.
With the rise of deep learning, natural language understanding techniques are becoming more effective and are not as reliant on costly annotated data. This leads to an explosion of possibilities of what businesses can do with language. Alyona Medelyan explains what the newest NLU tools can achieve today and presents their common use cases.
Efficient, accurate, and robust ETL (extract, transform, load) pipelines are essential components for building successful data products. Johannes Bauer discusses the fundamental requirements for ETL pipelines, highlighting major guiding principles as well as challenges and outlining selected elements of ETL pipeline implementations using advanced elements of Scala.
GPU-based databases, visualization layers, and analytic platforms have an immense advantage over their CPU-bound counterparts. Todd Mostak explains how data scientists and analysts can execute and visualize SQL queries on billions of rows of data in millisecondsup to 1,000x faster than legacy CPU systemsby leveraging the parallel processing power and memory bandwidth of GPUs.
The real power and value proposition of Apache Spark is in building a unified use case that combines ETL, batch analytics, real-time stream analysis, machine learning, graph processing, and visualizations. Stephane Rion employs hands-on exercises using explore various Wikipedia datasets to illustrate the variety of ideal programming paradigms Spark makes possible.
The Data Lab is an innovation center that delivers social and economic benefit to Scotland by bringing industry, the public sector, and academia together to exploit new opportunities from data. Brian Hills shares insights and lessons learned during the center's first 18 months, organized into three themes: collaborative innovation, nurturing skills and talent, and community building.
Mads Hjorth offers a glimpse of a world-class digital public administration, showcasing how data has transformed the Danish public administration and its services toward citizens and businesses, and issues a call for cross-border collaboration to effectively address central challenges using modern data technologies.
Being good is hard. Being evil is fun and gets you paid more. Once more Duncan Ross and Francine Bennett explore how to do high-impact evil with data and analysis. Make the maximum (negative) impact on your friends, your business, and the worldor use this talk to avoid ethical dilemmas, develop ways to deal responsibly with data, or even do good. But that would be perverse.
With the analytic and predictive power of big data comes the responsibility to respect and protect individual privacy. As citizens, we should hold organizations to account; as data practitioners, we must find intelligent ways to analyze data without violating privacy. Jason McFall discusses privacy risks and surveys leading privacy-preserving analysis techniques.
Who does your computer think I am? Today, every person is digitally represented in a multitude of IT systems, based on invisible algorithms that pervasively control pieces of our lives through decisions made based on our preferences, interests, and even future actions. Joerg Blumtritt and Majken Sander explore these judgments, discuss their consequences, and present possible solutions.
Mark Donsky and Chang She explore canonical case studies that demonstrate how leading banks, healthcare, and pharmaceutical organizations are tackling Hadoop governance challenges head-on. You'll learn how to ensure data doesn't get lost, help users find and trust the data they need, and protect yourself against a data breachall at Hadoop scale.
Anirudh Koul and Saqib Shaik explore cutting-edge advances at the intersection of vision, language, and deep learning that help the blind community "see" the physical world and explain how developers can utilize this state-of-the-art image-captioning and computer-vision technology in their own applications.
Daniele Quercia discusses the launch of Goodcitylife.orga global group of like-minded people who are passionate about building technologies whose focus is not necessarily to create a smart city but to give a good life to city dwellers.
With the rise of deep learning, natural language understanding techniques are becoming more effective and are not as reliant on costly annotated data. This leads to an explosion of possibilities of what businesses can do with language. Alyona Medelyan explains what the newest NLU tools can achieve today and presents their common use cases.
Efficient, accurate, and robust ETL (extract, transform, load) pipelines are essential components for building successful data products. Johannes Bauer discusses the fundamental requirements for ETL pipelines, highlighting major guiding principles as well as challenges and outlining selected elements of ETL pipeline implementations using advanced elements of Scala.
GPU-based databases, visualization layers, and analytic platforms have an immense advantage over their CPU-bound counterparts. Todd Mostak explains how data scientists and analysts can execute and visualize SQL queries on billions of rows of data in millisecondsup to 1,000x faster than legacy CPU systemsby leveraging the parallel processing power and memory bandwidth of GPUs.
Mark Grover, Jonathan Seidman, Ted Malaska, and Gwen Shapira, the authors of Hadoop Application Architectures, participate in an open Q&A session on considerations and recommendations for the architecture and design of applications using Hadoop. Come with questions about your use case and its big data architecture or just listen in on the conversation.
Mark Samson, Jayesh Seshadri, Wellington Chevreuil, and James Kinley, the instructors of the the full-day tutorial Apache Hadoop Operations for Production Systems, field a wide range of detailed questions about Hadoop, from debugging and tuning across different layers to tools and subsystems to keep your Hadoop clusters always up, running, and secure.
Ian Wrigley, Neha Narkhede, and Flavio Junqueira field a wide range of detailed questions about Apache Kafka. Even if you dont have a specific question, join in to hear what others are asking.
Apache Beam/Google Cloud Dataflow engineers Tyler Akidau, Kenneth Knowles, and Slava Chernyak will be on hand to answer a wide range of detailed questions about stream processing. Even if you dont have a specific question, join in to hear what others are asking.
Mark Grover, Jonathan Seidman, Ted Malaska, and Gwen Shapira, the authors of Hadoop Application Architectures, participate in an open Q&A session on considerations and recommendations for the architecture and design of applications using Hadoop. Come with questions about your use case and its big data architecture or just listen in on the conversation.
Mark Samson, Jayesh Seshadri, Wellington Chevreuil, and James Kinley, the instructors of the the full-day tutorial Apache Hadoop Operations for Production Systems, field a wide range of detailed questions about Hadoop, from debugging and tuning across different layers to tools and subsystems to keep your Hadoop clusters always up, running, and secure.
Ian Wrigley, Neha Narkhede, and Flavio Junqueira field a wide range of detailed questions about Apache Kafka. Even if you dont have a specific question, join in to hear what others are asking.
Apache Beam/Google Cloud Dataflow engineers Tyler Akidau, Kenneth Knowles, and Slava Chernyak will be on hand to answer a wide range of detailed questions about stream processing. Even if you dont have a specific question, join in to hear what others are asking.
The IoT combined with big data analytics enables organizations to track new patterns and signs and bring data together that previously was not only a challenge to integrate but also way too expensive. Frank Saeuberlich and Eliano Marques explain why data management, data integration, and multigenre analytics are foundational to driving business value from IoT initiatives.
As we strive to realize big data's value, many seek more agile and capable analytic systems that ensure end-to-end security. Chris Selland and Richard Gascoigne explore Hewlett Packard Enterprise's robust yet flexible offering that scales with evolving needs, covering HPE's big data reference architecture, Vertica SQL on Hadoop, and machine learning as a service.
Raghunath Nambiar reviews the big data landscape, reflects on big data lessons learned in enterprise over the last few years, and explores how these organizations avoid their big data environments becoming unmanageable by using simplex management for deployment, administration, monitoring, and reporting no matter how much the environment scales.
Louise Matthews covers industry trends and transformative business use cases drawn from a wide range of market sectors across Europe to bring the future of data to life.
Liberty Global are the largest international cable company in the World.  Roberto will take you on their journey in to BI and Big Data from proof of concept which lead them to an Oracle Big Data Appliance solution.
Data sizes are getting larger all the time. Querying terabytes just isn't cool anymore; now you need to query petabytes. Jordan Tigani puts BigQuery to the test by performing interactive analytics against a 1 PB dataset, showcasing the exciting new features that make this process easy, fast, and affordable, and demonstrates the simplicity of managing your petabyte-scale data with "NoOps".
The Internet of Things and big data analytics are currently two of the hottest topics in IT. But how do you get started using them? Emil Andreas Siemes and Stephan Ann demonstrate how to use Apache NiFi to ingest, transform, and route sensor data into Hadoop and how to do further predictive analytics.
There are (too?) many options for BI on Hadoop. Some are great at exploration, some are great at OLAP, some are fast, and some are flexible. Understanding the options and how they work with Hadoop systems is a key challenge for many organizations. Tomer Shiran provides a survey of the main options, both traditional (Tableau, Qlik, etc.) and new (Platfora, Datameer, etc.).
Apache Spark is on fire. Over the past five years, more and more organizations have looked to leverage Spark to operationalize their teams and the delivery of analytics to their respective businesses. Adrian Houselander and Joy Spohn demonstrate two use cases of how Apache Spark and Apache Hadoop are being used to harness valuable insights from complex data across cloud and hybrid environments.
The metaphors used online have always borrowed heavily from the offline world, but as our online and offline worlds converge, the biggest opportunities for innovative experiences will come from blending them intentionally. Kate ONeill examines how the meaning and understanding of place relates to identity, culture, and intent and how we can shape our audiences' experiences more meaningfully.
The IoT combined with big data analytics enables organizations to track new patterns and signs and bring data together that previously was not only a challenge to integrate but also way too expensive. Frank Saeuberlich and Eliano Marques explain why data management, data integration, and multigenre analytics are foundational to driving business value from IoT initiatives.
As we strive to realize big data's value, many seek more agile and capable analytic systems that ensure end-to-end security. Chris Selland and Richard Gascoigne explore Hewlett Packard Enterprise's robust yet flexible offering that scales with evolving needs, covering HPE's big data reference architecture, Vertica SQL on Hadoop, and machine learning as a service.
Raghunath Nambiar reviews the big data landscape, reflects on big data lessons learned in enterprise over the last few years, and explores how these organizations avoid their big data environments becoming unmanageable by using simplex management for deployment, administration, monitoring, and reporting no matter how much the environment scales.
Louise Matthews covers industry trends and transformative business use cases drawn from a wide range of market sectors across Europe to bring the future of data to life.
Liberty Global are the largest international cable company in the World.  Roberto will take you on their journey in to BI and Big Data from proof of concept which lead them to an Oracle Big Data Appliance solution.
Data sizes are getting larger all the time. Querying terabytes just isn't cool anymore; now you need to query petabytes. Jordan Tigani puts BigQuery to the test by performing interactive analytics against a 1 PB dataset, showcasing the exciting new features that make this process easy, fast, and affordable, and demonstrates the simplicity of managing your petabyte-scale data with "NoOps".
The Internet of Things and big data analytics are currently two of the hottest topics in IT. But how do you get started using them? Emil Andreas Siemes and Stephan Ann demonstrate how to use Apache NiFi to ingest, transform, and route sensor data into Hadoop and how to do further predictive analytics.
There are (too?) many options for BI on Hadoop. Some are great at exploration, some are great at OLAP, some are fast, and some are flexible. Understanding the options and how they work with Hadoop systems is a key challenge for many organizations. Tomer Shiran provides a survey of the main options, both traditional (Tableau, Qlik, etc.) and new (Platfora, Datameer, etc.).
Apache Spark is on fire. Over the past five years, more and more organizations have looked to leverage Spark to operationalize their teams and the delivery of analytics to their respective businesses. Adrian Houselander and Joy Spohn demonstrate two use cases of how Apache Spark and Apache Hadoop are being used to harness valuable insights from complex data across cloud and hybrid environments.
The metaphors used online have always borrowed heavily from the offline world, but as our online and offline worlds converge, the biggest opportunities for innovative experiences will come from blending them intentionally. Kate ONeill examines how the meaning and understanding of place relates to identity, culture, and intent and how we can shape our audiences' experiences more meaningfully.
With fewer than 500 North Atlantic right whales left in the world's oceans, knowing the health and status of each whale is integral to the efforts of researchers working to protect the species from extinction. To interest the data science community, NOAA Fisheries organized a competition hosted on Kaggle.com. Robert Bogucki and Maciej Klimek outline the winning solution.
There are many factors to consider when building your data stack, but the architecture could be your biggest challenge. Yet it could also be the best predictor for success. Given the many elements to take into account and the lack of a proven playbook, Ben Sharma explains where you start to assemble your own best practices for building a scalable data architecture.
Chris Kammermann explores how big data is guiding the future of one of the worlds most popular apps, Shazam, focusing on how the company is empowering employees to have fun with big data through tools like Splunkin turn helping to create new products and revenue streams, as well as the term datafuncation."
Many businesses have undertaken big data projects, but for every successful project, there are dozens that have failed or stagnated. Seb Darrington explores the reasons why such projects hit obstacles, typical challenges, and how to overcome them along your own big data journey.
Data is no longer a by-product of business transactions; now, data is the business. Franz Aman explains how data lakes can put the power of big data into the hands of every business person, sharing the inside scoop on how he turned marketing into a new kind of revenue-generation machine and interviewing an Informatica customer about how data lakes have innovated and transformed their business.
Moty Fania shares Intels IT experience implementing an on-premises IoT platform for internal use cases. The platform was based on open source big data technologies and containers and was designed as a multitenant platform with built-in analytical capabilities. Moty highlights the key lessons learned from this journey and offers a thorough review of the platforms architecture.
Learn how Spark and Hadoop enable data scientists to help companies reduce costs, increase profits, improve products, retain customers, and identify new opportunities. Through in-class simulations and exercises, Kai Voigt walks attendees through applying data science methods to real-world challenges in different industries, offering preparation for data scientist roles in the field.
With fewer than 500 North Atlantic right whales left in the world's oceans, knowing the health and status of each whale is integral to the efforts of researchers working to protect the species from extinction. To interest the data science community, NOAA Fisheries organized a competition hosted on Kaggle.com. Robert Bogucki and Maciej Klimek outline the winning solution.
There are many factors to consider when building your data stack, but the architecture could be your biggest challenge. Yet it could also be the best predictor for success. Given the many elements to take into account and the lack of a proven playbook, Ben Sharma explains where you start to assemble your own best practices for building a scalable data architecture.
Chris Kammermann explores how big data is guiding the future of one of the worlds most popular apps, Shazam, focusing on how the company is empowering employees to have fun with big data through tools like Splunkin turn helping to create new products and revenue streams, as well as the term datafuncation."
Many businesses have undertaken big data projects, but for every successful project, there are dozens that have failed or stagnated. Seb Darrington explores the reasons why such projects hit obstacles, typical challenges, and how to overcome them along your own big data journey.
Data is no longer a by-product of business transactions; now, data is the business. Franz Aman explains how data lakes can put the power of big data into the hands of every business person, sharing the inside scoop on how he turned marketing into a new kind of revenue-generation machine and interviewing an Informatica customer about how data lakes have innovated and transformed their business.
Moty Fania shares Intels IT experience implementing an on-premises IoT platform for internal use cases. The platform was based on open source big data technologies and containers and was designed as a multitenant platform with built-in analytical capabilities. Moty highlights the key lessons learned from this journey and offers a thorough review of the platforms architecture.
Learn how Spark and Hadoop enable data scientists to help companies reduce costs, increase profits, improve products, retain customers, and identify new opportunities. Through in-class simulations and exercises, Kai Voigt walks attendees through applying data science methods to real-world challenges in different industries, offering preparation for data scientist roles in the field.
With fewer than 500 North Atlantic right whales left in the world's oceans, knowing the health and status of each whale is integral to the efforts of researchers working to protect the species from extinction. To interest the data science community, NOAA Fisheries organized a competition hosted on Kaggle.com. Robert Bogucki and Maciej Klimek outline the winning solution.
There are many factors to consider when building your data stack, but the architecture could be your biggest challenge. Yet it could also be the best predictor for success. Given the many elements to take into account and the lack of a proven playbook, Ben Sharma explains where you start to assemble your own best practices for building a scalable data architecture.
Chris Kammermann explores how big data is guiding the future of one of the worlds most popular apps, Shazam, focusing on how the company is empowering employees to have fun with big data through tools like Splunkin turn helping to create new products and revenue streams, as well as the term datafuncation."
Many businesses have undertaken big data projects, but for every successful project, there are dozens that have failed or stagnated. Seb Darrington explores the reasons why such projects hit obstacles, typical challenges, and how to overcome them along your own big data journey.
Data is no longer a by-product of business transactions; now, data is the business. Franz Aman explains how data lakes can put the power of big data into the hands of every business person, sharing the inside scoop on how he turned marketing into a new kind of revenue-generation machine and interviewing an Informatica customer about how data lakes have innovated and transformed their business.
Moty Fania shares Intels IT experience implementing an on-premises IoT platform for internal use cases. The platform was based on open source big data technologies and containers and was designed as a multitenant platform with built-in analytical capabilities. Moty highlights the key lessons learned from this journey and offers a thorough review of the platforms architecture.
Learn how Spark and Hadoop enable data scientists to help companies reduce costs, increase profits, improve products, retain customers, and identify new opportunities. Through in-class simulations and exercises, Kai Voigt walks attendees through applying data science methods to real-world challenges in different industries, offering preparation for data scientist roles in the field.
Federated analytics, a new approach to analyzing big data, balances privacy, autonomy, IP protection and supports unprecedented collaboration across large distributed datasets that contain proprietary and/or protected information. Gilad Olswang explains how Intel harnesses the power of federated analytics in the Collaborative Cancer Cloud project.
Turning big data into tangible business value can be a struggle even for highly skilled data scientists. Zoltan Prekopcsak outlines the best practices that make life easier, simplify the process, and implement results faster.
Stream-based technologies allow big data applications to deal with low-latency decisions and provide a more agile way to develop and deploy applications. Tugdual Grall details the various elements of a stream-based application and outlines the key capabilities of modern messaging layers like Apache Kafka and MapR Streams.
Joe Goldberg discusses the attributes required of a batch management platform that can accelerate development by enabling programmers to generate workflows as code, support continuous deployment with rich APIs and lightweight workflow scheduling infrastructure, and optimize production with comprehensive enterprise operational capabilities like SLA management and full log and output management.
Big data analytics brings value to enterprises, helping them achieve operational excellence. The big question is how you implement it. Drawing on firsthand experience, Assaf Araki and Itay Yogev share how Intel built a big data analytics competency center, exploring the key elements that help Intel grow its people and capabilities and the challenges and lessons learned.
Federated analytics, a new approach to analyzing big data, balances privacy, autonomy, IP protection and supports unprecedented collaboration across large distributed datasets that contain proprietary and/or protected information. Gilad Olswang explains how Intel harnesses the power of federated analytics in the Collaborative Cancer Cloud project.
Turning big data into tangible business value can be a struggle even for highly skilled data scientists. Zoltan Prekopcsak outlines the best practices that make life easier, simplify the process, and implement results faster.
Stream-based technologies allow big data applications to deal with low-latency decisions and provide a more agile way to develop and deploy applications. Tugdual Grall details the various elements of a stream-based application and outlines the key capabilities of modern messaging layers like Apache Kafka and MapR Streams.
Joe Goldberg discusses the attributes required of a batch management platform that can accelerate development by enabling programmers to generate workflows as code, support continuous deployment with rich APIs and lightweight workflow scheduling infrastructure, and optimize production with comprehensive enterprise operational capabilities like SLA management and full log and output management.
Big data analytics brings value to enterprises, helping them achieve operational excellence. The big question is how you implement it. Drawing on firsthand experience, Assaf Araki and Itay Yogev share how Intel built a big data analytics competency center, exploring the key elements that help Intel grow its people and capabilities and the challenges and lessons learned.
Big data and data science have great potential to accelerate business, but how do you reconcile the opportunity with the sea of possible technologies? Conventional data strategy offers little to guide us, focusing more on governance than on creating new value. Scott Kurth and John Akred explain how to create a modern data strategy that powers data-driven business.
In a hands-on tutorial designed for executives, product managers, and business leaders, Marc Warner explores what's possible (and not) with machine learning and what that means for businesses. Attendees will gain experience with cutting-edge artificial intelligence by building their very own handwriting recognition engine. No technical background required.
Big data and data science have great potential to accelerate business, but how do you reconcile the opportunity with the sea of possible technologies? Conventional data strategy offers little to guide us, focusing more on governance than on creating new value. Scott Kurth and John Akred explain how to create a modern data strategy that powers data-driven business.
In a hands-on tutorial designed for executives, product managers, and business leaders, Marc Warner explores what's possible (and not) with machine learning and what that means for businesses. Attendees will gain experience with cutting-edge artificial intelligence by building their very own handwriting recognition engine. No technical background required.
Strata + Hadoop World program chairs Roger Magoulas, Doug Cutting, and Alistair Croll welcome you to the first day of keynotes.
Strata + Hadoop World program chairs Roger Magoulas, Doug Cutting, and Alistair Croll welcome you to the second day of keynotes.
Clouderas Mike Olson is joined by Manuel Martin Marquez, a senior research and data scientist at CERN, to discuss Hadoop's role in the modern data strategy and how CERN is using Hadoop and information discovery to help drive operational efficiency for the Large Hadron Collider.
Martin Willcox shares the lessons he's learned from successful Teradata IoT projects about about how to manage and leverage sensor data and explains why data management, data integration, and multigenre analytics are foundational to driving business value from IoT initiatives.
The traditional data warehouse of the 1990s was quaintly called the single source of truth. Joe Hellerstein explains why today we take a far more relativistic view: the meaning of data depends on the context in which it is used.
Piotr Niedwied explores how deepsense.io created the worlds best deep learning model for identifying individual right whales using aerial photography for the NOAA (National Oceanic and Atmospheric Administration) and explains what happened when the solution was covered by news media around the globe.
Data has more potential value when it can be shared. In order to monetize data, it must first be made shareable: shareable data is an asset that can be sold, traded, or used to create new data marketplaces. Mona Vernon outlines a framework to structure thinking about data shareability and monetization and explores these new business opportunities.
Federated analytics, a new approach to analyzing big data, supports unprecedented collaboration across large distributed datasets that contain proprietary and/or protected information. Gilad Olswang explains how Intel harnesses the power of federated analytics in the Collaborative Cancer Cloud project.
The news media in recent months has been full of dire warnings about the risk that AI poses to the human race. Should we be concerned? If so, what can we do about it? While some in the mainstream AI community dismiss these concerns, Stuart Russell argues that a fundamental reorientation of the field is required.
The cybersecurity landscape is quickly changing, and Apache Hadoop is becoming the analytics and data management platform of choice for cybersecurity practitioners. Tom Reilly explains why organizations are turning toward the open source ecosystem to break down traditional cybersecurity analytics and data constraints in order to detect a new breed of sophisticated attacks.
David Selby shares some of the challenges he has faced coercing meaning from data and explains why he is particularly enthusiastic about the latest technological developments in the data science field.
Stefanie Posavec recently completed a year-long drawing project with Giorgia Lupi called Dear Data, where each week they manually gathered and drew their data on a postcard to send to the other. Stefanie discusses the professional insights she gained spending a year on such an intensive personal data project.
Google is no stranger to big data, pioneering several big data technologies grown and tested internallyincluding MapReduce, BigTable, and most recently Dataflow and TensorFlow, as well as one of the most heavily used tools at Google, BigQueryand making them available to everyone. Jordan Tigani shares what big data means for Google and announces several new BigQuery features.
Cat Drew explains how the UK's Policy Lab and GDS data teams are bringing more of a data, digital, and design approach to policy making, showcasing some of the Policy Lab projects that have used ethnography and data science to create fresh insight to change the way we think about policy problems.
Megan Price demonstrates how machine-learning methods help us determine what we know, and what we don't, about the ongoing conflict in Syria. Megan then explains why these methods can be crucial to better understand patterns of violence, enabling better policy decisions, resource allocation, and ultimately, accountability and justice.
The famous Oracle at Delphi had a secret: Its prophecies were interpreted by Temple Guides, using a very early version of ethnographic research. With todays near-blind faith in the predictive power of Big Data, its time to take a lesson from the Ancient Greeks.
Strata + Hadoop World program chairs Roger Magoulas, Doug Cutting, and Alistair Croll welcome you to the first day of keynotes.
Strata + Hadoop World program chairs Roger Magoulas, Doug Cutting, and Alistair Croll welcome you to the second day of keynotes.
Clouderas Mike Olson is joined by Manuel Martin Marquez, a senior research and data scientist at CERN, to discuss Hadoop's role in the modern data strategy and how CERN is using Hadoop and information discovery to help drive operational efficiency for the Large Hadron Collider.
Martin Willcox shares the lessons he's learned from successful Teradata IoT projects about about how to manage and leverage sensor data and explains why data management, data integration, and multigenre analytics are foundational to driving business value from IoT initiatives.
The traditional data warehouse of the 1990s was quaintly called the single source of truth. Joe Hellerstein explains why today we take a far more relativistic view: the meaning of data depends on the context in which it is used.
Piotr Niedwied explores how deepsense.io created the worlds best deep learning model for identifying individual right whales using aerial photography for the NOAA (National Oceanic and Atmospheric Administration) and explains what happened when the solution was covered by news media around the globe.
Data has more potential value when it can be shared. In order to monetize data, it must first be made shareable: shareable data is an asset that can be sold, traded, or used to create new data marketplaces. Mona Vernon outlines a framework to structure thinking about data shareability and monetization and explores these new business opportunities.
Federated analytics, a new approach to analyzing big data, supports unprecedented collaboration across large distributed datasets that contain proprietary and/or protected information. Gilad Olswang explains how Intel harnesses the power of federated analytics in the Collaborative Cancer Cloud project.
The news media in recent months has been full of dire warnings about the risk that AI poses to the human race. Should we be concerned? If so, what can we do about it? While some in the mainstream AI community dismiss these concerns, Stuart Russell argues that a fundamental reorientation of the field is required.
The cybersecurity landscape is quickly changing, and Apache Hadoop is becoming the analytics and data management platform of choice for cybersecurity practitioners. Tom Reilly explains why organizations are turning toward the open source ecosystem to break down traditional cybersecurity analytics and data constraints in order to detect a new breed of sophisticated attacks.
David Selby shares some of the challenges he has faced coercing meaning from data and explains why he is particularly enthusiastic about the latest technological developments in the data science field.
Stefanie Posavec recently completed a year-long drawing project with Giorgia Lupi called Dear Data, where each week they manually gathered and drew their data on a postcard to send to the other. Stefanie discusses the professional insights she gained spending a year on such an intensive personal data project.
Google is no stranger to big data, pioneering several big data technologies grown and tested internallyincluding MapReduce, BigTable, and most recently Dataflow and TensorFlow, as well as one of the most heavily used tools at Google, BigQueryand making them available to everyone. Jordan Tigani shares what big data means for Google and announces several new BigQuery features.
Cat Drew explains how the UK's Policy Lab and GDS data teams are bringing more of a data, digital, and design approach to policy making, showcasing some of the Policy Lab projects that have used ethnography and data science to create fresh insight to change the way we think about policy problems.
Megan Price demonstrates how machine-learning methods help us determine what we know, and what we don't, about the ongoing conflict in Syria. Megan then explains why these methods can be crucial to better understand patterns of violence, enabling better policy decisions, resource allocation, and ultimately, accountability and justice.
The famous Oracle at Delphi had a secret: Its prophecies were interpreted by Temple Guides, using a very early version of ethnographic research. With todays near-blind faith in the predictive power of Big Data, its time to take a lesson from the Ancient Greeks.
Looking for a deeper understanding of how to architect real-time data processing solutions? This tutorial will provide this understanding using a real-world example of a fraud detection system. Well use this example to discuss considerations for building such a system, how youd integrate various technologies, and why those choices make sense for the use case in question.
Hadoop is emerging as the standard for big data processing and analytics. However, as usage of Hadoop clusters grow, so do the demands of managing and monitoring these systems. In this tutorial, attendees will get an overview of all phases of successfully managing Hadoop clusters, with an emphasis on production systems.
Southeast Asia provides a unique challenge to large recommender systems: how will you design one system that recommends products to millions of users, many whom are spread across several countries, with their own language and cultural preferences? Well, you don't. Instead, we will explore a hybrid system that integrates inputs from a variety of recommenders and deploys it on a distributed system.
We will introduce Apache SINGA, a flexible and scalable deep learning platform for big data analytics. SINGA is flexible to support various deep learning models, and is general to provide scalable training architecture. We will also show two applications to demonstrate how SINGA is helpful for healthcare data analytics, predicting risk-of-readmission and modeling chronic disease progression.
In this talk, we will explain how data scientists use nested data structures to increase analytic productivity. We will use two well-known relational schemas - TPC-H and Twitter - to demonstrate how to simplify data science workloads with nested schemas. Also, we will outline best practices for converting flat relational schemas into nested ones, and give examples of data science-style analysis.
Our teams main focus at PayPal is to boost customer engagement. This talk is about how we use predictive modeling to recommend products to consumers. We will talk about the technologies we use and how we deploy our models to production.
Privacy in the world of big data is often considered as a legal or regulatory function. However, there are technology solutions for analytics that can be used today to protect users' privacy and to enable applications over data that is too sensitive to share. We will illustrate the state-of-the-art in privacy-preserving machine learning, including new techniques we have developed.
Using the EPOC headset from Emotiv, I can capture the big data stream of EEG from our brains.  I will share my results on a lie detector experiment comparing brain waves when telling the truth and lying.  I have built classifiers based on the EEG data using Azure Machine Learning to predict whether a subject is telling the truth.  The effectiveness of multiple classifiers can be easily compared.
This talk is about an application of big data predictive analytics to improve the online customer experience. The application is built using big data infrastructure with Hadoop, Cassandra, and machine learning algorithms using R and Python, that predict customer intent and take actions in real time to deliver an enhanced experience. Key challenges and lessons learned are also discussed.
In this session, attendees will learn the concepts underlying graph data analytics based on MUFG's experiences. Moreover, it will cover how to analyze huge graph data with Apache Spark GraphX. Finally, it will explore what type of data tends to cause problems and how to solve them.
Many data applications are written in Python or R, but developing and deploying these applications at scale or in production is a pain point for many users. We will discuss our new efforts to bridge the gap between familiar in-memory data tools and distributed data systems. In particular, we are working to enable users to streamline interactions with Hadoop and scalable query engines like Impala.
Geospatial data is revolutionising the marketing research industry.  In this talk, Nielsen researchers will describe how such information is being used by the company to improve internal processes and to give new insights into client behaviour.  The goal is to give clients an analytic edge, as will be illustrated through key methodology and insights of recent projects.
The advent of next-generation DNA sequencing technologies is revolutionizing life sciences research by routinely generating extremely large data sets. Big data tools developed to handle large-scale internet data (like Hadoop) will help scientists effectively manage this new scale of data, and also enable addressing a host of questions that were previously out of reach.
In this session we will take a look at a practical review of what is deep learning and introduce DL4J. We'll look at how it supports deep learning in the enterprise on the JVM. Well discuss the architecture of DL4Js scale-out parallelization on Hadoop and Spark in support of modern machine learning workflows.
Looking for a deeper understanding of how to architect real-time data processing solutions? This tutorial will provide this understanding using a real-world example of a fraud detection system. Well use this example to discuss considerations for building such a system, how youd integrate various technologies, and why those choices make sense for the use case in question.
Hadoop is emerging as the standard for big data processing and analytics. However, as usage of Hadoop clusters grow, so do the demands of managing and monitoring these systems. In this tutorial, attendees will get an overview of all phases of successfully managing Hadoop clusters, with an emphasis on production systems.
Southeast Asia provides a unique challenge to large recommender systems: how will you design one system that recommends products to millions of users, many whom are spread across several countries, with their own language and cultural preferences? Well, you don't. Instead, we will explore a hybrid system that integrates inputs from a variety of recommenders and deploys it on a distributed system.
We will introduce Apache SINGA, a flexible and scalable deep learning platform for big data analytics. SINGA is flexible to support various deep learning models, and is general to provide scalable training architecture. We will also show two applications to demonstrate how SINGA is helpful for healthcare data analytics, predicting risk-of-readmission and modeling chronic disease progression.
In this talk, we will explain how data scientists use nested data structures to increase analytic productivity. We will use two well-known relational schemas - TPC-H and Twitter - to demonstrate how to simplify data science workloads with nested schemas. Also, we will outline best practices for converting flat relational schemas into nested ones, and give examples of data science-style analysis.
Our teams main focus at PayPal is to boost customer engagement. This talk is about how we use predictive modeling to recommend products to consumers. We will talk about the technologies we use and how we deploy our models to production.
Privacy in the world of big data is often considered as a legal or regulatory function. However, there are technology solutions for analytics that can be used today to protect users' privacy and to enable applications over data that is too sensitive to share. We will illustrate the state-of-the-art in privacy-preserving machine learning, including new techniques we have developed.
Using the EPOC headset from Emotiv, I can capture the big data stream of EEG from our brains.  I will share my results on a lie detector experiment comparing brain waves when telling the truth and lying.  I have built classifiers based on the EEG data using Azure Machine Learning to predict whether a subject is telling the truth.  The effectiveness of multiple classifiers can be easily compared.
This talk is about an application of big data predictive analytics to improve the online customer experience. The application is built using big data infrastructure with Hadoop, Cassandra, and machine learning algorithms using R and Python, that predict customer intent and take actions in real time to deliver an enhanced experience. Key challenges and lessons learned are also discussed.
In this session, attendees will learn the concepts underlying graph data analytics based on MUFG's experiences. Moreover, it will cover how to analyze huge graph data with Apache Spark GraphX. Finally, it will explore what type of data tends to cause problems and how to solve them.
Many data applications are written in Python or R, but developing and deploying these applications at scale or in production is a pain point for many users. We will discuss our new efforts to bridge the gap between familiar in-memory data tools and distributed data systems. In particular, we are working to enable users to streamline interactions with Hadoop and scalable query engines like Impala.
Geospatial data is revolutionising the marketing research industry.  In this talk, Nielsen researchers will describe how such information is being used by the company to improve internal processes and to give new insights into client behaviour.  The goal is to give clients an analytic edge, as will be illustrated through key methodology and insights of recent projects.
The advent of next-generation DNA sequencing technologies is revolutionizing life sciences research by routinely generating extremely large data sets. Big data tools developed to handle large-scale internet data (like Hadoop) will help scientists effectively manage this new scale of data, and also enable addressing a host of questions that were previously out of reach.
In this session we will take a look at a practical review of what is deep learning and introduce DL4J. We'll look at how it supports deep learning in the enterprise on the JVM. Well discuss the architecture of DL4Js scale-out parallelization on Hadoop and Spark in support of modern machine learning workflows.
In this half-day tutorial, attendees will get a taste of how large-scale data science techniques and technologies developed for the consumer internet can be applied in the world of Telecom.
This tutorial is all about managing large volumes of data coming at your data center fast and continuously. If you don't have a strategy, then allow me to help. Amazing Apache Project software can make this problem a lot easier to deal with. Spend a few hours and learn about how each part works, and how they work together. Your users will thank you.
This talk will broach the topic of how DataSpark has created an innovative way of understanding people and what is important to them, by leveraging advanced data science and the wealth of data in an aggregated manner, while adhering to high standards of data privacy.
We present a traffic measurement system that monitors subway and expressway traffic from telco location data.
GearPump is an akka based framework that processes real time data across a DAG of actors. Its data delivery is highly scalable with at least once data delivery guarantees.
We are developing a platform to process massive sensor data obtained from social infrastructures and industrial machinery all over the world, in order to achieve advanced safety management. In this session, we'll talk about the capability of Spark to realize time-series data processing, the best practices of application development, and realistic lessons on operating Spark on YARN.
Predictive maintenance is a technique to predict when an in-service machine will fail so that maintenance can be planned in advance. This talk introduces the landscape and challenges of predictive maintenance applications in the industry. Through a real-world example, the talk also illustrates how to formulate a predictive maintenance problem with three machine learning models.
Like most large internet sites, Telecom networks are constantly under attack by highly sophisticated fraudsters. Historically, carriers have tried to isolate fraudulent behavior through complex rules. However, increasingly there is a need to use machine learning algorithms that can keep up with the changing face of Telecom fraud.
Real-time analytics are becoming increasingly important to telecommunication operators due to the large amount of data that flows through their networks. Drawing from our experience at Huawei, we present StreamDM, a new open source data mining and machine learning library on top of Spark Streaming. We will present its implemented advanced methods, and demonstrate its ease of use and extensibility.
In this talk, we introduce a recent effort in Spark to employ randomized algorithms for a number of common, expensive methods: membership testing, cardinality, stratified sampling, frequent items, quantile estimation.
In this talk, we will first take a look at current IoT standards, solutions, and common  challenges; change management; and near real-time decision-making capabilities that are yet to be adequately addressed.
Eric Frenkiel, CEO/cofounder, MemSQL, will demonstrate a prototype of a futuristic smart city where all household energy devices are tracked in real-time. He will show the challenges, design choices & architecture required to enable urban planners/energy companies to see what is possible for efficient energy consumption through a real-time data pipeline combining Kafka+Spark+an in-memory database.
The fast evolution of services and mobile terminals combined with the aggressive competition between mobile operators is driving a continuous upgrade of the radio access network (RAN). This upgrade process is expensive and time consuming, and it scales with the number of base stations. This talk stresses the importance of the customer and proposes a new methodology for an efficient RAN upgrade.
Algorithms are what make things "smart." More or less arbitrary, subjective decisions are regularly built into our connected things, when we choose a certain method or set parameters. These underlying value judgments imposed on users are hardly present in the privacy discussion or business point of view. However, they may be more important than the more obvious data collection and security.
In this half-day tutorial, attendees will get a taste of how large-scale data science techniques and technologies developed for the consumer internet can be applied in the world of Telecom.
This tutorial is all about managing large volumes of data coming at your data center fast and continuously. If you don't have a strategy, then allow me to help. Amazing Apache Project software can make this problem a lot easier to deal with. Spend a few hours and learn about how each part works, and how they work together. Your users will thank you.
This talk will broach the topic of how DataSpark has created an innovative way of understanding people and what is important to them, by leveraging advanced data science and the wealth of data in an aggregated manner, while adhering to high standards of data privacy.
We present a traffic measurement system that monitors subway and expressway traffic from telco location data.
GearPump is an akka based framework that processes real time data across a DAG of actors. Its data delivery is highly scalable with at least once data delivery guarantees.
We are developing a platform to process massive sensor data obtained from social infrastructures and industrial machinery all over the world, in order to achieve advanced safety management. In this session, we'll talk about the capability of Spark to realize time-series data processing, the best practices of application development, and realistic lessons on operating Spark on YARN.
Predictive maintenance is a technique to predict when an in-service machine will fail so that maintenance can be planned in advance. This talk introduces the landscape and challenges of predictive maintenance applications in the industry. Through a real-world example, the talk also illustrates how to formulate a predictive maintenance problem with three machine learning models.
Like most large internet sites, Telecom networks are constantly under attack by highly sophisticated fraudsters. Historically, carriers have tried to isolate fraudulent behavior through complex rules. However, increasingly there is a need to use machine learning algorithms that can keep up with the changing face of Telecom fraud.
Real-time analytics are becoming increasingly important to telecommunication operators due to the large amount of data that flows through their networks. Drawing from our experience at Huawei, we present StreamDM, a new open source data mining and machine learning library on top of Spark Streaming. We will present its implemented advanced methods, and demonstrate its ease of use and extensibility.
In this talk, we introduce a recent effort in Spark to employ randomized algorithms for a number of common, expensive methods: membership testing, cardinality, stratified sampling, frequent items, quantile estimation.
In this talk, we will first take a look at current IoT standards, solutions, and common  challenges; change management; and near real-time decision-making capabilities that are yet to be adequately addressed.
Eric Frenkiel, CEO/cofounder, MemSQL, will demonstrate a prototype of a futuristic smart city where all household energy devices are tracked in real-time. He will show the challenges, design choices & architecture required to enable urban planners/energy companies to see what is possible for efficient energy consumption through a real-time data pipeline combining Kafka+Spark+an in-memory database.
The fast evolution of services and mobile terminals combined with the aggressive competition between mobile operators is driving a continuous upgrade of the radio access network (RAN). This upgrade process is expensive and time consuming, and it scales with the number of base stations. This talk stresses the importance of the customer and proposes a new methodology for an efficient RAN upgrade.
Algorithms are what make things "smart." More or less arbitrary, subjective decisions are regularly built into our connected things, when we choose a certain method or set parameters. These underlying value judgments imposed on users are hardly present in the privacy discussion or business point of view. However, they may be more important than the more obvious data collection and security.
This talk is a tutorial for the machine learning library scikit-learn in Python. It starts with a short introduction into what machine learning is, and then dives in-depth into how to use scikit-learn in practice. The tutorial will be in the format of an IPython notebook and includes exercises.
All analytics is prescriptive analytics; it just depends on who's writing the prescription, human or machine. In this talk, I will present how to humanize the big data experience, promote collaboration between business users and data scientists, and bridge the gap between human and machine.
Big data and data science have great potential for accelerating business, but how do you reconcile the opportunity with the sea of possible technologies? Conventional data strategy has little to guide us, focusing more on governance than on creating new value. In this talk, we explain the how to create a modern data strategy that powers data-driven business.
LinkedIn describes how theyve built a self-serve petabyte-scale reporting platform centered around Hadoop, that powers all business decision making at LinkedIn. We describe how we overcame challenges to scale to over a thousand analysts, over a thousand metrics, and provide daily, hourly, as well as real-time reports. This has reduced turnaround times for dashboards from weeks to a few hours.
As the lead for the Accenture-UC Berkeley Data & Analytics Partnership, Hallie has worked with a multidisciplinary team of professors, data scientists, and students to design the Applied Learning Course in Data Science. She will talk about her experience and lessons learned for bringing together technical and business minds to help data science feature more prominently in business strategy.
Getting from raw data to deploying data-driven solutions requires technology, data, and people. All of which exist. So why arent we seeing more truly data-driven companies: what's missing and why? Find out how lack of collaboration is what is keeping companies from imagining and actually doing what is possible to accomplish with big data.
Public safety and national security are increasingly being challenged by technology; the need to use data to detect and investigate criminal activities has increased dramatically.  But with the sheer volume of data and noise, law enforcement organisations are struggling to keep up. This session will examine trends and use cases on how big data can be utilised to make the world a safer place.
Private sector companies are becoming more data-driven, but what does it take to help the social sector become data-driven? DataKind is a global nonprofit that harnesses the power of data science in the service of humanity. Learn about two DataKind Singapore projects that brought together data science volunteers and nonprofit organizations to move the needle in the fight against climate change.
ANZ has adopted an innovative approach to drive continuous business and identification of business value opportunities using disruptive and new big data technologies.
Using data science to make better corporate, financial, and strategic decisions.
In "The Power of Myth," Joseph Campbell distilled the modern story form as "A Hero's Journey." This talk presents the relevance of the story form in data analysis, and shows examples of how to tell insightful data stories.
This talk will briefly explain what neural nets are and why theyre important, as well as give context about GPUs. Then we will walk through code and launch a neural net on a GPU. I will cover key pitfalls you may hit and techniques to diagnose and troubleshoot. You will walk away understanding how to start using GPUs and where to go for additional help.
High performance design requires the ability to optimally measure a system, understand its working, predict its performance, and continuously refine it. Central to this process is leveraging the data in an intelligent way. This talk explains how this can be achieved.
Big data and data science have great potential for accelerating business, but how do you reconcile the opportunity with the sea of possible technologies? Conventional data strategy has little to guide us, focusing more on governance than on creating new value. In this tutorial, we explain how to create a modern data strategy that powers data-driven business.
This talk is a tutorial for the machine learning library scikit-learn in Python. It starts with a short introduction into what machine learning is, and then dives in-depth into how to use scikit-learn in practice. The tutorial will be in the format of an IPython notebook and includes exercises.
All analytics is prescriptive analytics; it just depends on who's writing the prescription, human or machine. In this talk, I will present how to humanize the big data experience, promote collaboration between business users and data scientists, and bridge the gap between human and machine.
Big data and data science have great potential for accelerating business, but how do you reconcile the opportunity with the sea of possible technologies? Conventional data strategy has little to guide us, focusing more on governance than on creating new value. In this talk, we explain the how to create a modern data strategy that powers data-driven business.
LinkedIn describes how theyve built a self-serve petabyte-scale reporting platform centered around Hadoop, that powers all business decision making at LinkedIn. We describe how we overcame challenges to scale to over a thousand analysts, over a thousand metrics, and provide daily, hourly, as well as real-time reports. This has reduced turnaround times for dashboards from weeks to a few hours.
As the lead for the Accenture-UC Berkeley Data & Analytics Partnership, Hallie has worked with a multidisciplinary team of professors, data scientists, and students to design the Applied Learning Course in Data Science. She will talk about her experience and lessons learned for bringing together technical and business minds to help data science feature more prominently in business strategy.
Getting from raw data to deploying data-driven solutions requires technology, data, and people. All of which exist. So why arent we seeing more truly data-driven companies: what's missing and why? Find out how lack of collaboration is what is keeping companies from imagining and actually doing what is possible to accomplish with big data.
Public safety and national security are increasingly being challenged by technology; the need to use data to detect and investigate criminal activities has increased dramatically.  But with the sheer volume of data and noise, law enforcement organisations are struggling to keep up. This session will examine trends and use cases on how big data can be utilised to make the world a safer place.
Private sector companies are becoming more data-driven, but what does it take to help the social sector become data-driven? DataKind is a global nonprofit that harnesses the power of data science in the service of humanity. Learn about two DataKind Singapore projects that brought together data science volunteers and nonprofit organizations to move the needle in the fight against climate change.
ANZ has adopted an innovative approach to drive continuous business and identification of business value opportunities using disruptive and new big data technologies.
Using data science to make better corporate, financial, and strategic decisions.
In "The Power of Myth," Joseph Campbell distilled the modern story form as "A Hero's Journey." This talk presents the relevance of the story form in data analysis, and shows examples of how to tell insightful data stories.
This talk will briefly explain what neural nets are and why theyre important, as well as give context about GPUs. Then we will walk through code and launch a neural net on a GPU. I will cover key pitfalls you may hit and techniques to diagnose and troubleshoot. You will walk away understanding how to start using GPUs and where to go for additional help.
High performance design requires the ability to optimally measure a system, understand its working, predict its performance, and continuously refine it. Central to this process is leveraging the data in an intelligent way. This talk explains how this can be achieved.
Big data and data science have great potential for accelerating business, but how do you reconcile the opportunity with the sea of possible technologies? Conventional data strategy has little to guide us, focusing more on governance than on creating new value. In this tutorial, we explain how to create a modern data strategy that powers data-driven business.
This session teaches use of modern data analysis and visualization tools for effective interactive data science. Attendees will learn how to use notebook environments to set up sharable and reproducible analysis pipelines, and will leverage tools for large scale analysis and web-based data visualization to drive further analysis and decision making.
In this tutorial, you will create end-to-end predictive models based on an extensive library of machine learning algorithms included in Microsoft Azure Machine Learning studio with its R and Python language extensibility. You will then deploy and consume the model and use it for making predictions over business data.
This session teaches use of modern data analysis and visualization tools for effective interactive data science. Attendees will learn how to use notebook environments to set up sharable and reproducible analysis pipelines, and will leverage tools for large scale analysis and web-based data visualization to drive further analysis and decision making.
In this tutorial, you will create end-to-end predictive models based on an extensive library of machine learning algorithms included in Microsoft Azure Machine Learning studio with its R and Python language extensibility. You will then deploy and consume the model and use it for making predictions over business data.
This session will investigate the trade-offs between real-time transactional access and fast analytic performance in Hadoop from the perspective of storage engine internals. We will discuss recent advances, evaluate benchmark results from current generation Hadoop technologies, and propose potential ways ahead for the Hadoop ecosystem to conquer its newest set of challenges.
Based on previous experience, there are many challenges in designing an Impala cluster for production, such as table schema, data placement, file format selection, hardware selection, and software stack parameters tuning. We will walk through a real-world case study in the banking and financial services sector to illustrate how we use our simulator-based approach to design an Impala cluster.
This session will address how one single Hadoop cluster can be built across many geographically distributed data centers to provide multitenant analytics services. We extend the overall architecture of Hadoop so that multiple tenants can securely access, share, and analyze data in their own isolated executing environments.
Apache Hadoop was designed when cloud models were in their infancy. Despite this fact, Hadoop has proven remarkably adept at migrating its architecture to work well in the context of the cloud, as production workloads migrate to a cloud environment. This talk will have cover several topics on adapting Hadoop to the cloud.
When people think of big data processing, they think of Apache Hadoop, but that doesn't mean traditional databases don't play a role. In most cases users will still draw from data stored in RDBMS systems. Apache Sqoop can be used to unlock that data and transfer it to Hadoop, enabling users with information stored in existing SQL tables to use new analytic tools.
Application developers have long created complex schemas to handle storing with minor relationships in an RDBMS. This talk will show how to convert an existing (complicated schema) music database to HBase for transactional workloads, plus how to use Drill against HBase for real-time queries. HBase column families will also be discussed.
This talk introduces a 12-step guide to help secure a data deployment in the cloud. Using the help of open source solutions and security best practices, you will be familiarized with a simple yet effective framework that can be used to fortify your own data-driven deployment in the cloud against accidental and malicious data breaches.
We're living in an age of big data, a time when metadata about most of our movements and actions are collected and stored in real time. These data offer unprecedented insights on how we behave. Mathematical analysis of metadata, however, reveals how unique our behavior is and how this behavior puts fundamental constraints on our privacy.
Find out how the world's most sophisticated Hadoop deployments are addressing data governance challenges head-on, while preserving Hadoop's flexibility, through an integrated data management and governance approach.
Designing data visualizations presents us with unique and interesting challenges: how to tell a compelling story; how to deliver important information in a forthright, clear format; and how to make visualizations beautiful and engaging. In this talk, Julie will share a few disruptive designs and connect those back to vizipedia, her compiled data visualization library.
Understand techniques to effectively visualise multi-dimensional data to aid exploratory data analysis. We will look at standard 2D/3D, geometric transformations, glyph-based, pixel-based, and stacking-based approaches to visualise this data, and also explore the interactive approaches needed to make them work.
UBM Asia is the largest trade show organizer in Asia. To deal with duplicate customer records and ensure clean marketing data, UBM Asia has built an end to end solution using Reifier from Nube Technologies built atop Spark. This talk will discuss UBM's use case and our use of Reifier fuzzy matching engine, Spark and machine learning. We will also cover Reifier's architecture and usage of Spark.
This session will investigate the trade-offs between real-time transactional access and fast analytic performance in Hadoop from the perspective of storage engine internals. We will discuss recent advances, evaluate benchmark results from current generation Hadoop technologies, and propose potential ways ahead for the Hadoop ecosystem to conquer its newest set of challenges.
Based on previous experience, there are many challenges in designing an Impala cluster for production, such as table schema, data placement, file format selection, hardware selection, and software stack parameters tuning. We will walk through a real-world case study in the banking and financial services sector to illustrate how we use our simulator-based approach to design an Impala cluster.
This session will address how one single Hadoop cluster can be built across many geographically distributed data centers to provide multitenant analytics services. We extend the overall architecture of Hadoop so that multiple tenants can securely access, share, and analyze data in their own isolated executing environments.
Apache Hadoop was designed when cloud models were in their infancy. Despite this fact, Hadoop has proven remarkably adept at migrating its architecture to work well in the context of the cloud, as production workloads migrate to a cloud environment. This talk will have cover several topics on adapting Hadoop to the cloud.
When people think of big data processing, they think of Apache Hadoop, but that doesn't mean traditional databases don't play a role. In most cases users will still draw from data stored in RDBMS systems. Apache Sqoop can be used to unlock that data and transfer it to Hadoop, enabling users with information stored in existing SQL tables to use new analytic tools.
Application developers have long created complex schemas to handle storing with minor relationships in an RDBMS. This talk will show how to convert an existing (complicated schema) music database to HBase for transactional workloads, plus how to use Drill against HBase for real-time queries. HBase column families will also be discussed.
This talk introduces a 12-step guide to help secure a data deployment in the cloud. Using the help of open source solutions and security best practices, you will be familiarized with a simple yet effective framework that can be used to fortify your own data-driven deployment in the cloud against accidental and malicious data breaches.
We're living in an age of big data, a time when metadata about most of our movements and actions are collected and stored in real time. These data offer unprecedented insights on how we behave. Mathematical analysis of metadata, however, reveals how unique our behavior is and how this behavior puts fundamental constraints on our privacy.
Find out how the world's most sophisticated Hadoop deployments are addressing data governance challenges head-on, while preserving Hadoop's flexibility, through an integrated data management and governance approach.
Designing data visualizations presents us with unique and interesting challenges: how to tell a compelling story; how to deliver important information in a forthright, clear format; and how to make visualizations beautiful and engaging. In this talk, Julie will share a few disruptive designs and connect those back to vizipedia, her compiled data visualization library.
Understand techniques to effectively visualise multi-dimensional data to aid exploratory data analysis. We will look at standard 2D/3D, geometric transformations, glyph-based, pixel-based, and stacking-based approaches to visualise this data, and also explore the interactive approaches needed to make them work.
UBM Asia is the largest trade show organizer in Asia. To deal with duplicate customer records and ensure clean marketing data, UBM Asia has built an end to end solution using Reifier from Nube Technologies built atop Spark. This talk will discuss UBM's use case and our use of Reifier fuzzy matching engine, Spark and machine learning. We will also cover Reifier's architecture and usage of Spark.
The real power and value proposition of Apache Spark is in building a unified use case that combines ETL, batch analytics, real-time stream analysis, machine learning, graph processing and visualizations. In class we will explore various Wikipedia datasets while applying the ideal programming paradigm for each analysis. The class will comprise of about 50% lecture and 50% hands on labs + demos.
Baidu runs Tachyon in production with more than 100 nodes managing 2PB space! In this talk we will focus on how Tachyon can help improve big data analytics (ad-hoc query) with 30X performance improvement within Baidu.
At IDAs Government Analytics department, our team of data scientists work with bus operators to offer demand-driven express bus routes by combining crowdsourcing and big data. We use Apache Spark to analyze ticketing, taxi, and crowdsourced data to find bus routes that are both time-saving and financially viable. We show how these insights are delivered into a new transport option for commuters.
Aesop is an open source reliable change data propagation system. It has been used to build tiered data stores using best in class SQL and NoSQL databases. Aesop provides simple pubsub-like interfaces with implementations for popular technologies like MySQL, HBase, Redis, Elasticsearch, and Kafka. Aesop scales to multi-node clusters that process millions of data records.
Kafka provides the low latency, high throughput, high availability, and scale that financial services firms require. But can it also provide complete reliability? In this session, we will go over everything that happens to a message - from producer to consumer - and pinpoint all the places where data can be lost if you are not careful.
In this session, we will discuss common archictectural patterns for building streaming applications.
Join me for a whirlwind tour of the conceptual building blocks of massive-scale data processing systems over the last decade, comparing and contrasting systems at Google with popular open source systems in use today.
The GDELT Project is a real-time open data global graph over human society, inventorying the worlds events, emotions, and narratives in 65 languages, used by organizations from the UN to Wall Street.  Google BigQuery enables real-time querying and whole-of-data analysis of GDELT, such as exploring the cycles of world history through mass cross-correlation.
Have you faced the challenge of storing and optimally serving multibillion-row EAV modeled data out of a traditional data store? Monolithic data stores fall short, even with fast storage like SSDs for a large online marketplace, quantified here as 3 billion catalog entries and 100 million catalog updates in a day. This talk is about paradigms and patterns we adopted to address this problem.
This talk will show architectures and techniques for combining Apache Cassandra and Spark to yield a 10-1000x improvement in OLAP analytical performance, and introduce a new open source database that takes advantage of these techniques.
Organizations frequently rely on dedicated query layers, such as relational databases and key/value stores, for faster query latencies; but these technologies suffer many drawbacks for analytic use cases. In this session, we discuss examine using Druid to power applications designed to analyze sensor data, and why the architecture is well suited for different use cases in smart cities.
Current memory size is far from enough to host data sets. NVM has emerged to respond to this need. However, how to integrate NVM to support a modernized big data system is a challenge. In this talk, we present our efforts to make a tiered store in Tachyon, which provided a software solution for next-gen data center platforms with NVM.
This talk will cover Spark design patterns in time series analysis, visualizing data, and Monte Carlo simulation; and will show you what it is like to approach financial modeling with Spark.
The real power and value proposition of Apache Spark is in building a unified use case that combines ETL, batch analytics, real-time stream analysis, machine learning, graph processing and visualizations. In class we will explore various Wikipedia datasets while applying the ideal programming paradigm for each analysis. The class will comprise of about 50% lecture and 50% hands on labs + demos.
Baidu runs Tachyon in production with more than 100 nodes managing 2PB space! In this talk we will focus on how Tachyon can help improve big data analytics (ad-hoc query) with 30X performance improvement within Baidu.
At IDAs Government Analytics department, our team of data scientists work with bus operators to offer demand-driven express bus routes by combining crowdsourcing and big data. We use Apache Spark to analyze ticketing, taxi, and crowdsourced data to find bus routes that are both time-saving and financially viable. We show how these insights are delivered into a new transport option for commuters.
Aesop is an open source reliable change data propagation system. It has been used to build tiered data stores using best in class SQL and NoSQL databases. Aesop provides simple pubsub-like interfaces with implementations for popular technologies like MySQL, HBase, Redis, Elasticsearch, and Kafka. Aesop scales to multi-node clusters that process millions of data records.
Kafka provides the low latency, high throughput, high availability, and scale that financial services firms require. But can it also provide complete reliability? In this session, we will go over everything that happens to a message - from producer to consumer - and pinpoint all the places where data can be lost if you are not careful.
In this session, we will discuss common archictectural patterns for building streaming applications.
Join me for a whirlwind tour of the conceptual building blocks of massive-scale data processing systems over the last decade, comparing and contrasting systems at Google with popular open source systems in use today.
The GDELT Project is a real-time open data global graph over human society, inventorying the worlds events, emotions, and narratives in 65 languages, used by organizations from the UN to Wall Street.  Google BigQuery enables real-time querying and whole-of-data analysis of GDELT, such as exploring the cycles of world history through mass cross-correlation.
Have you faced the challenge of storing and optimally serving multibillion-row EAV modeled data out of a traditional data store? Monolithic data stores fall short, even with fast storage like SSDs for a large online marketplace, quantified here as 3 billion catalog entries and 100 million catalog updates in a day. This talk is about paradigms and patterns we adopted to address this problem.
This talk will show architectures and techniques for combining Apache Cassandra and Spark to yield a 10-1000x improvement in OLAP analytical performance, and introduce a new open source database that takes advantage of these techniques.
Organizations frequently rely on dedicated query layers, such as relational databases and key/value stores, for faster query latencies; but these technologies suffer many drawbacks for analytic use cases. In this session, we discuss examine using Druid to power applications designed to analyze sensor data, and why the architecture is well suited for different use cases in smart cities.
Current memory size is far from enough to host data sets. NVM has emerged to respond to this need. However, how to integrate NVM to support a modernized big data system is a challenge. In this talk, we present our efforts to make a tiered store in Tachyon, which provided a software solution for next-gen data center platforms with NVM.
This talk will cover Spark design patterns in time series analysis, visualizing data, and Monte Carlo simulation; and will show you what it is like to approach financial modeling with Spark.
Cloudera University's one-day essentials course presents an overview of Apache Hadoop and how it can help decision-makers meet business goals, providing a fundamental introduction to the main components of Hadoop and its use cases in various industries. This course is a good starting point for any role or set of objectives and is part of the data analyst learning path.
Cloudera University's one-day essentials course presents an overview of Apache Hadoop and how it can help decision-makers meet business goals, providing a fundamental introduction to the main components of Hadoop and its use cases in various industries. This course is a good starting point for any role or set of objectives and is part of the data analyst learning path.
Join us as we review the Big Data landscape and reflect on Big Data lessons being learned in enterprise over the last few years and how these organisations are avoiding their Big Data environments becoming unmanageable by using simplex management for deployment, administration, monitoring and reporting no matter how much the environment scales.
With Big Data system using Hadoop platform, we resolved the problem that make slow down the performance with existing legacy system based on RDBMS. And we set up real-time pattern analysis system using Spark. It provides easy and quick solutions to hands-on worker to monitor and diagnose manufacturing processes rather than traditional legacy system based on RDBMS.
In this talk, we will present our efforts on building large scale distributed ML on Apache Spark with many "web-scale" companies, including very complex and advanced analytics applications / algorithms (e.g., topic modelling, deep neural network, etc.), as well as massively scalable learning system/platform leveraging both application and infrastructure specific optimizations.
n this practical demonstration, participants will see how they can perform a simple, but meaningful analysis of social sentiment data using freely available and easy to deploy tools. Participants will be equipped with the download links, scripts, and complete step-by-step walkthrough of the analysis from start to finish.
Big data and analytics continue to be a disruptive business force. Are we entering another phase  real-time digital business transformation, where businesses are realizing that the time to adjust to market and customer opportunities and threats is shrinking quickly?
Attend this session to: *Learn about the key drivers behind the shift to Hadoop based platforms *Understand the common steps in the journey to adopting Hadoop *Hear real-word case studies of business transformation using Hadoop *Get insight into the core components that make up Open Enterprise Hadoop
As organization strive to survive and breakthrough other markets, discover how Big Data Analytics can be the key to unlock new business opportunities or simply exploit a companys full potential in its own competitive space. Companies are increasingly swimming in more and more data so developing the abilities to breathe data will be crucial to stay ahead of competition.
In 2015 Woodside is working with Accenture to deliver predictive analytics to Woodsides LNG operations. By combining Accentures expertise in data analytics and Woodsides leading operational experience in oil and gas, valuable, actionable insights have been discovered throughout 2015.
Join us as we review the Big Data landscape and reflect on Big Data lessons being learned in enterprise over the last few years and how these organisations are avoiding their Big Data environments becoming unmanageable by using simplex management for deployment, administration, monitoring and reporting no matter how much the environment scales.
With Big Data system using Hadoop platform, we resolved the problem that make slow down the performance with existing legacy system based on RDBMS. And we set up real-time pattern analysis system using Spark. It provides easy and quick solutions to hands-on worker to monitor and diagnose manufacturing processes rather than traditional legacy system based on RDBMS.
In this talk, we will present our efforts on building large scale distributed ML on Apache Spark with many "web-scale" companies, including very complex and advanced analytics applications / algorithms (e.g., topic modelling, deep neural network, etc.), as well as massively scalable learning system/platform leveraging both application and infrastructure specific optimizations.
n this practical demonstration, participants will see how they can perform a simple, but meaningful analysis of social sentiment data using freely available and easy to deploy tools. Participants will be equipped with the download links, scripts, and complete step-by-step walkthrough of the analysis from start to finish.
Big data and analytics continue to be a disruptive business force. Are we entering another phase  real-time digital business transformation, where businesses are realizing that the time to adjust to market and customer opportunities and threats is shrinking quickly?
Attend this session to: *Learn about the key drivers behind the shift to Hadoop based platforms *Understand the common steps in the journey to adopting Hadoop *Hear real-word case studies of business transformation using Hadoop *Get insight into the core components that make up Open Enterprise Hadoop
As organization strive to survive and breakthrough other markets, discover how Big Data Analytics can be the key to unlock new business opportunities or simply exploit a companys full potential in its own competitive space. Companies are increasingly swimming in more and more data so developing the abilities to breathe data will be crucial to stay ahead of competition.
In 2015 Woodside is working with Accenture to deliver predictive analytics to Woodsides LNG operations. By combining Accentures expertise in data analytics and Woodsides leading operational experience in oil and gas, valuable, actionable insights have been discovered throughout 2015.
Hadoop lacks a mechanism to extend the distributed file system beyond the confines of a single cluster. Done right, active-active consensus can guarantee consistency of replicated file system changes regardless of Hadoop versions, distributions and communication latency. Find out how to perform selective data replication for cluster migration, disaster recovery, multi-site ingest, backup and more.
Join us to hear how  Big Data and Analytics Subject Matter Expert, Joanna Schloss,  envisions emerging technologies shaping the future of mission critical initiatives such as security and analytics.  How will in-memory, big data, and IoT shape and guide businesses with deployment and maintenance of key capabilities?
Flexible data model. SQL compatibility. Unlimited scale. Nearly all data systems require that you pick at most one or two of three. You can now have them all. I will show how a real-world relational database can be massively simplified using document structure, how that database can be queried using SQL and how it can scale to the trillion-row, TB-scale required by modern applications.
With Hadoop becoming the chosen Data Platform across enterprises, analytical lifecycles are now being powered with Hadoop being the centrepiece for discovery and deployment. During this talk, attendees will get insights from organisations that are building and deploying thousands of analytical models into their operational environments.
Companies who are successful with big data need to be analytics-driven. During this session, Neil will look at new analytics capabilities that are essential for big data to deliver results, and discuss how to maximize the time you spend providing differentiation for your organization. This session will also cover some common big data use cases in both industry and government.
 How different Industry Verticals are using HPE Platform for Hadoop and Big Data Analytics. The session Covers various use cases across Industry Verticals which are implemented by customer's across APJ and why HPE has a Unique Value Proposition for Hadoop Solutions.
SAP HANA Vora is a new in-memory query engine that leverages and extends the Apache Spark execution framework to provide enriched interactive analytics on Hadoop.
In this talk, we will discuss how a streamlined Spark stack including Tachyon and Zeppelin can solve both the need for speed and reduced development time. We will walk thru a sample use case that utilizes 20 years of data to look for insights and create a predictive model from the dataset.
This session focuses on strategies and technologies you can use to build a global Hadoop cloud with geo-distributed access and protection for analytics in various use-cases like IoT - handling billions of small files or multi-terabyte files in the same system.
Hadoop lacks a mechanism to extend the distributed file system beyond the confines of a single cluster. Done right, active-active consensus can guarantee consistency of replicated file system changes regardless of Hadoop versions, distributions and communication latency. Find out how to perform selective data replication for cluster migration, disaster recovery, multi-site ingest, backup and more.
Join us to hear how  Big Data and Analytics Subject Matter Expert, Joanna Schloss,  envisions emerging technologies shaping the future of mission critical initiatives such as security and analytics.  How will in-memory, big data, and IoT shape and guide businesses with deployment and maintenance of key capabilities?
Flexible data model. SQL compatibility. Unlimited scale. Nearly all data systems require that you pick at most one or two of three. You can now have them all. I will show how a real-world relational database can be massively simplified using document structure, how that database can be queried using SQL and how it can scale to the trillion-row, TB-scale required by modern applications.
With Hadoop becoming the chosen Data Platform across enterprises, analytical lifecycles are now being powered with Hadoop being the centrepiece for discovery and deployment. During this talk, attendees will get insights from organisations that are building and deploying thousands of analytical models into their operational environments.
Companies who are successful with big data need to be analytics-driven. During this session, Neil will look at new analytics capabilities that are essential for big data to deliver results, and discuss how to maximize the time you spend providing differentiation for your organization. This session will also cover some common big data use cases in both industry and government.
 How different Industry Verticals are using HPE Platform for Hadoop and Big Data Analytics. The session Covers various use cases across Industry Verticals which are implemented by customer's across APJ and why HPE has a Unique Value Proposition for Hadoop Solutions.
SAP HANA Vora is a new in-memory query engine that leverages and extends the Apache Spark execution framework to provide enriched interactive analytics on Hadoop.
In this talk, we will discuss how a streamlined Spark stack including Tachyon and Zeppelin can solve both the need for speed and reduced development time. We will walk thru a sample use case that utilizes 20 years of data to look for insights and create a predictive model from the dataset.
This session focuses on strategies and technologies you can use to build a global Hadoop cloud with geo-distributed access and protection for analytics in various use-cases like IoT - handling billions of small files or multi-terabyte files in the same system.
Strata + Hadoop World Program Chairs Roger Magoulas, Doug Cutting, and Alistair Croll welcome you to the first day of keynotes.
Strata + Hadoop World Program Chairs Roger Magoulas, Doug Cutting, and Alistair Croll welcome you to the second day of keynotes.
Hadoop has come a long way from monolithic storage and batch processing; today the ecosystem is diverse and flexible and is emerging as the foundation of next-generation analytic applications.  Join Mike Olson, Cloudera's Chief Strategy Officer, as he discusses new innovations across the ecosystem and gives a vision for Hadoop as an architectural must have for analytics transformation.
Why do taxi drivers not want to pick me up when I most need a taxi? Join GrabTaxi's Kevin Lee to learn how GrabTaxi uses machine learning to answer this age old question and build models for predicting taxi availability in order to improve matching on the platform.
Big data and analytics continue to be a disruptive business force. Are we entering another phase  real-time digital business transformation, where businesses are realizing that the time to adjust to market and customer opportunities and threats is shrinking quickly?
When data is  hidden and crunched, and used purely for organization and optimization, we may be losing out on a crucial value it can offer  that of empowerment, engagement and impactful behavioral change.
Join this keynote presentation to get tips from the future and hear about key patterns emerging from a wide cross section of corporate and institutional Hadoop journeys. Perhaps theyll inspire yours.
 In this session, well take a look at how music streaming delivers real time data that enables us to proxy a billion behaviors and apply the signals to other industries. Rishi was also a participant in the OReilly Study Music Science, published in 2015 by Alistair Croll.
Learn how the intersection of people, data and intelligent machines will have far-reaching impact on the productivity, efficiency and operations of industries around the world as organizations transform to become data-driven, insight-powered enterprises.
Within the next decade, 16 percent of current US jobs will be done by artificial intelligences. Its time to start thinking about how we onboard these employees. While well look at what it takes to get started with machine learning projects, our focus will be on the top 5 things you need to consider when your next employee is an AI.
The data century is upon us and Apache Hadoop has emerged as the platform for managing your big data opportunity.  The path to success is not without its perils, however, and without a thoughtful approach progress can be hindered by the impact of change, trust and security.
Dr. Vivian Balakrishnan is the Singapore Minister for Foreign Affairs and the Minister-in-charge of the Smart Nation Programme Office.
If you could simulate the results of your business decisions, wouldn't that change the way you manage your business? The availability of big data solutions today introduces new management principles, opportunities as well as challenges.
With the recent advances of big data and machine learning technologies, there has never been a better time for developing telecom data products. However there are various challenges associated with researching and developing telecom data products at scale.
Deep Learning is taking hold as a popular machine learning modeling technique because of its real world applications especially with regards to image, signal and language datasets (e.g. medical diagnosis, self-driving cars, real-time language translation). This talk provides an overview of what  deep learning is especially around recent applications.
The high volume, velocity, variety and veracity of big data have been pushing for more comprehensive solutions and services to enable decision-making and insight discovery across business market segments. Ziya Ma, General Manager of Big Data Software Technologies in Intel's Software and Services Group, will discuss Intels software enabling role for making this possible and easier.
In this talk, Reynold will look back and review Sparks growth in adoption, use cases, and development. He will then look forward and discuss both technical initiatives and the evolution of the Spark community for 2016.
This talk will highlight the top 5 mistakes we make in collecting and analyzing qualitative data, how to do it better, and how it can inspire your next big thing.
Strata + Hadoop World Program Chairs Roger Magoulas, Doug Cutting, and Alistair Croll welcome you to the first day of keynotes.
Strata + Hadoop World Program Chairs Roger Magoulas, Doug Cutting, and Alistair Croll welcome you to the second day of keynotes.
Hadoop has come a long way from monolithic storage and batch processing; today the ecosystem is diverse and flexible and is emerging as the foundation of next-generation analytic applications.  Join Mike Olson, Cloudera's Chief Strategy Officer, as he discusses new innovations across the ecosystem and gives a vision for Hadoop as an architectural must have for analytics transformation.
Why do taxi drivers not want to pick me up when I most need a taxi? Join GrabTaxi's Kevin Lee to learn how GrabTaxi uses machine learning to answer this age old question and build models for predicting taxi availability in order to improve matching on the platform.
Big data and analytics continue to be a disruptive business force. Are we entering another phase  real-time digital business transformation, where businesses are realizing that the time to adjust to market and customer opportunities and threats is shrinking quickly?
When data is  hidden and crunched, and used purely for organization and optimization, we may be losing out on a crucial value it can offer  that of empowerment, engagement and impactful behavioral change.
Join this keynote presentation to get tips from the future and hear about key patterns emerging from a wide cross section of corporate and institutional Hadoop journeys. Perhaps theyll inspire yours.
 In this session, well take a look at how music streaming delivers real time data that enables us to proxy a billion behaviors and apply the signals to other industries. Rishi was also a participant in the OReilly Study Music Science, published in 2015 by Alistair Croll.
Learn how the intersection of people, data and intelligent machines will have far-reaching impact on the productivity, efficiency and operations of industries around the world as organizations transform to become data-driven, insight-powered enterprises.
Within the next decade, 16 percent of current US jobs will be done by artificial intelligences. Its time to start thinking about how we onboard these employees. While well look at what it takes to get started with machine learning projects, our focus will be on the top 5 things you need to consider when your next employee is an AI.
The data century is upon us and Apache Hadoop has emerged as the platform for managing your big data opportunity.  The path to success is not without its perils, however, and without a thoughtful approach progress can be hindered by the impact of change, trust and security.
Dr. Vivian Balakrishnan is the Singapore Minister for Foreign Affairs and the Minister-in-charge of the Smart Nation Programme Office.
If you could simulate the results of your business decisions, wouldn't that change the way you manage your business? The availability of big data solutions today introduces new management principles, opportunities as well as challenges.
With the recent advances of big data and machine learning technologies, there has never been a better time for developing telecom data products. However there are various challenges associated with researching and developing telecom data products at scale.
Deep Learning is taking hold as a popular machine learning modeling technique because of its real world applications especially with regards to image, signal and language datasets (e.g. medical diagnosis, self-driving cars, real-time language translation). This talk provides an overview of what  deep learning is especially around recent applications.
The high volume, velocity, variety and veracity of big data have been pushing for more comprehensive solutions and services to enable decision-making and insight discovery across business market segments. Ziya Ma, General Manager of Big Data Software Technologies in Intel's Software and Services Group, will discuss Intels software enabling role for making this possible and easier.
In this talk, Reynold will look back and review Sparks growth in adoption, use cases, and development. He will then look forward and discuss both technical initiatives and the evolution of the Spark community for 2016.
This talk will highlight the top 5 mistakes we make in collecting and analyzing qualitative data, how to do it better, and how it can inspire your next big thing.
Tesco is best known for using data in customer segmentation with Clubcard. But it's also important in optimising a supply chain which moves 32,000,000 cases of food each week. Tom will talk about real-life applications of data science  from managing the impact of weather on sales to optimising truck loads. And share experiences about translating data science into real business change.
In the TV industry, audience is king. Until now, viewers were hidden behind their TV set. Today, Twitter is rapidly changing the game. People share their emotions publicly, in real-time. Capturing these conversations allows advertisers, marketers and TV Channels to discover their audience at a granular level never seen before.
We live on a finite and bounded planet. This fact seems largely ignored in our global economic systems. AMEE has compiled millions of environmental data points. We are now combining them with large-scale financial data to create a "forcing function" that will drive mainstream environmental sustainability.
Cloud computing enables cool massive-scale data analysis, but has a very large carbon footprint, especially since many public cloud providers are powered by coal-fired grids: annual data centre emissions are currently ~75 million tonnes and growing. This talk aims to increase understanding of the issue, and to demonstrate how to achieve big carbon reductions without reducing the analysis you do.
This session explores these concepts by first laying out the fundamentals of change and how all industry evolves through a commonly re-occuring pattern. Using this we will examine why one size never fits all in management, the explosion of change from big data and cloud computing to why new forms of organisation are emerging that differ from traditional companies.
Healthcare could be transformed by data. It is the subject of intense debate by clinicians, regulators, and politicians; too often patients are left out. Louise Marston and Laura Bunt from Nesta, the UK innovation agency, will discuss how the challenges in healthcare data exemplify the problems of personal data, and describe a vision of patients using data for their healthcare decisions in future.
With the arrival of low cost DNA sequencing, genomics is moving ever closer to clinical practice. This presentation discusses how bioinformaticians are meeting the challenges of harnessing the value of ever growing collection of valuable genomic data.
Quantifying one's self, a growing trend, is about self-awareness, pattern spotting & behaviour change. What is missing is "data literacy" i.e. data expertise at individual level, not just for businesses and institutions.Uncovering hidden cause and effect in one's behaviour increases individual's autonomy and for that we need to have access to analytical tools and raw data. How & where to get them?
A presentation about how the biggest financial news organisation in the world is handling big data on a daily basis through its terminal system, and the new data journalism projects it is developing at the moment to deliver ground-breaking news to the most influential people in the world.
Data journalism blurs the line between coders, data geeks and journalists. The Data Journalism Handbook encourages journalists to treat data as a source and to pick up their computer to try new ways of reporting.  This session highlights key lessons from the book, including a) getting stories from data (big or small) b) business models for data driven newsrooms and c) how to get started.
Data, data everywhere... from our unique experience providing social data to hundreds of customers, we have learned the biggest problems you'll encounter when you get all the data you wish for. In this session, we'll share these problems, some solutions and how we meet our own challenges in dealing with massive flows of social data
In this talk, I will look at the next step in big data in general and open data in particular: transparency of insight and how the intelligent transformation of data into narratives can bring to light the stories within it and enable the higher level of understanding and insight needed to support evidence-based decision-making.
Strategy has changed. The step-change in data abundance, speed and competition means that static business plans striving for that 'perfect answer' are obsolete. We'll demonstrate how the Data Science underpinning race strategy engines used in Formula One to plan, track and update strategy in real-time are enabling Fortune 500 be more agile, and creating a new way of strategy planning.
Spreadsheets are used almost everywhere, for almost everything. Researchers from Delft University of Technology have studied spreadsheet users and their spreadsheets to learn more on how exactly they are built, maintained and migrated. In this session we present a case study concerning the analysis of 3 million spreadsheets we analyzed.
Program Chairs, Edd Dumbill and Kaitlin Thaney, welcome you to Strata in London
Liam Maxwell, Executive Director of the IT Reform Group in the Cabinet Office
For all of our machine learning algorithms and big data tools, so many of the problems we solve day-to-day are decidedly "first world": figuring out how to get the biggest ROI on ad dollars or crafting personalized movie recommendations.  Can we use our skills as data scientists to solve social problems as well, helping people find clean water as easily as they can find good restaurants?
What will Big Data mean to us as users, consumers and organisations?  And will it really be a big deal? In this presentation Mikael Bisgaard-Bohr will provide a fascinating view into where the Big Data wave is taking us, and why it is about so much more than just data.
Mapping real-world correspondence to data structures populating a storage matrix currently expanding by some 5 trillion bits per second is the challenge that brings us here.
Data provides critical insight into the way government works. When the UK government published every item of spending over 25,000, the data was hard to parse. The UK Guardian Datablog cleaned it up and asked readers to help pore through the numbers, making everyone a data journalist. Well cover the technologies the Guardian uses to analyze, visualize, and share data with the world.
Nobody knows statistics. They are as esoteric as chemical compounds are to chemistry. Yet data visualizations often incorporate a logarithmic scale, density traces, or seasonally adjusted numbers among other things. If this is the data deluge, we're bound to find everyone swept downstream. How do we prepare the average data consumer?
The real challenge ahead of us is not accumulating more information, or processing more information, or analytics, or replacing relational databases, or scaling data (i.e. not the 3 Vs). The real challenge is solving the information glut problem.
Everyone uses the term big data but no on can agree on what it means or even if it's novel. However the label is useful to describe the radically new ways that the world interacts with information - for which the public, policymakers and even data geeks, are unprepared.
Program Chairs, Edd Dumbill and Kaitlin Thaney, welcome you to the second day of Strata in London keynotes.
It's 1951 and you've got the world's first business computer and you've just been handed a Big Data problem. Go!
Big data isn't just multi-terabyte datasets hidden inside eventually-concurrent distributed databases in the cloud. Its also about the hidden data you carry with you all the time, data that is generated for you and about you, but not necessarily by you. Hidden data, your data, carrying on its secret life without your knowledge, but with your implicit and implied consent.
Alexandra Deschamps-Sonsino, Founder of Good Night Lamp / Founder of Designswarm
Data is great. Data is powerful. But when some data is missing, bias can be introduced, distorting the overall picture.
Jeni Tennison, Technical Director of the newly formed Open Data Institute, will describe the ODIs twin aims of helping data owners achieve their organisational objectives through publishing open data, and helping those who reuse that data to add value responsibly and effectively, thereby turning open data dreams into reality.
Tesco is best known for using data in customer segmentation with Clubcard. But it's also important in optimising a supply chain which moves 32,000,000 cases of food each week. Tom will talk about real-life applications of data science  from managing the impact of weather on sales to optimising truck loads. And share experiences about translating data science into real business change.
In the TV industry, audience is king. Until now, viewers were hidden behind their TV set. Today, Twitter is rapidly changing the game. People share their emotions publicly, in real-time. Capturing these conversations allows advertisers, marketers and TV Channels to discover their audience at a granular level never seen before.
We live on a finite and bounded planet. This fact seems largely ignored in our global economic systems. AMEE has compiled millions of environmental data points. We are now combining them with large-scale financial data to create a "forcing function" that will drive mainstream environmental sustainability.
Cloud computing enables cool massive-scale data analysis, but has a very large carbon footprint, especially since many public cloud providers are powered by coal-fired grids: annual data centre emissions are currently ~75 million tonnes and growing. This talk aims to increase understanding of the issue, and to demonstrate how to achieve big carbon reductions without reducing the analysis you do.
This session explores these concepts by first laying out the fundamentals of change and how all industry evolves through a commonly re-occuring pattern. Using this we will examine why one size never fits all in management, the explosion of change from big data and cloud computing to why new forms of organisation are emerging that differ from traditional companies.
Healthcare could be transformed by data. It is the subject of intense debate by clinicians, regulators, and politicians; too often patients are left out. Louise Marston and Laura Bunt from Nesta, the UK innovation agency, will discuss how the challenges in healthcare data exemplify the problems of personal data, and describe a vision of patients using data for their healthcare decisions in future.
With the arrival of low cost DNA sequencing, genomics is moving ever closer to clinical practice. This presentation discusses how bioinformaticians are meeting the challenges of harnessing the value of ever growing collection of valuable genomic data.
Quantifying one's self, a growing trend, is about self-awareness, pattern spotting & behaviour change. What is missing is "data literacy" i.e. data expertise at individual level, not just for businesses and institutions.Uncovering hidden cause and effect in one's behaviour increases individual's autonomy and for that we need to have access to analytical tools and raw data. How & where to get them?
A presentation about how the biggest financial news organisation in the world is handling big data on a daily basis through its terminal system, and the new data journalism projects it is developing at the moment to deliver ground-breaking news to the most influential people in the world.
Data journalism blurs the line between coders, data geeks and journalists. The Data Journalism Handbook encourages journalists to treat data as a source and to pick up their computer to try new ways of reporting.  This session highlights key lessons from the book, including a) getting stories from data (big or small) b) business models for data driven newsrooms and c) how to get started.
Data, data everywhere... from our unique experience providing social data to hundreds of customers, we have learned the biggest problems you'll encounter when you get all the data you wish for. In this session, we'll share these problems, some solutions and how we meet our own challenges in dealing with massive flows of social data
In this talk, I will look at the next step in big data in general and open data in particular: transparency of insight and how the intelligent transformation of data into narratives can bring to light the stories within it and enable the higher level of understanding and insight needed to support evidence-based decision-making.
Strategy has changed. The step-change in data abundance, speed and competition means that static business plans striving for that 'perfect answer' are obsolete. We'll demonstrate how the Data Science underpinning race strategy engines used in Formula One to plan, track and update strategy in real-time are enabling Fortune 500 be more agile, and creating a new way of strategy planning.
Spreadsheets are used almost everywhere, for almost everything. Researchers from Delft University of Technology have studied spreadsheet users and their spreadsheets to learn more on how exactly they are built, maintained and migrated. In this session we present a case study concerning the analysis of 3 million spreadsheets we analyzed.
Program Chairs, Edd Dumbill and Kaitlin Thaney, welcome you to Strata in London
Liam Maxwell, Executive Director of the IT Reform Group in the Cabinet Office
For all of our machine learning algorithms and big data tools, so many of the problems we solve day-to-day are decidedly "first world": figuring out how to get the biggest ROI on ad dollars or crafting personalized movie recommendations.  Can we use our skills as data scientists to solve social problems as well, helping people find clean water as easily as they can find good restaurants?
What will Big Data mean to us as users, consumers and organisations?  And will it really be a big deal? In this presentation Mikael Bisgaard-Bohr will provide a fascinating view into where the Big Data wave is taking us, and why it is about so much more than just data.
Mapping real-world correspondence to data structures populating a storage matrix currently expanding by some 5 trillion bits per second is the challenge that brings us here.
Data provides critical insight into the way government works. When the UK government published every item of spending over 25,000, the data was hard to parse. The UK Guardian Datablog cleaned it up and asked readers to help pore through the numbers, making everyone a data journalist. Well cover the technologies the Guardian uses to analyze, visualize, and share data with the world.
Nobody knows statistics. They are as esoteric as chemical compounds are to chemistry. Yet data visualizations often incorporate a logarithmic scale, density traces, or seasonally adjusted numbers among other things. If this is the data deluge, we're bound to find everyone swept downstream. How do we prepare the average data consumer?
The real challenge ahead of us is not accumulating more information, or processing more information, or analytics, or replacing relational databases, or scaling data (i.e. not the 3 Vs). The real challenge is solving the information glut problem.
Everyone uses the term big data but no on can agree on what it means or even if it's novel. However the label is useful to describe the radically new ways that the world interacts with information - for which the public, policymakers and even data geeks, are unprepared.
Program Chairs, Edd Dumbill and Kaitlin Thaney, welcome you to the second day of Strata in London keynotes.
It's 1951 and you've got the world's first business computer and you've just been handed a Big Data problem. Go!
Big data isn't just multi-terabyte datasets hidden inside eventually-concurrent distributed databases in the cloud. Its also about the hidden data you carry with you all the time, data that is generated for you and about you, but not necessarily by you. Hidden data, your data, carrying on its secret life without your knowledge, but with your implicit and implied consent.
Alexandra Deschamps-Sonsino, Founder of Good Night Lamp / Founder of Designswarm
Data is great. Data is powerful. But when some data is missing, bias can be introduced, distorting the overall picture.
Jeni Tennison, Technical Director of the newly formed Open Data Institute, will describe the ODIs twin aims of helping data owners achieve their organisational objectives through publishing open data, and helping those who reuse that data to add value responsibly and effectively, thereby turning open data dreams into reality.
A practical step-by-step description of how the LAMP based Top10 Alpha was turned into fully data-driven product. Based around a real-time data processing pipeline and asynchronous stack, Top10's infrastructure now hinges on AKKA, along with Scala, Nodejs and a host of other technologies. This has enabled interesting uses of the data and new, exciting user-facing features.
Big data often doesn't sit well with companies that want to move fast. Technologies like Hadoop can be expensive to setup, slow to produce results, and time consuming to maintain. Streaming algorithms provide an alternative. They are simple to implement, very efficient, and give real-time results. In this talk I will describe several key streaming algorithms, and give examples of their use.
Data Science projects are difficult to realise as they require both mathematical and IT abstractions at once.  We need databases, linear algebra, message queues... all at once.  Traditional environments like Java/C#/Matlab/Mathematica provide only one. I will talk about the new language, Clojure, provides all the platform power of the JVM, as well as the language and libraries to do data science.
Logic programming recently gained new interest with people processing large data volumes with Hadoop. This talk demonstrates the basic concepts by using Cascalog.
This presentation will give an overview of mapreduce-based algorithms described in recent papers written by academic and industrial researchers. Included areas: AI/Machine Learning, Bioinformatics, Information Retrieval. Focus will be on patterns of problems and the corresponding mapreduce solution patterns. Some background material: http://mapreducepatterns.org
As open data and linked data communities grow, so do the number and average size of freely available datasets. Often these datasets are modelled and interlinked using RDF. This talk shares tips and tricks, use cases and practical examples of how to effectively use tools from the Hadoop ecosystem to process large RDF datasets.
A practical step-by-step description of how the LAMP based Top10 Alpha was turned into fully data-driven product. Based around a real-time data processing pipeline and asynchronous stack, Top10's infrastructure now hinges on AKKA, along with Scala, Nodejs and a host of other technologies. This has enabled interesting uses of the data and new, exciting user-facing features.
Big data often doesn't sit well with companies that want to move fast. Technologies like Hadoop can be expensive to setup, slow to produce results, and time consuming to maintain. Streaming algorithms provide an alternative. They are simple to implement, very efficient, and give real-time results. In this talk I will describe several key streaming algorithms, and give examples of their use.
Data Science projects are difficult to realise as they require both mathematical and IT abstractions at once.  We need databases, linear algebra, message queues... all at once.  Traditional environments like Java/C#/Matlab/Mathematica provide only one. I will talk about the new language, Clojure, provides all the platform power of the JVM, as well as the language and libraries to do data science.
Logic programming recently gained new interest with people processing large data volumes with Hadoop. This talk demonstrates the basic concepts by using Cascalog.
This presentation will give an overview of mapreduce-based algorithms described in recent papers written by academic and industrial researchers. Included areas: AI/Machine Learning, Bioinformatics, Information Retrieval. Focus will be on patterns of problems and the corresponding mapreduce solution patterns. Some background material: http://mapreducepatterns.org
As open data and linked data communities grow, so do the number and average size of freely available datasets. Often these datasets are modelled and interlinked using RDF. This talk shares tips and tricks, use cases and practical examples of how to effectively use tools from the Hadoop ecosystem to process large RDF datasets.
Failing software projects already is easier than we'd love to admit. When dealing with big data - a topic hyped quite a bit - the chance of projects failing miserably are even higher. This talk highlights some of the most prominent anti-patterns when dealing with data analysis, scaling and data science.
In this session well discuss our experience extending Hadoop development to new platforms and languages, and key aspects of using non-JVM languages in the Hadoop environment.
When I left Last.fm to join Massive Media, I basically moved from a data science forerunner to a newcomer. I had to evaluate everything I learned and start over completely with a clean slate, which resulted in a pretty clear perspective on how to find good data scientists, what they should be doing, what tools they should be using, and how to organize them to work together efficiently as team.
Googles Dremel is a scalable, interactive ad-hoc query system capable of running SQL-like queries over trillion-row tables in seconds. BigQuery is the externalization of this technology as a REST API and web app. This session will discuss the capabilities of Dremel and dive into the design challenges necessary to make this technology accessible and performant for developers and business users.
Late last summer, Etsy made a seemingly innocuous change to its search engine that had far reaching impact. The change was coordinated with three major data-driven product launches, from search to advertising to analytics. Big data can cause big changes, and this talk focuses on big data from an end-to-end product view, ranging from the underlying technology to understanding longer-term impacts.
Establishing cause and effect from observational data is extremely difficult. However by introducing randomization, or better still, controlled experiments, it becomes possible to establish true causality. This talk will survey the the difficulties and pitfalls of establishing cause and effect from observed data, and explain ways to introduce experimentation.
Social games are the poster children of  metrics-driven design.  The way that analytics is used to optimise design for games has lessons which are transferable to other domains.  But even poster children have problems.  We look at the landscape of analytical tools designed to support game design refinement, identify the main pitfalls involved in practice, and suggest workarounds.
When constructing a music recommender system, which is more important: a musicological understanding of the catalog of music in a system or the number of times two particular songs were played one after the other and were `liked?   Even better, if a system knows the latter, does the former even matter?  Do machines that predict behavior need to learn to listen? Or is observing behavior enough?
Failures in the datacentre can threaten the availability and data in your Hadoop cluster unless you have strategies to reduce this risk. This talk uses real customer data to introduce the threats to data integrity and availability -and shows how to a minimize the risks.
A guide to how real world companies have architected their big data ecosystems, incorporating Hadoop, NoSQL and data warehouse technologies.
Apache Hadoop 2 has a new MapReduce engine, which is built on a new general resource management system for running distributed applications called YARN. This talk explains the architecture of YARN, and discusses what this means to users of MapReduce and related frameworks, and to developers writing new parallel processing applications.
Nearest neighbor (k-nn for short) models are conceptually just about the simplest kind of behavioral model possible but are generally considered infeasible for production.  This talk will describe the knn project and how it can reduce thousand-year computations to a few hours or make real-time use of k-nn models practical.  Practical results will be shown and implementation methods described.
Observing how other humans interact is so interesting that we do it recreationally, we call it "people watching". Evolution has equipped us both with a desire to people watch, and with the tools we need to do it, but it's hard to describe what it is we're doing. If we could, we could make our machines people watch for us, potentially yielding novel insights into our own social interactions.
Masters at web scraping and data journalism from ScraperWiki tell tales and give practical advice from years of cleaning data. What are common gotchas when fixing up data before you make it do something, and how do you get round them? Illustrated with real examples from the world of journalism and business.
Failing software projects already is easier than we'd love to admit. When dealing with big data - a topic hyped quite a bit - the chance of projects failing miserably are even higher. This talk highlights some of the most prominent anti-patterns when dealing with data analysis, scaling and data science.
In this session well discuss our experience extending Hadoop development to new platforms and languages, and key aspects of using non-JVM languages in the Hadoop environment.
When I left Last.fm to join Massive Media, I basically moved from a data science forerunner to a newcomer. I had to evaluate everything I learned and start over completely with a clean slate, which resulted in a pretty clear perspective on how to find good data scientists, what they should be doing, what tools they should be using, and how to organize them to work together efficiently as team.
Googles Dremel is a scalable, interactive ad-hoc query system capable of running SQL-like queries over trillion-row tables in seconds. BigQuery is the externalization of this technology as a REST API and web app. This session will discuss the capabilities of Dremel and dive into the design challenges necessary to make this technology accessible and performant for developers and business users.
Late last summer, Etsy made a seemingly innocuous change to its search engine that had far reaching impact. The change was coordinated with three major data-driven product launches, from search to advertising to analytics. Big data can cause big changes, and this talk focuses on big data from an end-to-end product view, ranging from the underlying technology to understanding longer-term impacts.
Establishing cause and effect from observational data is extremely difficult. However by introducing randomization, or better still, controlled experiments, it becomes possible to establish true causality. This talk will survey the the difficulties and pitfalls of establishing cause and effect from observed data, and explain ways to introduce experimentation.
Social games are the poster children of  metrics-driven design.  The way that analytics is used to optimise design for games has lessons which are transferable to other domains.  But even poster children have problems.  We look at the landscape of analytical tools designed to support game design refinement, identify the main pitfalls involved in practice, and suggest workarounds.
When constructing a music recommender system, which is more important: a musicological understanding of the catalog of music in a system or the number of times two particular songs were played one after the other and were `liked?   Even better, if a system knows the latter, does the former even matter?  Do machines that predict behavior need to learn to listen? Or is observing behavior enough?
Failures in the datacentre can threaten the availability and data in your Hadoop cluster unless you have strategies to reduce this risk. This talk uses real customer data to introduce the threats to data integrity and availability -and shows how to a minimize the risks.
A guide to how real world companies have architected their big data ecosystems, incorporating Hadoop, NoSQL and data warehouse technologies.
Apache Hadoop 2 has a new MapReduce engine, which is built on a new general resource management system for running distributed applications called YARN. This talk explains the architecture of YARN, and discusses what this means to users of MapReduce and related frameworks, and to developers writing new parallel processing applications.
Nearest neighbor (k-nn for short) models are conceptually just about the simplest kind of behavioral model possible but are generally considered infeasible for production.  This talk will describe the knn project and how it can reduce thousand-year computations to a few hours or make real-time use of k-nn models practical.  Practical results will be shown and implementation methods described.
Observing how other humans interact is so interesting that we do it recreationally, we call it "people watching". Evolution has equipped us both with a desire to people watch, and with the tools we need to do it, but it's hard to describe what it is we're doing. If we could, we could make our machines people watch for us, potentially yielding novel insights into our own social interactions.
Masters at web scraping and data journalism from ScraperWiki tell tales and give practical advice from years of cleaning data. What are common gotchas when fixing up data before you make it do something, and how do you get round them? Illustrated with real examples from the world of journalism and business.
The furore over phone hacking has again led to demands for a privacy law, whilst media scorn at super-injunctions protecting celebrities from sex scandals pulls the debate in the other direction.    But the average Joe/sephine seems happy to populate their social media presence with increasing amounts of detail.  Is privacy only a concern of the famous?  And should they be shaping law and policy?
To what public sector problems is open data the solution? This talk will describe how opening data has allowed The National Archives to introduce a new operating model for revising legislation, updating the government's legislation database, bringing private investment in to improve open, public and free, legislation data. It will describe the operating model and technology behind this approach.
Does pre-competetive collaboration ease the pain of adopting disruptive big-data technologies? This question is tacked using the example of management/analysis of large genomic sequence data sets, and their role in the development of personalised medicine.
Overview of cutting edge research from UCL's Centre for Advanced Spatial Analysis.
emoto is a unique data art project that sets out to visualise the worldwide emotional response to the Olympic Games 2012. We track social media sites for emotional status messages related to the Games and visualise the crowd's reponse in real-time and in an aggregate data sculpture. This talk will show behind the scenes material on how to master real-time large scale social media mining.
Readers and preparers of graphs: Learn to recognize and avoid some common graphical mistakes to understand your data better and make better decisions from data.
Enabling data as an asset and matching it to the needs of customers and/or citizens is a problem. There is a need for careful designed feature sets and conclusive usability in data applications. The talk is about success and failure in producing data application for newspapers and building an Open Data-friendly geo-data-startup.
What are the new tools to help us make useful products and media from Big Data? How do we re-tool the methods from traditional User Experience Design and create a new discipline of Data Experience Design? How do we involve business, tech and user needs to create novel and useful experiences?
Data-intensive scientific communication is broken. Ironically, the components necessary for open and executable science exist in isolation. clearScience is a pilot at Sage Bionetworks to assemble these componentsdata, code, and compute infrastructure into a stack that not only facilitates effective reporting of science, but delivery of the science itself.
In this session Ill present recent work and research, and discuss how subjectivity can be used to create subtle and emotive, physical data-driven artwork that demonstrate patterns in data.
Through big data technologies we can now begin to consider the machine as an active participant in our experiences and decisions - true collaborators. This talk will discuss the foundations of creativity and intuition, show examples of how machines are augmenting our decisions today and the roles they will play in the future, and explore how our traditional interfaces will disappear as a result.
You want to publish your data for clients, developers or the general public to use and enjoy. But which file formats to use? Which standards? How to provide an API? Should you visualize the data? And if so, how? DataMarket has been on the receiving end of data from many of the World's key data providers and is now helping leading information companies publishing theirs. Here we share our findings.
Learn how the Miso Project - an open source toolkit - can help build engaging interactive content and let authors focus on telling stories with data.
Web-scale citizen science such as Zooniverse (www.zooniverse.org) has provided a temporary solution to the flood of data that confronts researchers of 21st century, however the solution is a short-term one. In this presentation I will outline a potential strategy for combining a large web community and significant compute resources to create a scalable, intelligent classification engine.
The furore over phone hacking has again led to demands for a privacy law, whilst media scorn at super-injunctions protecting celebrities from sex scandals pulls the debate in the other direction.    But the average Joe/sephine seems happy to populate their social media presence with increasing amounts of detail.  Is privacy only a concern of the famous?  And should they be shaping law and policy?
To what public sector problems is open data the solution? This talk will describe how opening data has allowed The National Archives to introduce a new operating model for revising legislation, updating the government's legislation database, bringing private investment in to improve open, public and free, legislation data. It will describe the operating model and technology behind this approach.
Does pre-competetive collaboration ease the pain of adopting disruptive big-data technologies? This question is tacked using the example of management/analysis of large genomic sequence data sets, and their role in the development of personalised medicine.
Overview of cutting edge research from UCL's Centre for Advanced Spatial Analysis.
emoto is a unique data art project that sets out to visualise the worldwide emotional response to the Olympic Games 2012. We track social media sites for emotional status messages related to the Games and visualise the crowd's reponse in real-time and in an aggregate data sculpture. This talk will show behind the scenes material on how to master real-time large scale social media mining.
Readers and preparers of graphs: Learn to recognize and avoid some common graphical mistakes to understand your data better and make better decisions from data.
Enabling data as an asset and matching it to the needs of customers and/or citizens is a problem. There is a need for careful designed feature sets and conclusive usability in data applications. The talk is about success and failure in producing data application for newspapers and building an Open Data-friendly geo-data-startup.
What are the new tools to help us make useful products and media from Big Data? How do we re-tool the methods from traditional User Experience Design and create a new discipline of Data Experience Design? How do we involve business, tech and user needs to create novel and useful experiences?
Data-intensive scientific communication is broken. Ironically, the components necessary for open and executable science exist in isolation. clearScience is a pilot at Sage Bionetworks to assemble these componentsdata, code, and compute infrastructure into a stack that not only facilitates effective reporting of science, but delivery of the science itself.
In this session Ill present recent work and research, and discuss how subjectivity can be used to create subtle and emotive, physical data-driven artwork that demonstrate patterns in data.
Through big data technologies we can now begin to consider the machine as an active participant in our experiences and decisions - true collaborators. This talk will discuss the foundations of creativity and intuition, show examples of how machines are augmenting our decisions today and the roles they will play in the future, and explore how our traditional interfaces will disappear as a result.
You want to publish your data for clients, developers or the general public to use and enjoy. But which file formats to use? Which standards? How to provide an API? Should you visualize the data? And if so, how? DataMarket has been on the receiving end of data from many of the World's key data providers and is now helping leading information companies publishing theirs. Here we share our findings.
Learn how the Miso Project - an open source toolkit - can help build engaging interactive content and let authors focus on telling stories with data.
Web-scale citizen science such as Zooniverse (www.zooniverse.org) has provided a temporary solution to the flood of data that confronts researchers of 21st century, however the solution is a short-term one. In this presentation I will outline a potential strategy for combining a large web community and significant compute resources to create a scalable, intelligent classification engine.
With all of the data that is now available how do you put it at the fingertips of the analysts and derive real business value? In this session we will look at a blueprint for how to do just that and change the way your business works and thinks.
Massive analytics has emerged as an offshoot of Big Data with tremendous upside potential for businesses that can figure out how to manage that data.  CIOs must reduce the TCO to support massive data computation while enhancing analysis workflows. This session explores new capabilities for combined storage and computation with Hadoop MapReduce to solve todays Big Data challenges
In this talk Shaun Connolly, VP Corporate Strategy for Hortonworks, will look at Hadoop's opportunity and the value it can unlock. Along the way he will discuss the kind of efforts required from the community, the solution ecosystem, and the enterprise in order to solidify Hadoop's place within the enterprise.
In this talk Shaun Connolly, VP Corporate Strategy for Hortonworks, will look at Hadoop's opportunity and the value it can unlock. Along the way he will discuss the kind of efforts required from the community, the solution ecosystem, and the enterprise in order to solidify Hadoop's place within the enterprise.
Today's complex data is not only big, but also semi-structured and densely connected. In this session we'll look at how size, structure and connectedness have converged to transform the data landscape.
Oil exploration provides insight into the world of big data: huge data volumes have driven production for decades, and subsurface machine sensor data is being assimilated at ever-increasing rates. This example shows how a big data analytical ecosystem integrates relational decision support with the wild world of big data  here including seismic imaging and reservoir modeling  for exploitation.
This session is a must for Innovators, Business Sponsors and Data Scientists where you will hear about Greenplums Unified Analytics Platform  the only analytics focused collaboration environment in the world today. Learn how you can bring the best of your ideas to market sooner and truly transform you business.
This session will focus on going from a good visualization to a great visualization by focusing on organization, user interface, and formatting.
With all of the data that is now available how do you put it at the fingertips of the analysts and derive real business value? In this session we will look at a blueprint for how to do just that and change the way your business works and thinks.
Massive analytics has emerged as an offshoot of Big Data with tremendous upside potential for businesses that can figure out how to manage that data.  CIOs must reduce the TCO to support massive data computation while enhancing analysis workflows. This session explores new capabilities for combined storage and computation with Hadoop MapReduce to solve todays Big Data challenges
In this talk Shaun Connolly, VP Corporate Strategy for Hortonworks, will look at Hadoop's opportunity and the value it can unlock. Along the way he will discuss the kind of efforts required from the community, the solution ecosystem, and the enterprise in order to solidify Hadoop's place within the enterprise.
In this talk Shaun Connolly, VP Corporate Strategy for Hortonworks, will look at Hadoop's opportunity and the value it can unlock. Along the way he will discuss the kind of efforts required from the community, the solution ecosystem, and the enterprise in order to solidify Hadoop's place within the enterprise.
Today's complex data is not only big, but also semi-structured and densely connected. In this session we'll look at how size, structure and connectedness have converged to transform the data landscape.
Oil exploration provides insight into the world of big data: huge data volumes have driven production for decades, and subsurface machine sensor data is being assimilated at ever-increasing rates. This example shows how a big data analytical ecosystem integrates relational decision support with the wild world of big data  here including seismic imaging and reservoir modeling  for exploitation.
This session is a must for Innovators, Business Sponsors and Data Scientists where you will hear about Greenplums Unified Analytics Platform  the only analytics focused collaboration environment in the world today. Learn how you can bring the best of your ideas to market sooner and truly transform you business.
This session will focus on going from a good visualization to a great visualization by focusing on organization, user interface, and formatting.
Many data science and data analytics applications are written in Python or R, but developing and deploying these applications at scale or in production is a pain point for many users. We will discuss our new efforts to bridge the gap between familiar in-memory data tools and distributed data management systems using Python and Impala.
The talk covers the development of the O'Reilly Media Report, "Mapping big data: A data driven market report."
This presentation identifies some of the areas in data creation and analytics where we perpetuate the simplistic representation of the world. It uses queer theory to demonstrate alternative ways of creating and analyzing data to take non-normative cases into consideration.
Customer journey analytics systems of large corporations must handle a great volume of events on a daily basis. Apriori aggregation used by early systems often caused signal loss due to ever-changing customer activity rates. We will present a new method that identifies paths inherent in raw cross-channel data, and that captures traffic patterns via nodes of interest across all channels of data.
Real-time analytics are becoming increasingly important due to the large amount of data that is being created continuously. Drawing from our experiences in Huawei Noah's Ark Lab, we present StreamDM, a new open source data mining and machine learning library designed on top of Spark Streaming. We will show its advanced methods, and how easily it can be used and extended.
In this presentation I will describe the way in which Data Science is helping the Wall Street Journal produce better journalism strategies, personalize our subscribers experience, and optimize revenue and overall customer engagement.
In this talk, we will explain how data scientists use nested data structures to increase analytic productivity. We will use two well-known relational schemas - TPC-H and Twitter - to demonstrate how to simplify data science workloads with nested schemas. Also, we will outline best practices for converting flat relational schemas into nested ones, and give examples of data science-style analysis.
More users than ever are accessing web applications from multiple devices. When logged-out users receive mixed experiment treatments, weird and wacky results can start appearing in your experiment analyses. Find out what we've learned about this problem at Airbnb and how our data scientists and engineers teamed up to solve it.
Large datasets have large numbers of anomalies, and the challenge is not just identifying anomalies but rank ordering them to create alerts, so that data scientists can examine the most interesting ones.  We discuss three case studies that integrate machine learning and data engineering, and extract six techniques for identifying anomalies and ranking ordering them by their potential significance.
Reaching 100,000,000 antivirus users was a big challenge for Avira, but we managed to achieve the goal. The challenge that arises now is to convince our users to stay with us, by offering the best possible experience to each one of them. In this presentation we will share the entire flow of the user churn prevention, from building custom surveys to using machine learning algorithms.
Probabilistic programming has already revolutionized machine learning and will have a similar impact on the emerging field of data science. By automating the inference process, it dramatically increases the number of people who can build complex Bayesian models custom-made to the specific problem at hand; and makes experts vastly more effective in devising new machine learning methods.
Machine learning tools offer promise in helping solve data curation problems. While the principles are well-understood, the engineering details in configuring and deploying ML techniques are the biggest hurdle. Leveraging data semantics and domain-specific knowledge is key in delivering the optimizations necessary for truly scalable ML curation solutions.
Bayesian methods are well-suited for business applications because they provide concrete guidance for decision-making under uncertainty. But many data science teams lack the background to take advantage of these methods. In this presentation I will explain the advantages and suggest ways for teams to develop skills and add Bayesian methods to their toolkit.
Spatial analytics is often hampered by the arbitrary choice of units, allowing local heterogeneity to obscure true patterns. A new smart clustering technique lets us use large quantities of open municipal data to literally redraw city maps to reflect facts on the ground, not administrative boundaries. This talk will explain what smart clusters are and the promise they hold for urban science.
This talk describes the development of a machine learning model that infers Airbnb host preferences for accommodation requests based on their past behavior. The model is used to surface likely matches more prominently on Airbnbs search results. In our A/B testing the model showed about a 3.75% increase in booking conversion, resulting in many more trips on Airbnb.
Many data science and data analytics applications are written in Python or R, but developing and deploying these applications at scale or in production is a pain point for many users. We will discuss our new efforts to bridge the gap between familiar in-memory data tools and distributed data management systems using Python and Impala.
The talk covers the development of the O'Reilly Media Report, "Mapping big data: A data driven market report."
This presentation identifies some of the areas in data creation and analytics where we perpetuate the simplistic representation of the world. It uses queer theory to demonstrate alternative ways of creating and analyzing data to take non-normative cases into consideration.
Customer journey analytics systems of large corporations must handle a great volume of events on a daily basis. Apriori aggregation used by early systems often caused signal loss due to ever-changing customer activity rates. We will present a new method that identifies paths inherent in raw cross-channel data, and that captures traffic patterns via nodes of interest across all channels of data.
Real-time analytics are becoming increasingly important due to the large amount of data that is being created continuously. Drawing from our experiences in Huawei Noah's Ark Lab, we present StreamDM, a new open source data mining and machine learning library designed on top of Spark Streaming. We will show its advanced methods, and how easily it can be used and extended.
In this presentation I will describe the way in which Data Science is helping the Wall Street Journal produce better journalism strategies, personalize our subscribers experience, and optimize revenue and overall customer engagement.
In this talk, we will explain how data scientists use nested data structures to increase analytic productivity. We will use two well-known relational schemas - TPC-H and Twitter - to demonstrate how to simplify data science workloads with nested schemas. Also, we will outline best practices for converting flat relational schemas into nested ones, and give examples of data science-style analysis.
More users than ever are accessing web applications from multiple devices. When logged-out users receive mixed experiment treatments, weird and wacky results can start appearing in your experiment analyses. Find out what we've learned about this problem at Airbnb and how our data scientists and engineers teamed up to solve it.
Large datasets have large numbers of anomalies, and the challenge is not just identifying anomalies but rank ordering them to create alerts, so that data scientists can examine the most interesting ones.  We discuss three case studies that integrate machine learning and data engineering, and extract six techniques for identifying anomalies and ranking ordering them by their potential significance.
Reaching 100,000,000 antivirus users was a big challenge for Avira, but we managed to achieve the goal. The challenge that arises now is to convince our users to stay with us, by offering the best possible experience to each one of them. In this presentation we will share the entire flow of the user churn prevention, from building custom surveys to using machine learning algorithms.
Probabilistic programming has already revolutionized machine learning and will have a similar impact on the emerging field of data science. By automating the inference process, it dramatically increases the number of people who can build complex Bayesian models custom-made to the specific problem at hand; and makes experts vastly more effective in devising new machine learning methods.
Machine learning tools offer promise in helping solve data curation problems. While the principles are well-understood, the engineering details in configuring and deploying ML techniques are the biggest hurdle. Leveraging data semantics and domain-specific knowledge is key in delivering the optimizations necessary for truly scalable ML curation solutions.
Bayesian methods are well-suited for business applications because they provide concrete guidance for decision-making under uncertainty. But many data science teams lack the background to take advantage of these methods. In this presentation I will explain the advantages and suggest ways for teams to develop skills and add Bayesian methods to their toolkit.
Spatial analytics is often hampered by the arbitrary choice of units, allowing local heterogeneity to obscure true patterns. A new smart clustering technique lets us use large quantities of open municipal data to literally redraw city maps to reflect facts on the ground, not administrative boundaries. This talk will explain what smart clusters are and the promise they hold for urban science.
This talk describes the development of a machine learning model that infers Airbnb host preferences for accommodation requests based on their past behavior. The model is used to surface likely matches more prominently on Airbnbs search results. In our A/B testing the model showed about a 3.75% increase in booking conversion, resulting in many more trips on Airbnb.
All-Day: Strata's regular data science track has great talks with real-world experience from leading edge speakers. But we didn't just stop therewe added the Hardcore Data Science day to give you a chance to go even deeper. The Hardcore day will add new techniques and technologies to your data science toolbox, shared by leading data science practitioners from startups, industry, consulting...
The combination of data, technology, and analytics creates previously impossible business intelligence opportunities.   How well companies can capture and manage their data so that it can be easily and consistently queried will be a key differentiator in deriving commercial value from data.   Learn how Goldman is developing an enterprise platform to unify and manage data across the firm.
This talk takes a provocative stand: many metrics we cherish lose their value because the granularity of modern data collection enables us to identify and optimize toward hidden signals that used to be noise, and now come to the forefront. One such metric is the click-through rate in advertising, but the mechanism is ubiquitous and we should pay close attention to the mechanism at work.
A global record company and a force in the music business partnered with award-winning data innovation consulting firm Caserta Concepts to re-architect its core data platform, with a data framework based on AWS, EMR, Redshift, and other big data technologies. This session presents the architecture, technologies, and techniques used to achieve an agile data ingestion and analytics platform.
To anticipate who will succeed and invest wisely, investors spend a lot of time trying to understand the longer-term trends within an industry. In this panel discussion, well consider the big trends in big data, asking top-tier VCs to look over the horizon and discuss the visions they have two or more years in the future.
At BuzzFeed, a technology and media company, the question of virality of content via sharing dominates. Now, for the first time since the company was founded in 2006, data scientists can identify ways pieces of content spread across multiple social networks. In this paper, we present a close look into the way BuzzFeed defines and analyzes the virality of content.
Most people are familiar with the basic principles driving todays hottest big data and enterprise companies. But whats really going on underneath the hood? In this session, Kleiner Perkins Caufield & Byers General Partner Michael Abbott unboxes a variety of startups in the space to examine the technology, architecture, and innovations theyve harnessed to deliver superior products and services.
Two years ago Walmart eCommerce moved from a small Hadoop cluster to a big one (250 modes) and has since used Hadoop to consolidate 10 different websites, including Sams Club online, into one website. Walmart eCommerce stores use all the incoming data in one central Hadoop cluster, which is driving the companys focus to provide personalized, best-in-class customer experiences.
Pharmaceutical companies follow a highly structured process for the approval of medications. From a financial viewpoint, the binary occasion of a drugs passage offers a rare scientific opportunity: a well-defined, recurrent, and critical event spanning over multiple companies.  We will show that integrating multiple datatypes uncovers how drug passage influences the market, and vice versa.
How can a retailer discover that expensive handbags have a large upside in Lancaster, PA, a fact that doesn't fit demographic stereotypes? The answer lies in understanding customer choice, that what a customer buys is constrained and influenced by what they're offered. Explore a new approach to machine learning, which models customer choice patterns and preferences from sparse transactional data.
Karen Rubin has spent the last nine months exploring What would happen if you invested in women CEOs?" In doing so, she has developed an investment algorithm that invests in the women-led companies of the Fortune 1000. Based on a simulation run from 2002-2014, this algorithm would have outperformed the S&P 500 by more than 200%. In this talk she will share her algorithm and results.
It's 2015. We understand the technology - how to build functional data pipelines, analytics, and reporting. We have algorithms. We understand the culture issues of how to build a data-driven organization. This talk is about how to use these assets to imagine and create previously impossible products.
All-Day: Strata's regular data science track has great talks with real-world experience from leading edge speakers. But we didn't just stop therewe added the Hardcore Data Science day to give you a chance to go even deeper. The Hardcore day will add new techniques and technologies to your data science toolbox, shared by leading data science practitioners from startups, industry, consulting...
The combination of data, technology, and analytics creates previously impossible business intelligence opportunities.   How well companies can capture and manage their data so that it can be easily and consistently queried will be a key differentiator in deriving commercial value from data.   Learn how Goldman is developing an enterprise platform to unify and manage data across the firm.
This talk takes a provocative stand: many metrics we cherish lose their value because the granularity of modern data collection enables us to identify and optimize toward hidden signals that used to be noise, and now come to the forefront. One such metric is the click-through rate in advertising, but the mechanism is ubiquitous and we should pay close attention to the mechanism at work.
A global record company and a force in the music business partnered with award-winning data innovation consulting firm Caserta Concepts to re-architect its core data platform, with a data framework based on AWS, EMR, Redshift, and other big data technologies. This session presents the architecture, technologies, and techniques used to achieve an agile data ingestion and analytics platform.
To anticipate who will succeed and invest wisely, investors spend a lot of time trying to understand the longer-term trends within an industry. In this panel discussion, well consider the big trends in big data, asking top-tier VCs to look over the horizon and discuss the visions they have two or more years in the future.
At BuzzFeed, a technology and media company, the question of virality of content via sharing dominates. Now, for the first time since the company was founded in 2006, data scientists can identify ways pieces of content spread across multiple social networks. In this paper, we present a close look into the way BuzzFeed defines and analyzes the virality of content.
Most people are familiar with the basic principles driving todays hottest big data and enterprise companies. But whats really going on underneath the hood? In this session, Kleiner Perkins Caufield & Byers General Partner Michael Abbott unboxes a variety of startups in the space to examine the technology, architecture, and innovations theyve harnessed to deliver superior products and services.
Two years ago Walmart eCommerce moved from a small Hadoop cluster to a big one (250 modes) and has since used Hadoop to consolidate 10 different websites, including Sams Club online, into one website. Walmart eCommerce stores use all the incoming data in one central Hadoop cluster, which is driving the companys focus to provide personalized, best-in-class customer experiences.
Pharmaceutical companies follow a highly structured process for the approval of medications. From a financial viewpoint, the binary occasion of a drugs passage offers a rare scientific opportunity: a well-defined, recurrent, and critical event spanning over multiple companies.  We will show that integrating multiple datatypes uncovers how drug passage influences the market, and vice versa.
How can a retailer discover that expensive handbags have a large upside in Lancaster, PA, a fact that doesn't fit demographic stereotypes? The answer lies in understanding customer choice, that what a customer buys is constrained and influenced by what they're offered. Explore a new approach to machine learning, which models customer choice patterns and preferences from sparse transactional data.
Karen Rubin has spent the last nine months exploring What would happen if you invested in women CEOs?" In doing so, she has developed an investment algorithm that invests in the women-led companies of the Fortune 1000. Based on a simulation run from 2002-2014, this algorithm would have outperformed the S&P 500 by more than 200%. In this talk she will share her algorithm and results.
It's 2015. We understand the technology - how to build functional data pipelines, analytics, and reporting. We have algorithms. We understand the culture issues of how to build a data-driven organization. This talk is about how to use these assets to imagine and create previously impossible products.
Python has become an increasingly important part of the data engineer and analytic tool landscape. Pydata at Strata provides in-depth coverage of the tools and techniques gaining traction with the data audience, including IPython Notebook, NumPy/matplotlib for visualization, SciPy, scikit-learn, and how to scale Python performance, including how to handle large, distributed data sets.
Over the last year, my team has gone from being a Hadoop Infrastructure team that was constantly fixing problems and cleaning up messes, to declaring ourselves to be a Data Platform team, expanding into investigating new tools, teaching coworkers about big data, and consulting with other teams about how to meet their data needs.
Zymergen has industrialized the process of genome engineering to build microbes that produce chemicals at scale. High-throughput microbe development is driven by integrating machine learning and open source software for complex data storage, search, and bioinformatics. See how we built this futuristic vision for synthetic biology, and learn how NoSQL can power massive scale experimentation.
This presentation is a real-world case study about moving a large portfolio of batch analytical programs that process 30 billion or more transactions every day, from a proprietary MPP database appliance architecture to the Hadoop ecosystem in the cloud, leveraging Hive, Amazon EMR, and S3.
Modern data infrastructures operate on vast volumes of continuously produced data generated by independent channels. Enterprises such as consumer banks that have many such channels are starting to implement a single view of customers that can power all customer touchpoints.  In this session we present an architectural approach for implementing such a solution using a customer event hub.
Enterprise data warehouses have become a large cost center. As their data volumes grow, enterprises want to move their warehouses on to Hadoop. But it is not an easy task. How do you solve this problem? The speakers have designed and deployed large scale data warehouses on Hadoop. In this talk, they will examine the technical underpinnings of their solution with a real-world example.
Many workloads are being migrated from data warehouses to Hadoop; but without a good methodology, the migration process can be challenging. In this talk, well discuss such a methodology in detail: from cluster sizing, to query tuning, to production readiness.
Lockheed Martin builds unmanned and manned human space systems, which require systems that are tested for all possible conditions  even for unforeseen situations. We present a test system that is a learning system built on big data technologies, that supports the testing of the Orion Multi-Purpose Crew Vehicle being designed for long-duration, human-rated deep space exploration.
Saavn is the leading music streaming service in the South Asian market. This talk will focus on how we are leveraging data to adapt to very specific demands on the market. We will demonstrate how Hadoop, Kafka, and Storm came together to help us solve some of the challenges.
Scott and Ray will discuss a real-life use case from a large manufacturing company, where data was produced in remote factories faster than it could be sent through the internet. This session is an interactive discussion around how to resolve the issue of "big data, small internet."
Hadoop has evolved into a rich collection of technologies that enable a broad range of use cases.  However, the technology innovation has outpaced the skills of most developers. The open-source Cask Data Application Platform (CDAP) project was initiated to close this developer gap.  In this session, we will show how three different organizations utilized CDAP to deliver solutions on Hadoop.
In this project, we re-engineered a few barely-usable legacy solutions from the past, and made them viable again by exploiting the speed and performance of Hadoop platform-based execution.
Python has become an increasingly important part of the data engineer and analytic tool landscape. Pydata at Strata provides in-depth coverage of the tools and techniques gaining traction with the data audience, including IPython Notebook, NumPy/matplotlib for visualization, SciPy, scikit-learn, and how to scale Python performance, including how to handle large, distributed data sets.
Over the last year, my team has gone from being a Hadoop Infrastructure team that was constantly fixing problems and cleaning up messes, to declaring ourselves to be a Data Platform team, expanding into investigating new tools, teaching coworkers about big data, and consulting with other teams about how to meet their data needs.
Zymergen has industrialized the process of genome engineering to build microbes that produce chemicals at scale. High-throughput microbe development is driven by integrating machine learning and open source software for complex data storage, search, and bioinformatics. See how we built this futuristic vision for synthetic biology, and learn how NoSQL can power massive scale experimentation.
This presentation is a real-world case study about moving a large portfolio of batch analytical programs that process 30 billion or more transactions every day, from a proprietary MPP database appliance architecture to the Hadoop ecosystem in the cloud, leveraging Hive, Amazon EMR, and S3.
Modern data infrastructures operate on vast volumes of continuously produced data generated by independent channels. Enterprises such as consumer banks that have many such channels are starting to implement a single view of customers that can power all customer touchpoints.  In this session we present an architectural approach for implementing such a solution using a customer event hub.
Enterprise data warehouses have become a large cost center. As their data volumes grow, enterprises want to move their warehouses on to Hadoop. But it is not an easy task. How do you solve this problem? The speakers have designed and deployed large scale data warehouses on Hadoop. In this talk, they will examine the technical underpinnings of their solution with a real-world example.
Many workloads are being migrated from data warehouses to Hadoop; but without a good methodology, the migration process can be challenging. In this talk, well discuss such a methodology in detail: from cluster sizing, to query tuning, to production readiness.
Lockheed Martin builds unmanned and manned human space systems, which require systems that are tested for all possible conditions  even for unforeseen situations. We present a test system that is a learning system built on big data technologies, that supports the testing of the Orion Multi-Purpose Crew Vehicle being designed for long-duration, human-rated deep space exploration.
Saavn is the leading music streaming service in the South Asian market. This talk will focus on how we are leveraging data to adapt to very specific demands on the market. We will demonstrate how Hadoop, Kafka, and Storm came together to help us solve some of the challenges.
Scott and Ray will discuss a real-life use case from a large manufacturing company, where data was produced in remote factories faster than it could be sent through the internet. This session is an interactive discussion around how to resolve the issue of "big data, small internet."
Hadoop has evolved into a rich collection of technologies that enable a broad range of use cases.  However, the technology innovation has outpaced the skills of most developers. The open-source Cask Data Application Platform (CDAP) project was initiated to close this developer gap.  In this session, we will show how three different organizations utilized CDAP to deliver solutions on Hadoop.
In this project, we re-engineered a few barely-usable legacy solutions from the past, and made them viable again by exploiting the speed and performance of Hadoop platform-based execution.
All-day: For business strategists, marketers, product managers, and entrepreneurs, Data-Driven Business looks at how to use data to make better business decisions faster. Packed with case studies, panels, and eye-opening presentations, this fast-paced day focuses on how to solve today's thorniest business problems with big data. It's the missing MBA for a data-driven, always-on business world.
All-day: For business strategists, marketers, product managers, and entrepreneurs, Data-Driven Business looks at how to use data to make better business decisions faster. Packed with case studies, panels, and eye-opening presentations, this fast-paced day focuses on how to solve today's thorniest business problems with big data. It's the missing MBA for a data-driven, always-on business world.
From advanced visualization, collaboration, and reproducibility to data manipulation, R Day at Strata covers a raft of current topics that analysts and R users need to pay attention to. The R Day tutorials come from leading luminaries and R committers, the folks keeping the R ecosystem apace of the challenges facing analysts and others who work with data.
The flexibility and simplicity of JSON have made it one of the most common formats for data. Data engines need to be able to load, process, and query JSON and nested data types quickly and efficiently. There are multiple approaches to processing JSON data,  each with trade offs. In this session well compare and contrast the approaches taken by systems such as Hive, Drill, BigQuery, and others.
Hadoop is supremely flexible, but with that flexibility comes integration challenges. In this talk, we introduce a new service that eliminates the need for components to support individual file formats, handle security, perform auditing, and implement sophisticated IO scheduling and other common processing that is at the bottom of any computation.
This session will investigate the trade-offs between real-time transactional access and fast analytic performance in Hadoop, from the perspective of storage engine internals. We will discuss recent advances, evaluate benchmark results from current generation Hadoop technologies, and propose potential ways ahead for the Hadoop ecosystem to conquer its newest set of challenges.
In this session, attendees will learn how erasure coding (HDFS-7285) can greatly reduce the storage overhead of HDFS without sacrificing data reliability.
Hadoop gives the ability to keep all data together for shared use and analysis. People use Apache HBase for fast updates and low latency data access and Apache Hive for analytics. To improve sharing of this data, users need to be able to access their transactional and analytic data through one tool. This talk will cover work in the Hive, HBase, and Phoenix communities to deliver on this promise.
Even after 25 years, the TPC-C benchmark still sets the standard for online transaction processing (OLTP) database benchmarking. It has traditionally been the arena for RDBMSs like Oracle Database, IBM DB2, and Microsoft SQL Server to do battle. Now, for the first time, a Hadoop database has successfully completed TPC-C benchmarks. Can it change the equation for OLTP workload price/performance?
Columnar data formats such as Apache Parquet promise much in terms of performance, but need help from modern CPUs to fully realize all the benefits. In this talk we'll show how the combination of the newest SIMD instruction sets, and an open-source columnar file format, can provide an enormous performance advantage. Our example system will be Impala, Parquet, and Intel's AVX2 instruction set.
This session will delve into the multiple different meanings of "virtualized HDFS." It will lead an investigation into the abstraction of the HDFS protocol in order to permit any storage device to deliver data to a Hadoop application in a performance critical environment. It will include a discussion and assessment of the work in this area done by projects such as Tachyon and MemHDFS.
The HDFS File Browser now has improved accessibility and is easier to use! Hadoop 2.4.0 introduced a new UI for file browsing with WebHDFS. This feature set has been expanded to include write operations and file uploads. Authentication issues have been addressed and the file browser is now configured with HttpFS. We'll present a demonstration and overview of possible configuration requirements.
Data has gravity. Jim Gray once said that, compared to the cost of moving bytes around, everything else is free, and because of what this means for the economics of computing, the more data you have, the more it wants to be near other data. That means all big data systems, eventually, will live in centralized cloud environments. On the other hand, different data is processed in different ways.
Financial markets emanate massive amounts of data from which machines can, in principle, learn to invest with minimal initial guidance from humans. I contrast human and machine strengths and weaknesses in making investment decisions.
From advanced visualization, collaboration, and reproducibility to data manipulation, R Day at Strata covers a raft of current topics that analysts and R users need to pay attention to. The R Day tutorials come from leading luminaries and R committers, the folks keeping the R ecosystem apace of the challenges facing analysts and others who work with data.
The flexibility and simplicity of JSON have made it one of the most common formats for data. Data engines need to be able to load, process, and query JSON and nested data types quickly and efficiently. There are multiple approaches to processing JSON data,  each with trade offs. In this session well compare and contrast the approaches taken by systems such as Hive, Drill, BigQuery, and others.
Hadoop is supremely flexible, but with that flexibility comes integration challenges. In this talk, we introduce a new service that eliminates the need for components to support individual file formats, handle security, perform auditing, and implement sophisticated IO scheduling and other common processing that is at the bottom of any computation.
This session will investigate the trade-offs between real-time transactional access and fast analytic performance in Hadoop, from the perspective of storage engine internals. We will discuss recent advances, evaluate benchmark results from current generation Hadoop technologies, and propose potential ways ahead for the Hadoop ecosystem to conquer its newest set of challenges.
In this session, attendees will learn how erasure coding (HDFS-7285) can greatly reduce the storage overhead of HDFS without sacrificing data reliability.
Hadoop gives the ability to keep all data together for shared use and analysis. People use Apache HBase for fast updates and low latency data access and Apache Hive for analytics. To improve sharing of this data, users need to be able to access their transactional and analytic data through one tool. This talk will cover work in the Hive, HBase, and Phoenix communities to deliver on this promise.
Even after 25 years, the TPC-C benchmark still sets the standard for online transaction processing (OLTP) database benchmarking. It has traditionally been the arena for RDBMSs like Oracle Database, IBM DB2, and Microsoft SQL Server to do battle. Now, for the first time, a Hadoop database has successfully completed TPC-C benchmarks. Can it change the equation for OLTP workload price/performance?
Columnar data formats such as Apache Parquet promise much in terms of performance, but need help from modern CPUs to fully realize all the benefits. In this talk we'll show how the combination of the newest SIMD instruction sets, and an open-source columnar file format, can provide an enormous performance advantage. Our example system will be Impala, Parquet, and Intel's AVX2 instruction set.
This session will delve into the multiple different meanings of "virtualized HDFS." It will lead an investigation into the abstraction of the HDFS protocol in order to permit any storage device to deliver data to a Hadoop application in a performance critical environment. It will include a discussion and assessment of the work in this area done by projects such as Tachyon and MemHDFS.
The HDFS File Browser now has improved accessibility and is easier to use! Hadoop 2.4.0 introduced a new UI for file browsing with WebHDFS. This feature set has been expanded to include write operations and file uploads. Authentication issues have been addressed and the file browser is now configured with HttpFS. We'll present a demonstration and overview of possible configuration requirements.
Data has gravity. Jim Gray once said that, compared to the cost of moving bytes around, everything else is free, and because of what this means for the economics of computing, the more data you have, the more it wants to be near other data. That means all big data systems, eventually, will live in centralized cloud environments. On the other hand, different data is processed in different ways.
Financial markets emanate massive amounts of data from which machines can, in principle, learn to invest with minimal initial guidance from humans. I contrast human and machine strengths and weaknesses in making investment decisions.
As the Hadoop ecosystem grows more complex, there is widespread desire for open metadata solutions: common ground for collaboration across users, and interoperability across software solutions. We motivate a new class of open metadata services for big data, via science and enterprise use cases.  We also set out challenges for a new class of "meta-on-use" approaches fit for agile analytics.
Amazon Kinesis is a fully managed service for real-time streaming big data ingestion and processing. This talk explores Kinesis concepts in detail, including best practices for scaling your core streaming data ingestion pipeline. We then discuss building and deploying Kinesis processing applications using capabilities like Kinesis Client Libraries, AWS Lambda, and Amazon EMR (via Spark).
Tachyon is a memory-centric fault-tolerant distributed storage system, which enables reliable file sharing at memory-speed. It is open source and is deployed at multiple companies. In addition, Tachyon has more than 80 contributors from over 30 institutions. In this talk, we present Tachyon's architecture, performance evaluation, and several use cases we have seen in the real world.
Big data processing is challenged by four conflicting desires: latency, accuracy, simplicity, and cost. Google Cloud Dataflow intelligently merges the desired unified and open sourced programming model, backed by a fully managed cloud service. Dataflow enables developers to answer questions with the right level of latency and accuracy, with low operational overhead regardless of size/complexity.
Even the best data scientist can't do anything if they cannot easily get access to the necessary data. Simply making the data available is Step 1 toward becoming a data-driven organization. In this talk, we'll explore how Apache Kafka can replace slow, fragile ETL processes with real-time data pipelines, and discuss best practices for data formats and integration with existing systems.
This talk will cover how search and Solr have become a critical part of the Hadoop stack, and have also emerged as one of the highest performing solutions for analytics over big data. We'll also cover new analytics capabilities in Solr that marry full-text search, faceted search, statistics, and grouping, joining into a powerful engine for powering next-generation big data analytics applications.
The Netflix Data Platform is a constantly evolving, large scale infrastructure running in the (AWS) cloud. We are especially focused on performance and ease of use, with initiatives including Presto integration, Spark, and our big data portal and API. This talk will dive into the various technologies we use, the motivations behind our approach, and the business benefits we get.
Often the hardest step in processing streams is being able to collect all your data in a structured way. We present Copycat, a framework for data ingestion that addresses some common impedance mismatches between data sources and stream processing systems. Copycat uses Kafka as an intermediary, making it easy to get streaming, fault-tolerant data ingestion across a variety of data sources.
As companies increase the number of deployments of machine learning-based applications, the number of models that need to be monitored grow at a tremendous pace. In this talk, we outline some of the key challenges in large-scale deployments of machine learning models, then describe a methodology to manage such models in production to mitigate the technical debt.
MrGeo is a geospatial toolkit designed to provide raster-based geospatial capabilities that can be performed at scale by leveraging the Hadoop ecosystem.  This session will provide an overview of the MrGeo design for storing and processing large-scale raster datasets in the cloud, highlight core operations, and present performance benchmarks for some example operations on open data sets.
Recommendation engines are cognitive computing applications. Their algorithms learn from experience. What if a recommendation engine could help analysts sort through big data? Building a query recommendation engine is complex. Well share some of the technical challenges and learnings from building a cognitive application in daily use today, by analyst teams from eBay to Square.
As the Hadoop ecosystem grows more complex, there is widespread desire for open metadata solutions: common ground for collaboration across users, and interoperability across software solutions. We motivate a new class of open metadata services for big data, via science and enterprise use cases.  We also set out challenges for a new class of "meta-on-use" approaches fit for agile analytics.
Amazon Kinesis is a fully managed service for real-time streaming big data ingestion and processing. This talk explores Kinesis concepts in detail, including best practices for scaling your core streaming data ingestion pipeline. We then discuss building and deploying Kinesis processing applications using capabilities like Kinesis Client Libraries, AWS Lambda, and Amazon EMR (via Spark).
Tachyon is a memory-centric fault-tolerant distributed storage system, which enables reliable file sharing at memory-speed. It is open source and is deployed at multiple companies. In addition, Tachyon has more than 80 contributors from over 30 institutions. In this talk, we present Tachyon's architecture, performance evaluation, and several use cases we have seen in the real world.
Big data processing is challenged by four conflicting desires: latency, accuracy, simplicity, and cost. Google Cloud Dataflow intelligently merges the desired unified and open sourced programming model, backed by a fully managed cloud service. Dataflow enables developers to answer questions with the right level of latency and accuracy, with low operational overhead regardless of size/complexity.
Even the best data scientist can't do anything if they cannot easily get access to the necessary data. Simply making the data available is Step 1 toward becoming a data-driven organization. In this talk, we'll explore how Apache Kafka can replace slow, fragile ETL processes with real-time data pipelines, and discuss best practices for data formats and integration with existing systems.
This talk will cover how search and Solr have become a critical part of the Hadoop stack, and have also emerged as one of the highest performing solutions for analytics over big data. We'll also cover new analytics capabilities in Solr that marry full-text search, faceted search, statistics, and grouping, joining into a powerful engine for powering next-generation big data analytics applications.
The Netflix Data Platform is a constantly evolving, large scale infrastructure running in the (AWS) cloud. We are especially focused on performance and ease of use, with initiatives including Presto integration, Spark, and our big data portal and API. This talk will dive into the various technologies we use, the motivations behind our approach, and the business benefits we get.
Often the hardest step in processing streams is being able to collect all your data in a structured way. We present Copycat, a framework for data ingestion that addresses some common impedance mismatches between data sources and stream processing systems. Copycat uses Kafka as an intermediary, making it easy to get streaming, fault-tolerant data ingestion across a variety of data sources.
As companies increase the number of deployments of machine learning-based applications, the number of models that need to be monitored grow at a tremendous pace. In this talk, we outline some of the key challenges in large-scale deployments of machine learning models, then describe a methodology to manage such models in production to mitigate the technical debt.
MrGeo is a geospatial toolkit designed to provide raster-based geospatial capabilities that can be performed at scale by leveraging the Hadoop ecosystem.  This session will provide an overview of the MrGeo design for storing and processing large-scale raster datasets in the cloud, highlight core operations, and present performance benchmarks for some example operations on open data sets.
Recommendation engines are cognitive computing applications. Their algorithms learn from experience. What if a recommendation engine could help analysts sort through big data? Building a query recommendation engine is complex. Well share some of the technical challenges and learnings from building a cognitive application in daily use today, by analyst teams from eBay to Square.
This is a day to learn about the data innovations that have the potential to blindside even the most careful organizations. Aimed at decision makers, the Innovation + Growth program focuses on how data-oriented startups, academics, and venture capitalists approach innovation and the potential to disrupt incumbent business models.
This is a day to learn about the data innovations that have the potential to blindside even the most careful organizations. Aimed at decision makers, the Innovation + Growth program focuses on how data-oriented startups, academics, and venture capitalists approach innovation and the potential to disrupt incumbent business models.
In the last year Spark has seen substantial growth in adoption as well as the pace and scope of development. This talk will look forward and discuss both technical initiatives and the evolution of the Spark community.
R is the favorite language of many data scientists. In addition to a language and runtime, R is a rich ecosystem of libraries for a wide range of use cases from statistical inference to data visualization. However, handling large or distributed data with R is challenging. Hence R is used along with other frameworks and languages by most data scientist.
A revolution in DNA sequencing technology has led to exponential growth in the genomics data available to discover new drugs, diagnose patients, and understand the fundamental biology of human disease. Existing bioinformatics tools will have difficulty scaling to meet the challenges posed by this growth. Learn about next-generation tools for bioinformatics and genomics using Spark and Parquet.
In this talk we will show how Sony Mobile uses large scale analytics on Spark to generate insights to Lifelog users about themselves and the population, and how we use analytics to build a user lifecycle model that allows us to take actions toward increased user engagement and retention.
This session explores best practices of creating both unit and integration tests for Spark programs as well as acceptance tests for the data produced by our Spark jobs. We will explore the difficulties with testing streaming programs, options for setting up integration testing with Spark, and also examine best practices for acceptance tests.
How much can you expect to lose? The financial statistic Value at Risk seeks to answer this question, but is computationally intensive to estimate. At Cloudera, weve assisted several organizations in using Spark to compute VaR and other financial statistics. The talk, which walks through a basic VaR calculation, aims to give a feel for what it is like to approach financial modeling with Spark.
As the adoption of Spark Streaming in the industry is increasing, so is the community's demand for more features. Since the beginning of this year, we have made significant improvements in performance, usability, and semantic guarantees. In this talk, I discuss these improvements, as well as the features we plan to add in the near future.
The Big Data Platform team at Netflix continues to push big data processing in the cloud with the addition of Spark to our platform.  Recent enhancements to Spark allow us to effectively leverage it for processing against a 10+ petabyte warehouse backed by S3.  We will share our experiences and performance of production jobs along with the pains and gains of deploying Spark at scale on YARN.
Deep learning algorithms have been used in many real-world applications, such as computer vision, machine translation, and fraud detection. We'll present an overview of the system architecture, the training and running of Deep Learning models such as Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs) on Spark with Tachyon, including the use of GPUs to improve execution time.
Apache Spark is often seen as a replacement for MapReduce in Hadoop systems, but Spark clusters can also be deployed and managed by Mesos. This talk explains how to use Mesos for Spark applications. We'll examine the pros and cons of using Mesos vs. Hadoop YARN as a data platform, and discuss practical issues when running Spark on Mesos. We'll even discuss how to combine the two with Myriad.
Comcast uses Hadoop as the big data platform in several areas of its business.  Their use cases have evolved in recent years and include personalization, clickthru analytics, modeling, and customer support initiatives, all adding up to billions of dollars in revenue.
In the last year Spark has seen substantial growth in adoption as well as the pace and scope of development. This talk will look forward and discuss both technical initiatives and the evolution of the Spark community.
R is the favorite language of many data scientists. In addition to a language and runtime, R is a rich ecosystem of libraries for a wide range of use cases from statistical inference to data visualization. However, handling large or distributed data with R is challenging. Hence R is used along with other frameworks and languages by most data scientist.
A revolution in DNA sequencing technology has led to exponential growth in the genomics data available to discover new drugs, diagnose patients, and understand the fundamental biology of human disease. Existing bioinformatics tools will have difficulty scaling to meet the challenges posed by this growth. Learn about next-generation tools for bioinformatics and genomics using Spark and Parquet.
In this talk we will show how Sony Mobile uses large scale analytics on Spark to generate insights to Lifelog users about themselves and the population, and how we use analytics to build a user lifecycle model that allows us to take actions toward increased user engagement and retention.
This session explores best practices of creating both unit and integration tests for Spark programs as well as acceptance tests for the data produced by our Spark jobs. We will explore the difficulties with testing streaming programs, options for setting up integration testing with Spark, and also examine best practices for acceptance tests.
How much can you expect to lose? The financial statistic Value at Risk seeks to answer this question, but is computationally intensive to estimate. At Cloudera, weve assisted several organizations in using Spark to compute VaR and other financial statistics. The talk, which walks through a basic VaR calculation, aims to give a feel for what it is like to approach financial modeling with Spark.
As the adoption of Spark Streaming in the industry is increasing, so is the community's demand for more features. Since the beginning of this year, we have made significant improvements in performance, usability, and semantic guarantees. In this talk, I discuss these improvements, as well as the features we plan to add in the near future.
The Big Data Platform team at Netflix continues to push big data processing in the cloud with the addition of Spark to our platform.  Recent enhancements to Spark allow us to effectively leverage it for processing against a 10+ petabyte warehouse backed by S3.  We will share our experiences and performance of production jobs along with the pains and gains of deploying Spark at scale on YARN.
Deep learning algorithms have been used in many real-world applications, such as computer vision, machine translation, and fraud detection. We'll present an overview of the system architecture, the training and running of Deep Learning models such as Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs) on Spark with Tachyon, including the use of GPUs to improve execution time.
Apache Spark is often seen as a replacement for MapReduce in Hadoop systems, but Spark clusters can also be deployed and managed by Mesos. This talk explains how to use Mesos for Spark applications. We'll examine the pros and cons of using Mesos vs. Hadoop YARN as a data platform, and discuss practical issues when running Spark on Mesos. We'll even discuss how to combine the two with Myriad.
Comcast uses Hadoop as the big data platform in several areas of its business.  Their use cases have evolved in recent years and include personalization, clickthru analytics, modeling, and customer support initiatives, all adding up to billions of dollars in revenue.
Spark Camp provides a day long hands-on intro to the Spark platform including the core API, Spark SQL, Spark Streaming, MLlib, GraphX, and more. We will cover each Spark component through a series of technical talks targeted at developers who are new to Spark -- intermixed with hands-on lab work.
Spark Camp provides a day long hands-on intro to the Spark platform including the core API, Spark SQL, Spark Streaming, MLlib, GraphX, and more. We will cover each Spark component through a series of technical talks targeted at developers who are new to Spark -- intermixed with hands-on lab work.
Looking for a deeper understanding of how to architect real-time data processing solutions? Then this tutorial is for you. In Part 1 of "Architecture Day," We will build a fraud-detection system, and use it as an example to discuss considerations for building such a system; how youd integrate various technologies; and why those choices make sense for the use case in question.
Kafka provides the low latency, high throughput, high availability, and scale that financial services firms require. But can it also provide complete reliability? In this session, we will go over everything that happens to a message - from producer to consumer, and pinpoint all the places where data can be lost - if you are not careful.
Many people are acquiring smart devices, and yet do not have an understanding of the data these devices gather about them and what can be done with this data if it is aggregated over time.  The talk will demonstrate what data several popular devicesincluding the Nest Thermostat and a few othersgather and show what can be learned about an individual from this data.
This talk will present the design and implementation of a new system, called Heron, that is now the de facto stream data processing engine inside Twitter. Share our experiences in running Heron in production.
With the move to real-time data analytics and machine learning, streaming applications are becoming more relied upon than ever before. Discover how to build and deploy a globally scalable streaming system. This includes producing messages in one data center and consuming them in another data center, as well as how to make the guarantees that nothing is ever lost.
Over the past year, Spark Streaming has emerged as the leading platform to implement IoT and similar real-time use cases. This session includes a brief introduction to Spark Streamings micro-batch architecture for real-time stream processing, as well as a live demo of an example use case that includes processing and alerting on-time series data (such as sensor data).
Capturing and integrating device-based and other health data for research is frustratingly difficult. We explain the open source technology framework for capturing and routing device-based health data for use by healthcare providers and for access, via a trusted analytic container, to researchers we developed, working with OReilly Media and support from the Robert Wood Johnson Foundation.
By 2020, researchers estimate there will be 100 million internet connected devices. To process this data in real timewhether from mobile phones or jet engineswill be the new normal. How are companies today adapting to this new real time stream of data?
This talk introduces the landscape and challenges of predictive maintenance applications in the industry, illustrates how to formulate (data labeling and feature engineering) the problem with three machine learning models (regression, binary classification, multi-class classification), and showcases how the models can be conveniently trained and compared with different algorithms.
Using an open source technology stack, we implemented a solution for real-time analysis of sensor data from mining equipment. We will share the technical architecture used to show the tools we implemented for real-time complex event processing, why we implemented Spark instead of Storm, some of the challenges faced, benchmarks achieved, and tips for easy integration.
The maturation and development of open source technologies has made it easier than ever for companies to derive insights from vast quantities of data. In this session, we will cover how to build a real-time analytics stack using Kafka, Samza, and Druid. This combination of technologies can power a robust data pipeline that supports real-time ingestion and flexible, low-latency queries.
Oulu Smart City has a lively living lab tradition; we continuously collect data and expand our ecosystem of companies, research institutes, city officials, and citizens, and develop data-intensive services on top of the ecosystem. We present real use cases implementing  big data platforms and development of higher level distributed reasoning and machine learning to exploit our data lake.
In the second (afternoon) half of the Architecture Day tutorial, attendees will build a data application from the ground up. As a part of the tutorial, we will demonstrate how Kite codifies the best practices from the Hadoop Architecture Day morning session.
Looking for a deeper understanding of how to architect real-time data processing solutions? Then this tutorial is for you. In Part 1 of "Architecture Day," We will build a fraud-detection system, and use it as an example to discuss considerations for building such a system; how youd integrate various technologies; and why those choices make sense for the use case in question.
Kafka provides the low latency, high throughput, high availability, and scale that financial services firms require. But can it also provide complete reliability? In this session, we will go over everything that happens to a message - from producer to consumer, and pinpoint all the places where data can be lost - if you are not careful.
Many people are acquiring smart devices, and yet do not have an understanding of the data these devices gather about them and what can be done with this data if it is aggregated over time.  The talk will demonstrate what data several popular devicesincluding the Nest Thermostat and a few othersgather and show what can be learned about an individual from this data.
This talk will present the design and implementation of a new system, called Heron, that is now the de facto stream data processing engine inside Twitter. Share our experiences in running Heron in production.
With the move to real-time data analytics and machine learning, streaming applications are becoming more relied upon than ever before. Discover how to build and deploy a globally scalable streaming system. This includes producing messages in one data center and consuming them in another data center, as well as how to make the guarantees that nothing is ever lost.
Over the past year, Spark Streaming has emerged as the leading platform to implement IoT and similar real-time use cases. This session includes a brief introduction to Spark Streamings micro-batch architecture for real-time stream processing, as well as a live demo of an example use case that includes processing and alerting on-time series data (such as sensor data).
Capturing and integrating device-based and other health data for research is frustratingly difficult. We explain the open source technology framework for capturing and routing device-based health data for use by healthcare providers and for access, via a trusted analytic container, to researchers we developed, working with OReilly Media and support from the Robert Wood Johnson Foundation.
By 2020, researchers estimate there will be 100 million internet connected devices. To process this data in real timewhether from mobile phones or jet engineswill be the new normal. How are companies today adapting to this new real time stream of data?
This talk introduces the landscape and challenges of predictive maintenance applications in the industry, illustrates how to formulate (data labeling and feature engineering) the problem with three machine learning models (regression, binary classification, multi-class classification), and showcases how the models can be conveniently trained and compared with different algorithms.
Using an open source technology stack, we implemented a solution for real-time analysis of sensor data from mining equipment. We will share the technical architecture used to show the tools we implemented for real-time complex event processing, why we implemented Spark instead of Storm, some of the challenges faced, benchmarks achieved, and tips for easy integration.
The maturation and development of open source technologies has made it easier than ever for companies to derive insights from vast quantities of data. In this session, we will cover how to build a real-time analytics stack using Kafka, Samza, and Druid. This combination of technologies can power a robust data pipeline that supports real-time ingestion and flexible, low-latency queries.
Oulu Smart City has a lively living lab tradition; we continuously collect data and expand our ecosystem of companies, research institutes, city officials, and citizens, and develop data-intensive services on top of the ecosystem. We present real use cases implementing  big data platforms and development of higher level distributed reasoning and machine learning to exploit our data lake.
In the second (afternoon) half of the Architecture Day tutorial, attendees will build a data application from the ground up. As a part of the tutorial, we will demonstrate how Kite codifies the best practices from the Hadoop Architecture Day morning session.
In this tutorial, attendees will get a taste of how large-scale data science techniques and technologies developed for the consumer internet can be applied in the world of finance. We will guide an exploration of the relationship between the traffic on Wikipedia pages to the movement of stock prices.
What are the essential components of a data platform? This tutorial will explain how the various parts of the Hadoop and big data ecosystem fit together in production to create a data platform supporting batch, interactive, and real-time analytical workloads.
Direct visual exploratory analysis of big data yields insights that are otherwise overlooked. By plotting all the data, patterns that can be obscured by traditional visualization methods are preserved. This presentation highlights the power of visualizing whole data sets through examining a market order book and identifying pricing strategies.
This session will demonstrate how data enables people to overcome their disabilities and live to their fullest. We will also point out critical underlying flaws of data interpretation (due to human bias), and offer action items for us to make the data world more inclusive, efficient, and connected.
7digital power a variety of music services with a diverse range of territories, devices and access models. They have been helping services transform the listening experience through visualising their data. Paul will demonstrate visualisations on listening bounce rate and content classification, giving examples of how these creative solutions to conveying information have helped engage people...
You no longer need to be a remote sensing specialist to leverage real-time geospatial data from space. You don't need to be an expert to harvest social media on the cheap. Geospatial data analysis is a mixing pot that brings together your private data and streams of data from all over. We will talk about how we are bringing this mixing pot together for the future of understanding data.
This panel brings together founders and technologists who live on the cutting edge of music science. Well look at the Turing problems of digital entertainment, as well as how providers strike a balance between human curation and machine optimization.
Data is all science, no art. Think of a film that inspired or moved you. Now imagine the filmmaker decided that instead of making the film, they would present the material to you in the form of a graph or a chart. Thats where we are with data.
The talk will focus on considerations for designing data visualizations for data profiling required in data preparation; and considerations for designing data visualizations for later exploratory analysis and consumption phases of the overall analysis process.
Data-driven decision-making can only be properly executed when the decision makers understand both the underlying data, and the types of manipulations that have been applied to it. In this session, well explore what exactly we "do" to data (aggregation, "cleaning," statistical modeling, machine learning), and how to visually communicate about the processes and implications of our work.
Linked Immersive Visualization Environments (LIVE) is a framework that my startup, LiquidLandscape, has developed for combining multiple, high-volume data visualizations (d3, WebGL, WebVR) to provide comprehensive situational awareness for financial markets. We will discuss architecture and design challenges of visualizing real-time data at speed and scale, with lots of visual examples.
The experience of data extends beyond capturing, storing, and presenting it. Data can help shape customer journeys through products, change the way organizations communicate, and be either a source of confusion or tool for communication. This talk will focus on how design thinking can be applied to data, and how data design can be applied to a wide array of consumer and organizational experiences.
Our understanding of happiness is becoming more nuanced, and much of that new knowledge relies on data from social media, quantified self apps, and large datasets. This session will look at the lessons we can learn from happiness data to design positive experiences with technology.
In this tutorial, attendees will get a taste of how large-scale data science techniques and technologies developed for the consumer internet can be applied in the world of finance. We will guide an exploration of the relationship between the traffic on Wikipedia pages to the movement of stock prices.
What are the essential components of a data platform? This tutorial will explain how the various parts of the Hadoop and big data ecosystem fit together in production to create a data platform supporting batch, interactive, and real-time analytical workloads.
Direct visual exploratory analysis of big data yields insights that are otherwise overlooked. By plotting all the data, patterns that can be obscured by traditional visualization methods are preserved. This presentation highlights the power of visualizing whole data sets through examining a market order book and identifying pricing strategies.
This session will demonstrate how data enables people to overcome their disabilities and live to their fullest. We will also point out critical underlying flaws of data interpretation (due to human bias), and offer action items for us to make the data world more inclusive, efficient, and connected.
7digital power a variety of music services with a diverse range of territories, devices and access models. They have been helping services transform the listening experience through visualising their data. Paul will demonstrate visualisations on listening bounce rate and content classification, giving examples of how these creative solutions to conveying information have helped engage people...
You no longer need to be a remote sensing specialist to leverage real-time geospatial data from space. You don't need to be an expert to harvest social media on the cheap. Geospatial data analysis is a mixing pot that brings together your private data and streams of data from all over. We will talk about how we are bringing this mixing pot together for the future of understanding data.
This panel brings together founders and technologists who live on the cutting edge of music science. Well look at the Turing problems of digital entertainment, as well as how providers strike a balance between human curation and machine optimization.
Data is all science, no art. Think of a film that inspired or moved you. Now imagine the filmmaker decided that instead of making the film, they would present the material to you in the form of a graph or a chart. Thats where we are with data.
The talk will focus on considerations for designing data visualizations for data profiling required in data preparation; and considerations for designing data visualizations for later exploratory analysis and consumption phases of the overall analysis process.
Data-driven decision-making can only be properly executed when the decision makers understand both the underlying data, and the types of manipulations that have been applied to it. In this session, well explore what exactly we "do" to data (aggregation, "cleaning," statistical modeling, machine learning), and how to visually communicate about the processes and implications of our work.
Linked Immersive Visualization Environments (LIVE) is a framework that my startup, LiquidLandscape, has developed for combining multiple, high-volume data visualizations (d3, WebGL, WebVR) to provide comprehensive situational awareness for financial markets. We will discuss architecture and design challenges of visualizing real-time data at speed and scale, with lots of visual examples.
The experience of data extends beyond capturing, storing, and presenting it. Data can help shape customer journeys through products, change the way organizations communicate, and be either a source of confusion or tool for communication. This talk will focus on how design thinking can be applied to data, and how data design can be applied to a wide array of consumer and organizational experiences.
Our understanding of happiness is becoming more nuanced, and much of that new knowledge relies on data from social media, quantified self apps, and large datasets. This session will look at the lessons we can learn from happiness data to design positive experiences with technology.
This is a hands-on workshop where youll learn how to leverage the capabilities of Kafka to collect, manage, and process stream data for big data projects and general purpose enterprise data integration needs alike. When your data is captured in real-time and available as real-time subscriptions, you can start to compute new datasets in real-time off these original feeds.
This tutorial is all about managing large volumes of data coming at your data center fast and continuously. If you don't have a strategy, then allow me to help. Amazing Apache Project software can make this problem a lot easier to deal with. Spend a few hours and learn about how each part works, and how they work together. Your users will thank you.
Ethical concerns about the use of personal information in new ways has led to calls for the creation of consumer subject review boards, which could evaluate, approve, or monitor out-of-context uses of information absent user consent. This conversation between a philosopher and lawyer will address how organizations can use existing ethical frameworks to create practical accountability mechanisms.
No matter how good the intentions, ethical questions are inherent in the work of using data for social good. How are organizations navigating ethical pitfalls in order to make an impact? The key is protecting the humanity behind the numbers. In this series of talks, we'll learn how organizations are dealing with ethical considerations inherent in projects that aim to use data for good.
No matter how good the intentions, ethical questions are inherent in the work of using data for social good. How are organizations navigating ethical pitfalls in order to make an impact? The key is protecting the humanity behind the numbers. In this series of talks, we'll hear from four speakers on how they are dealing with ethical considerations inherent in projects that aim to use data for good.
Because of the way sentiment analysis algorithms are trained, they systematically amplify the voices of those who express themselves unsubtly and aggressively. I will extrapolate from this observation to show the ways in which supervised machine learning has the potential to amplify social and economic privilege.
Who will watch the watchmen? This session will cover data integrity problems in open government introduced by the human element. Well then explore possible methodologies that will allow us to derive value from open government data, while still keeping a skeptical eye on the validity of the data itself.
Technology offers amazing big data use cases, but according to Gartner it's important to avoid "crossing the creepy line." Governance and security experts from Cloudera and MasterCard discuss the legal and ethical usage of big data. Ethical behavior drives trust - they are inseparably linked. For customers to trust and continue to do business with us requires an ethical data usage framework.
LinkedIns Security Data Science group uses various reputation systems as input to models designed to stop fraud and abuse. This session will discuss how we build these reputation systems and compare instantaneous online reputation scores to more complex offline systems.
Hadoop is widely used thanks to its ability to handle volume, velocity, and variety of data. However, this flexibility and scale presents challenges for securing and governing this data. To avoid your company making the front pages over a data breach, experts from MasterCard, Intel, and Cloudera share the Hadoop Security Maturity Model phase 0-4 and steps to get your cluster ready for a PCI audit.
Combining data in Hadoop for the purpose of data discovery often runs into barriers from the security group because of legal or corporate policy. This talk will discuss the challenges with implementing data governance in big data systems, a design pattern for addressing those challenges within an organization, and a recent case study.
Encryption is a requirement for many business sectors dealing with confidential information. To meet these requirements, transparent, end-to-end encryption was added to HDFS. This protects data while it is in-flight and at-rest, and can be used compatibly with existing Hadoop apps. We will cover the design and implementation of transparent encryption in HDFS, as well as performance results.
Moderator: Steve Totman, Big Data Evangelist at Cloudera Panelist: Kristi Cunningham, VP Enterprise Data Management at Capital One Panelist: Susan Meyer, Business Leader - Fraud Management Solutions at MasterCard Worldwide Panelist: Ben Harden, Managing Director at Captech Panelist: Mark Donsky, Navigator Product Manager at Cloudera
This is a hands-on workshop where youll learn how to leverage the capabilities of Kafka to collect, manage, and process stream data for big data projects and general purpose enterprise data integration needs alike. When your data is captured in real-time and available as real-time subscriptions, you can start to compute new datasets in real-time off these original feeds.
This tutorial is all about managing large volumes of data coming at your data center fast and continuously. If you don't have a strategy, then allow me to help. Amazing Apache Project software can make this problem a lot easier to deal with. Spend a few hours and learn about how each part works, and how they work together. Your users will thank you.
Ethical concerns about the use of personal information in new ways has led to calls for the creation of consumer subject review boards, which could evaluate, approve, or monitor out-of-context uses of information absent user consent. This conversation between a philosopher and lawyer will address how organizations can use existing ethical frameworks to create practical accountability mechanisms.
No matter how good the intentions, ethical questions are inherent in the work of using data for social good. How are organizations navigating ethical pitfalls in order to make an impact? The key is protecting the humanity behind the numbers. In this series of talks, we'll learn how organizations are dealing with ethical considerations inherent in projects that aim to use data for good.
No matter how good the intentions, ethical questions are inherent in the work of using data for social good. How are organizations navigating ethical pitfalls in order to make an impact? The key is protecting the humanity behind the numbers. In this series of talks, we'll hear from four speakers on how they are dealing with ethical considerations inherent in projects that aim to use data for good.
Because of the way sentiment analysis algorithms are trained, they systematically amplify the voices of those who express themselves unsubtly and aggressively. I will extrapolate from this observation to show the ways in which supervised machine learning has the potential to amplify social and economic privilege.
Who will watch the watchmen? This session will cover data integrity problems in open government introduced by the human element. Well then explore possible methodologies that will allow us to derive value from open government data, while still keeping a skeptical eye on the validity of the data itself.
Technology offers amazing big data use cases, but according to Gartner it's important to avoid "crossing the creepy line." Governance and security experts from Cloudera and MasterCard discuss the legal and ethical usage of big data. Ethical behavior drives trust - they are inseparably linked. For customers to trust and continue to do business with us requires an ethical data usage framework.
LinkedIns Security Data Science group uses various reputation systems as input to models designed to stop fraud and abuse. This session will discuss how we build these reputation systems and compare instantaneous online reputation scores to more complex offline systems.
Hadoop is widely used thanks to its ability to handle volume, velocity, and variety of data. However, this flexibility and scale presents challenges for securing and governing this data. To avoid your company making the front pages over a data breach, experts from MasterCard, Intel, and Cloudera share the Hadoop Security Maturity Model phase 0-4 and steps to get your cluster ready for a PCI audit.
Combining data in Hadoop for the purpose of data discovery often runs into barriers from the security group because of legal or corporate policy. This talk will discuss the challenges with implementing data governance in big data systems, a design pattern for addressing those challenges within an organization, and a recent case study.
Encryption is a requirement for many business sectors dealing with confidential information. To meet these requirements, transparent, end-to-end encryption was added to HDFS. This protects data while it is in-flight and at-rest, and can be used compatibly with existing Hadoop apps. We will cover the design and implementation of transparent encryption in HDFS, as well as performance results.
Moderator: Steve Totman, Big Data Evangelist at Cloudera Panelist: Kristi Cunningham, VP Enterprise Data Management at Capital One Panelist: Susan Meyer, Business Leader - Fraud Management Solutions at MasterCard Worldwide Panelist: Ben Harden, Managing Director at Captech Panelist: Mark Donsky, Navigator Product Manager at Cloudera
Whether starting a data science program, reaching the breaking point with your current data technology, or figuring out what the competition is up to, these sessions will give you a bird's-eye view of data technologies, techniques, and data-driven organizations.
Big data and data science have great potential for accelerating business, but how do you reconcile the opportunity with the sea of possible technologies? Conventional data strategy has little to guide us, focusing more on governance than on creating new value. In this tutorial, we explain how to create a modern data strategy that powers data-driven business.
Apache Hadoop was designed when cloud models were in their infancy. Despite this fact, Hadoop has proven remarkably adept at migrating its architecture to work well in the context of the cloud, as production workloads migrate to a cloud environment. This talk will cover several topics on adapting Hadoop to the cloud.
With the number of production Apache HBase clusters increasing, there is greater demand for running multiple applications on single clusters, for data reliability and availability, and for developers to better test their applications. Well lay out how these new demands can be addressed using multi-tenant, multi-cluster, or multi-container deployments, including the use of Docker.
With the explosion of big data open source technologies, companies can now build a powerful data warehouse.  But as they reach scale, theyll find that patching together numerous projects requires building their own tools to manage the data pipeline.  In this presentation we will talk about the tools youll likely need to build in-house to make your data infrastructure manageable.
Today's Hadoop Cluster now has multiple single points of failures. This talk focuses on identifying these failings and how to mitigate them.
Hadoops ability to handle large amounts of varied data has been a driving force behind the explosion of big data. Many organizations ambitions to become more data-driven, however, are held back by a shortage of resources as well as the time and expense needed to purchase and set up hardware and software infrastructure. The cloud offers a natural alternative to overcome these barriers.
I will deconstruct a real-world database schema into the corresponding NoSQL design. Along the way, we will see how the number of tables drops by nearly 5x and the ease of understanding the design increases by a similar degree. In spite of radical changes, the resulting denormalized and nested data can still be queried with SQL by using Apache Drill. These methods are practical and easy to apply.
Join the authors of Hadoop Application Architectures for an open Q/A session on considerations and recommendations for architecture and design of applications using Hadoop. Talk to us about your use-case and its big data architecture, or just come to listen in.
Join the Spark team for an informal question and answer session. Spark committers from Databricks will be on hand to field a wide range of detailed questions. Even if you dont have a specific question, join in to hear what others are asking.
Join the instructors of the all-day tutorial "Apache Hadoop operations for production systems," as they field a wide range of detailed questions. Even if you dont have a specific question, join in to hear what others are asking.
Join the team behind the tutorial Developing a modern enterprise data strategy," as they field a wide range of detailed questions. Even if you dont have a specific question, join in to hear what others are asking.
Ask the panel questions about Kudu and the tradeoffs between real-time transactional access and fast analytic performance.
Whether starting a data science program, reaching the breaking point with your current data technology, or figuring out what the competition is up to, these sessions will give you a bird's-eye view of data technologies, techniques, and data-driven organizations.
Big data and data science have great potential for accelerating business, but how do you reconcile the opportunity with the sea of possible technologies? Conventional data strategy has little to guide us, focusing more on governance than on creating new value. In this tutorial, we explain how to create a modern data strategy that powers data-driven business.
Apache Hadoop was designed when cloud models were in their infancy. Despite this fact, Hadoop has proven remarkably adept at migrating its architecture to work well in the context of the cloud, as production workloads migrate to a cloud environment. This talk will cover several topics on adapting Hadoop to the cloud.
With the number of production Apache HBase clusters increasing, there is greater demand for running multiple applications on single clusters, for data reliability and availability, and for developers to better test their applications. Well lay out how these new demands can be addressed using multi-tenant, multi-cluster, or multi-container deployments, including the use of Docker.
With the explosion of big data open source technologies, companies can now build a powerful data warehouse.  But as they reach scale, theyll find that patching together numerous projects requires building their own tools to manage the data pipeline.  In this presentation we will talk about the tools youll likely need to build in-house to make your data infrastructure manageable.
Today's Hadoop Cluster now has multiple single points of failures. This talk focuses on identifying these failings and how to mitigate them.
Hadoops ability to handle large amounts of varied data has been a driving force behind the explosion of big data. Many organizations ambitions to become more data-driven, however, are held back by a shortage of resources as well as the time and expense needed to purchase and set up hardware and software infrastructure. The cloud offers a natural alternative to overcome these barriers.
I will deconstruct a real-world database schema into the corresponding NoSQL design. Along the way, we will see how the number of tables drops by nearly 5x and the ease of understanding the design increases by a similar degree. In spite of radical changes, the resulting denormalized and nested data can still be queried with SQL by using Apache Drill. These methods are practical and easy to apply.
Join the authors of Hadoop Application Architectures for an open Q/A session on considerations and recommendations for architecture and design of applications using Hadoop. Talk to us about your use-case and its big data architecture, or just come to listen in.
Join the Spark team for an informal question and answer session. Spark committers from Databricks will be on hand to field a wide range of detailed questions. Even if you dont have a specific question, join in to hear what others are asking.
Join the instructors of the all-day tutorial "Apache Hadoop operations for production systems," as they field a wide range of detailed questions. Even if you dont have a specific question, join in to hear what others are asking.
Join the team behind the tutorial Developing a modern enterprise data strategy," as they field a wide range of detailed questions. Even if you dont have a specific question, join in to hear what others are asking.
Ask the panel questions about Kudu and the tradeoffs between real-time transactional access and fast analytic performance.
DJ Patil, U.S. Chief Data Scientist at White House Office of Science and Technology Policy
DJ Patil, U.S. Chief Data Scientist at White House Office of Science and Technology Policy
Drawing lessons from successes and failures in the music industry, book publishing and TV, David Boyle will share five lessons that are essential if youre to use data to make a difference in creative businesses.
DJ Patil, U.S. Chief Data Scientist at White House Office of Science and Technology Policy
DJ Patil, U.S. Chief Data Scientist at White House Office of Science and Technology Policy
Drawing lessons from successes and failures in the music industry, book publishing and TV, David Boyle will share five lessons that are essential if youre to use data to make a difference in creative businesses.
The term data vizualization can mean anything from charts and graphs to infographics to big data and everything in between. In this tutorial, well look at the basics of how to design with data, specifically using the industry standard D3 library. By the end, you'll be able to create data vizualizations with your own data sets.
Apache Drill is an open source distributed SQL engine for Hadoop, NoSQL databases, and other services. Drill's unique schema-free JSON data model enables self-service data exploration and analysis by eliminating the need to define/maintain schemas and transform data. This is a comprehensive hands-on tutorial that will enable you to start exploring and analyzing your data in place, wherever it is.
In this session, you will learn why organizations are embarking on a mission to understand the now of their businesses, what they are doing with their internal and external data to drive continuous insights, and how their businesses benefit from these insights.
This session describes how organizations are managing Hadoop and big data workflows with an enterprise workflow solution that provides a graphical user interface for managing all the complex components of the enterprise application fabric. They gain SLA management, forecasting and change impact analysis, auditing, reporting, and self-service via mobile devices.
You have 10 milliseconds. Less than the blink of an eye, the beat of a heart  thats how much time you have to ingest fast streams of data, perform analytics on the streams, and take action. Ten milliseconds to win a customer, 10 milliseconds to make a sale, 10 milliseconds to save a life  its not much time.
In this talk, we will describe a Cloud-optimized deployment model for Spark and Hadoop, and explore how these tools and Cloud-native services complement each other to form the most productive and efficient data processing platform.
In this session youll learn how Oracle has leveraged Spark-based machine learning (ML), natural language processing (NLP), and data graph semantics (Linked Open Data) to create the simplest and most powerful big data discovery and big data preparation tools in the market.
Capital One is on a mission to Change Banking for Good. Join Capital One as we take you through the journey of the Data Lab. How did we get started? What have we learned about mingling disciplines such as human centered design, full stack engineering, and data science? And how are we taking an entrepreneurial approach to develop successful solutions that deliver real impact?
Building design software for industries from engineering to construction, manufacturing to media, meant Autodesk needed to architect its analytics platform to handle massive amounts of data.  Learn how Autodesk uses open-source technologies like Kafka and Hadoop and integrates them with solutions like Splunk, Google BigQuery, and Tableau to achieve data insights at scale.
Businesses are moving from large-scale batch data analysis to large-scale real-time data analysis. Apache Storm has emerged as one of the most popular platforms for this purpose. This talk covers proven design patterns for real-time stream processing. They have been vetted in large-scale production deployments that process tens of billions of events/day and tens of terabytes of data/day.
Financial institutions use data such as streaming news feeds and proprietary data for insight. One company is taking filings from 130 countries and data from 500,000 equity instruments to create real-time applications. Data integration is essential for information to be trusted in these applications. Explore an architecture designed to capture all data and ensure it is trusted.
As big data becomes a pervasive force in the enterprise, many of our fundamental ideas around how to optimize compute, storage, network, and resource management are being stretched.
The term data vizualization can mean anything from charts and graphs to infographics to big data and everything in between. In this tutorial, well look at the basics of how to design with data, specifically using the industry standard D3 library. By the end, you'll be able to create data vizualizations with your own data sets.
Apache Drill is an open source distributed SQL engine for Hadoop, NoSQL databases, and other services. Drill's unique schema-free JSON data model enables self-service data exploration and analysis by eliminating the need to define/maintain schemas and transform data. This is a comprehensive hands-on tutorial that will enable you to start exploring and analyzing your data in place, wherever it is.
In this session, you will learn why organizations are embarking on a mission to understand the now of their businesses, what they are doing with their internal and external data to drive continuous insights, and how their businesses benefit from these insights.
This session describes how organizations are managing Hadoop and big data workflows with an enterprise workflow solution that provides a graphical user interface for managing all the complex components of the enterprise application fabric. They gain SLA management, forecasting and change impact analysis, auditing, reporting, and self-service via mobile devices.
You have 10 milliseconds. Less than the blink of an eye, the beat of a heart  thats how much time you have to ingest fast streams of data, perform analytics on the streams, and take action. Ten milliseconds to win a customer, 10 milliseconds to make a sale, 10 milliseconds to save a life  its not much time.
In this talk, we will describe a Cloud-optimized deployment model for Spark and Hadoop, and explore how these tools and Cloud-native services complement each other to form the most productive and efficient data processing platform.
In this session youll learn how Oracle has leveraged Spark-based machine learning (ML), natural language processing (NLP), and data graph semantics (Linked Open Data) to create the simplest and most powerful big data discovery and big data preparation tools in the market.
Capital One is on a mission to Change Banking for Good. Join Capital One as we take you through the journey of the Data Lab. How did we get started? What have we learned about mingling disciplines such as human centered design, full stack engineering, and data science? And how are we taking an entrepreneurial approach to develop successful solutions that deliver real impact?
Building design software for industries from engineering to construction, manufacturing to media, meant Autodesk needed to architect its analytics platform to handle massive amounts of data.  Learn how Autodesk uses open-source technologies like Kafka and Hadoop and integrates them with solutions like Splunk, Google BigQuery, and Tableau to achieve data insights at scale.
Businesses are moving from large-scale batch data analysis to large-scale real-time data analysis. Apache Storm has emerged as one of the most popular platforms for this purpose. This talk covers proven design patterns for real-time stream processing. They have been vetted in large-scale production deployments that process tens of billions of events/day and tens of terabytes of data/day.
Financial institutions use data such as streaming news feeds and proprietary data for insight. One company is taking filings from 130 countries and data from 500,000 equity instruments to create real-time applications. Data integration is essential for information to be trusted in these applications. Explore an architecture designed to capture all data and ensure it is trusted.
As big data becomes a pervasive force in the enterprise, many of our fundamental ideas around how to optimize compute, storage, network, and resource management are being stretched.
This hands-on, beginner-friendly tutorial provides a quick start to building intelligent business applications using machine learning. Learn about machine learning basics, feature engineering, recommender systems, and deep learning. The program includes hands-on portions to build and deploy large-scale machine learning applications.
Big data has moved beyond the bleeding-edge, early-adopter stage. If you're not using it now, you will be soon. But big data deployments are not a cookie-cutter, one-size-fits-all effort. Cisco Big Data Consulting Systems Engineer Robert Novak will present real-world deployment stories and use cases for big data on Cisco UCS, especially (but not exclusively) around Hadoop environments.
The promise of IoT is that it will forever change the way people and businesses interact with the world. Using illustrative use cases, Pivotal will demonstrate the fundamental concepts required to drive true impact from these connected devices. We will cover which models are most appropriate, what considerations around data access and processing are critical, and which tools available.
In-memory is no longer just a trend: its an imperative, for high volume, real-time data workloads. With the relational, distributed MemSQL database, modern enterprises are unlocking value from gigabytes and terabytes of data. Learn about some of latest applications and deployments of in-memory technology from Akamai Technologies, Novus, and Digital Ocean.
Security teams study many months and years of data for baselining and incident forensics, but IT operations may only want to store weeks or months of data to analyze for operational insights. And the two different needs can be difficult to reconcile. Learn how TELUS's security analysts provide value to both teams.
Hadoop multi-tenancy is becoming a must-have  in order to accommodate multiple lines of business, multiple concurrent Hadoop jobs, multiple versions of Hadoop, multiple applications, security isolation, and more. This session will discuss these requirements and share recommendations on how to deploy a secure multi-tenant Hadoop environment with simplicity, agility, and low management overhead.
While schema on read is powerful, its just a first step on the journey to understanding effective ways of working with data in new big data systems. In this talk we highlight new patterns of working with data.
Building a strategy and methodology that protects sensitive data is vital in securing your big data systems and enterprise assets. Learn how people protect big data in Hadoop, and understand how protecting the information is possible without removing the value of the data, or paying a performance penalty.
Cognitive computing has made the transition from a theoretical technology into one that is having a transformative impact on business and our daily lives. In this session, Tim Estes, CEO and founder of Digital Reasoning, will explore how key enabling technologies, such as artificial intelligence and natural language processing, have made this possible.
Pepsi analyst Matthew Derda and Trifacta Director Customer Success Doug Stradley discuss why data wrangling is critical to empowering analysts to efficiently access, and incorporate, diverse big data sources for organizational analysis. Get first-hand examples where traditional ETL and scripting approaches fall short, and why self-service approaches are critical to big data initiatives.
If youre struggling with determining which implementation of SQL on Hadoop can meet your analytics needs, youre not alone. Join us for a discussion on how YP.com, a leading local marketing solutions provider in the U.S. dedicated to helping local businesses and communities grow, uses HP Vertica for SQL on Hadoop to solve their organizations big data challenges.
UnitedHealth Group has long been defined by our innovative approach to health care, and our approach to IT and analytics is no different. With the goal of making health care more affordable by identifying fraud, waste, and abuse activities, this session will provide details on how we leveraged Hadoop for payment integrity analytics to identify thousands of high-risk providers and claims.
This hands-on, beginner-friendly tutorial provides a quick start to building intelligent business applications using machine learning. Learn about machine learning basics, feature engineering, recommender systems, and deep learning. The program includes hands-on portions to build and deploy large-scale machine learning applications.
Big data has moved beyond the bleeding-edge, early-adopter stage. If you're not using it now, you will be soon. But big data deployments are not a cookie-cutter, one-size-fits-all effort. Cisco Big Data Consulting Systems Engineer Robert Novak will present real-world deployment stories and use cases for big data on Cisco UCS, especially (but not exclusively) around Hadoop environments.
The promise of IoT is that it will forever change the way people and businesses interact with the world. Using illustrative use cases, Pivotal will demonstrate the fundamental concepts required to drive true impact from these connected devices. We will cover which models are most appropriate, what considerations around data access and processing are critical, and which tools available.
In-memory is no longer just a trend: its an imperative, for high volume, real-time data workloads. With the relational, distributed MemSQL database, modern enterprises are unlocking value from gigabytes and terabytes of data. Learn about some of latest applications and deployments of in-memory technology from Akamai Technologies, Novus, and Digital Ocean.
Security teams study many months and years of data for baselining and incident forensics, but IT operations may only want to store weeks or months of data to analyze for operational insights. And the two different needs can be difficult to reconcile. Learn how TELUS's security analysts provide value to both teams.
Hadoop multi-tenancy is becoming a must-have  in order to accommodate multiple lines of business, multiple concurrent Hadoop jobs, multiple versions of Hadoop, multiple applications, security isolation, and more. This session will discuss these requirements and share recommendations on how to deploy a secure multi-tenant Hadoop environment with simplicity, agility, and low management overhead.
While schema on read is powerful, its just a first step on the journey to understanding effective ways of working with data in new big data systems. In this talk we highlight new patterns of working with data.
Building a strategy and methodology that protects sensitive data is vital in securing your big data systems and enterprise assets. Learn how people protect big data in Hadoop, and understand how protecting the information is possible without removing the value of the data, or paying a performance penalty.
Cognitive computing has made the transition from a theoretical technology into one that is having a transformative impact on business and our daily lives. In this session, Tim Estes, CEO and founder of Digital Reasoning, will explore how key enabling technologies, such as artificial intelligence and natural language processing, have made this possible.
Pepsi analyst Matthew Derda and Trifacta Director Customer Success Doug Stradley discuss why data wrangling is critical to empowering analysts to efficiently access, and incorporate, diverse big data sources for organizational analysis. Get first-hand examples where traditional ETL and scripting approaches fall short, and why self-service approaches are critical to big data initiatives.
If youre struggling with determining which implementation of SQL on Hadoop can meet your analytics needs, youre not alone. Join us for a discussion on how YP.com, a leading local marketing solutions provider in the U.S. dedicated to helping local businesses and communities grow, uses HP Vertica for SQL on Hadoop to solve their organizations big data challenges.
UnitedHealth Group has long been defined by our innovative approach to health care, and our approach to IT and analytics is no different. With the goal of making health care more affordable by identifying fraud, waste, and abuse activities, this session will provide details on how we leveraged Hadoop for payment integrity analytics to identify thousands of high-risk providers and claims.
This session covers why continual, adaptive optimization is a key to success with real world machine learning models.  Bill will detail the applicability of machine learning tools with the pros/cons of each. Learn how to optimize processes to drive more predictable outcomes from business decisions.  Tools for automating access to changing data and removal of noise and error will also be reviewed.
Bill Schmarzo, EMC CTO of Global Services, and author of Big Data: Understanding How Data Powers Big Business," will utilize a workshop approach to help you identify where and how to integrate data and analytics into your business strategies.
This talk explores the actual behavior of eventual consistent systems aka mostly inconsistent systems, while presenting a paxos algorithm alternative. Well highlight the Amazon use case and various fixes made to S3 in order to enable Hadoop workflows, and alternatives offered by Cassandra, then explore Paxos as an alternative to such inconsistent systems for Hadoop Storage and HBase solutions.
The Internet of Everything (IoT) continues to give rise to new business models in the Retail, Industrial Manufacturing, Healthcare, Insurance, Medical device manufacturers, Telecommunications, and Technology industries. Learn what those efforts are and how to capitalize on these opportunities for your clients.
In this session, learn how leading customers have built a unified big data fabric on top of Hadoop, using technologies like Informatica to repeatably deliver trusted data assets to a large community of data consumers, for a multi-dimensional view of customers.
At Microsoft, we process exabytes of data to run our own businesses. Learn how you can process big data in the cloud at massive scale with no hardware to deploy, software to tune/configure, and infrastructure to manage. Well also talk about overcoming common obstacles in big data adoption such as a high learning curve, cost of implementation, tuning infrastructure, and providing security.
Can Hadoop now handle your enterprise analytic workloads? Actian SVP of Engineering Emma McGrattan will describe the various solutions that comprise the SQL on Hadoop landscape, identify the features that are important for those modernizing their enterprise analytic workloads on Hadoop, and describe the successes that Actian customers have had in moving their BI and Analytic workloads to Hadoop.
Forrester Research Principal Analyst Michele Goetz discusses findings from Delivering Governed Data for Analytics at Scale, a June 2015 commissioned study conducted by Forrester Consulting on behalf of Pentaho on the topic of data governance and delivery.
Join us to learn about how SAP HANA Vora  can be used as a stand-alone or in concert with SAP HANA platform to extend enterprise-grade analytics to Hadoop clusters and provide enriched, interactive analytics on Hadoop.
Everyone knows that Python isnt suitable for massive scale analytics, right? Wrong. Spark 1.3 introduced data frames, which allow for high performance Spark batch jobs, streaming, and machine learning over massive datasets. In this talk youll learn how to combine Cassandra, a highly scalable, always-on OLTP data store, with PySpark, a framework for distributed computation.
This session covers why continual, adaptive optimization is a key to success with real world machine learning models.  Bill will detail the applicability of machine learning tools with the pros/cons of each. Learn how to optimize processes to drive more predictable outcomes from business decisions.  Tools for automating access to changing data and removal of noise and error will also be reviewed.
Bill Schmarzo, EMC CTO of Global Services, and author of Big Data: Understanding How Data Powers Big Business," will utilize a workshop approach to help you identify where and how to integrate data and analytics into your business strategies.
This talk explores the actual behavior of eventual consistent systems aka mostly inconsistent systems, while presenting a paxos algorithm alternative. Well highlight the Amazon use case and various fixes made to S3 in order to enable Hadoop workflows, and alternatives offered by Cassandra, then explore Paxos as an alternative to such inconsistent systems for Hadoop Storage and HBase solutions.
The Internet of Everything (IoT) continues to give rise to new business models in the Retail, Industrial Manufacturing, Healthcare, Insurance, Medical device manufacturers, Telecommunications, and Technology industries. Learn what those efforts are and how to capitalize on these opportunities for your clients.
In this session, learn how leading customers have built a unified big data fabric on top of Hadoop, using technologies like Informatica to repeatably deliver trusted data assets to a large community of data consumers, for a multi-dimensional view of customers.
At Microsoft, we process exabytes of data to run our own businesses. Learn how you can process big data in the cloud at massive scale with no hardware to deploy, software to tune/configure, and infrastructure to manage. Well also talk about overcoming common obstacles in big data adoption such as a high learning curve, cost of implementation, tuning infrastructure, and providing security.
Can Hadoop now handle your enterprise analytic workloads? Actian SVP of Engineering Emma McGrattan will describe the various solutions that comprise the SQL on Hadoop landscape, identify the features that are important for those modernizing their enterprise analytic workloads on Hadoop, and describe the successes that Actian customers have had in moving their BI and Analytic workloads to Hadoop.
Forrester Research Principal Analyst Michele Goetz discusses findings from Delivering Governed Data for Analytics at Scale, a June 2015 commissioned study conducted by Forrester Consulting on behalf of Pentaho on the topic of data governance and delivery.
Join us to learn about how SAP HANA Vora  can be used as a stand-alone or in concert with SAP HANA platform to extend enterprise-grade analytics to Hadoop clusters and provide enriched, interactive analytics on Hadoop.
Everyone knows that Python isnt suitable for massive scale analytics, right? Wrong. Spark 1.3 introduced data frames, which allow for high performance Spark batch jobs, streaming, and machine learning over massive datasets. In this talk youll learn how to combine Cassandra, a highly scalable, always-on OLTP data store, with PySpark, a framework for distributed computation.
League of Legends has more than 67 million players per month. The company needed an analytics solution that would work well with their push-model data pipeline. In this session, data engineer Chris Kudelka will discuss how their game designers use Riot's data pipeline and Platfora to measure and validate player-focused changes like improvements to game servers and client performance.
Data lakes represent a new data architecture that provides enterprises with the scale and flexibility required for big data: unbounded storage for unbounded questions. While Hadoop is the de facto standard for implementing data lakes today, significant time and effort are still required. This talk introduces Cask Hydrator, a new open source data lake framework and drag-and-drop UI built on CDAP.
This talk is about the best practices approach to accelerate data discovery while complying with security and data governance needs. Learn how to implement an automated and governed inventory of your data assets. Open up your data lake with secure self-service to find and understand data quickly.
Whether youre a large enterprise or a startup, successfully competing with modern, nimble, fast-moving companies like Uber or Airbnb can only be done with modern, model-driven development environments and big data solutions. Infrastructure shouldnt restrict the interactions between relational data and big data. Development shouldnt slow analytics.
Enterprises find it far too costly and time-consuming to locate all of the data relevant to analysis. Data is so fragmented that most enterprises lack even a basic inventory of all sources and attributes -- an enormous constraint on getting return on your big data investment. Tamr Catalog solves this by creating an inventory of all enterprise metadata in a central, platform-neutral place.
All of us involved in big data are working to decrease time to insights. We're building Spark on Yarn clusters with Hadoop ecosystem components, and there are clear benefits to this implementation. However, there are other use cases that may benefit from a more streamlined stack.
Imagine the possibilities of having all of your data in one place  at a reasonable cost  with the computing potential to learn from relationships between data in all domains. Advanced analytics and Hadoop are changing the way organizations approach big data.Hear tips from the future and learn about key patterns emerging from a wide cross section of Hadoop journeys. Perhaps theyll inspire yours.
Want to get ramped up on how to use Amazon's big data web services and launch your first big data application on AWS?
With Spark becoming the rising star of cluster computing comes the prospect of putting it to use as a platform for end-to-end data science. At DeepSense.io we have built an intuitive interface to take Spark to the next level of usability. By introducing a layer that provides code-free UX and simplified resource management, Spark is brought even closer to the concepts known in data science.
To accelerate enterprise deployment of big data analytics, Intel and partners introduced an open source trusted analytic platform-as-a-service for data scientists and app developers to build and deploy advanced analytics applications at cloud scale. Join us and discover how you can customize and develop your own big data solutions with this platform.
The only guarantee in life is change. Thats exactly what makes the world interesting and innovative, and thats exactly what the large internet properties are counting on: to disrupt traditional businesses with an always-on, data-centric business model.
League of Legends has more than 67 million players per month. The company needed an analytics solution that would work well with their push-model data pipeline. In this session, data engineer Chris Kudelka will discuss how their game designers use Riot's data pipeline and Platfora to measure and validate player-focused changes like improvements to game servers and client performance.
Data lakes represent a new data architecture that provides enterprises with the scale and flexibility required for big data: unbounded storage for unbounded questions. While Hadoop is the de facto standard for implementing data lakes today, significant time and effort are still required. This talk introduces Cask Hydrator, a new open source data lake framework and drag-and-drop UI built on CDAP.
This talk is about the best practices approach to accelerate data discovery while complying with security and data governance needs. Learn how to implement an automated and governed inventory of your data assets. Open up your data lake with secure self-service to find and understand data quickly.
Whether youre a large enterprise or a startup, successfully competing with modern, nimble, fast-moving companies like Uber or Airbnb can only be done with modern, model-driven development environments and big data solutions. Infrastructure shouldnt restrict the interactions between relational data and big data. Development shouldnt slow analytics.
Enterprises find it far too costly and time-consuming to locate all of the data relevant to analysis. Data is so fragmented that most enterprises lack even a basic inventory of all sources and attributes -- an enormous constraint on getting return on your big data investment. Tamr Catalog solves this by creating an inventory of all enterprise metadata in a central, platform-neutral place.
All of us involved in big data are working to decrease time to insights. We're building Spark on Yarn clusters with Hadoop ecosystem components, and there are clear benefits to this implementation. However, there are other use cases that may benefit from a more streamlined stack.
Imagine the possibilities of having all of your data in one place  at a reasonable cost  with the computing potential to learn from relationships between data in all domains. Advanced analytics and Hadoop are changing the way organizations approach big data.Hear tips from the future and learn about key patterns emerging from a wide cross section of Hadoop journeys. Perhaps theyll inspire yours.
Want to get ramped up on how to use Amazon's big data web services and launch your first big data application on AWS?
With Spark becoming the rising star of cluster computing comes the prospect of putting it to use as a platform for end-to-end data science. At DeepSense.io we have built an intuitive interface to take Spark to the next level of usability. By introducing a layer that provides code-free UX and simplified resource management, Spark is brought even closer to the concepts known in data science.
To accelerate enterprise deployment of big data analytics, Intel and partners introduced an open source trusted analytic platform-as-a-service for data scientists and app developers to build and deploy advanced analytics applications at cloud scale. Join us and discover how you can customize and develop your own big data solutions with this platform.
The only guarantee in life is change. Thats exactly what makes the world interesting and innovative, and thats exactly what the large internet properties are counting on: to disrupt traditional businesses with an always-on, data-centric business model.
This three-day curriculum features advanced lectures and hands-on technical exercises for Spark usage in data exploration, analysis, and building big data applications.
This three-day curriculum features advanced lectures and hands-on technical exercises for Spark usage in data exploration, analysis, and building big data applications.
This three-day curriculum features advanced lectures and hands-on technical exercises for Spark usage in data exploration, analysis, and building big data applications.
This three-day curriculum features advanced lectures and hands-on technical exercises for Spark usage in data exploration, analysis, and building big data applications.
This three-day curriculum features advanced lectures and hands-on technical exercises for Spark usage in data exploration, analysis, and building big data applications.
This three-day curriculum features advanced lectures and hands-on technical exercises for Spark usage in data exploration, analysis, and building big data applications.
In this three-day course, you will: * Learn how to use machine learning, text analysis, and real-time analytics to solve frequently encountered, high-value business problems, * Understand data science methodology and end-to-end work flow of problem solution including data preparation, model building and validation, and model deployment, * Use Apache Spark and other tools for analytics.
In this three-day course, you will: * Learn how to use machine learning, text analysis, and real-time analytics to solve frequently encountered, high-value business problems, * Understand data science methodology and end-to-end work flow of problem solution including data preparation, model building and validation, and model deployment, * Use Apache Spark and other tools for analytics.
In this three-day course, you will: * Learn how to use machine learning, text analysis, and real-time analytics to solve frequently encountered, high-value business problems, * Understand data science methodology and end-to-end work flow of problem solution including data preparation, model building and validation, and model deployment, * Use Apache Spark and other tools for analytics.
In this three-day course, you will: * Learn how to use machine learning, text analysis, and real-time analytics to solve frequently encountered, high-value business problems, * Understand data science methodology and end-to-end work flow of problem solution including data preparation, model building and validation, and model deployment, * Use Apache Spark and other tools for analytics.
In this three-day course, you will: * Learn how to use machine learning, text analysis, and real-time analytics to solve frequently encountered, high-value business problems, * Understand data science methodology and end-to-end work flow of problem solution including data preparation, model building and validation, and model deployment, * Use Apache Spark and other tools for analytics.
In this three-day course, you will: * Learn how to use machine learning, text analysis, and real-time analytics to solve frequently encountered, high-value business problems, * Understand data science methodology and end-to-end work flow of problem solution including data preparation, model building and validation, and model deployment, * Use Apache Spark and other tools for analytics.
Cloudera Universitys three-day course for designing and building big data applications prepares you to analyze and solve real-world problems using Apache Hadoop and associated tools in the enterprise data hub (EDH).
Cloudera Universitys three-day course for designing and building big data applications prepares you to analyze and solve real-world problems using Apache Hadoop and associated tools in the enterprise data hub (EDH).
Cloudera Universitys three-day course for designing and building big data applications prepares you to analyze and solve real-world problems using Apache Hadoop and associated tools in the enterprise data hub (EDH).
Cloudera Universitys three-day course for designing and building big data applications prepares you to analyze and solve real-world problems using Apache Hadoop and associated tools in the enterprise data hub (EDH).
Cloudera Universitys three-day course for designing and building big data applications prepares you to analyze and solve real-world problems using Apache Hadoop and associated tools in the enterprise data hub (EDH).
Cloudera Universitys three-day course for designing and building big data applications prepares you to analyze and solve real-world problems using Apache Hadoop and associated tools in the enterprise data hub (EDH).
Program Chairs Roger Magoulas, Doug Cutting, and Alistair Croll welcome you to the first day of keynotes.
Mike Olson, CSO and Chairman, Cloudera
Unusual collaborations can often lead to new ways of taking, and analyzing data. This talk looks at lessons learned from working with chefs, circus performers, and preschoolers.
Join Microsofts Joseph Sirosh for a behind-the-scenes sneak peek into the creation of the viral phenomenon How-Old.net. He'll cover how it got to 50 million users in 7 days, the unexpected big data challenges that came with it, and the surprising learnings they had about people and systems.
Even in this era of intense medical breakthroughs, many illnesses still evade accurate and timely diagnosis.  Clinicians' must often rely on static diagnostic guidelines, that result in late care and too many false alarms. Half of all heart failure patients can go undiagnosed.
This keynote unveils why rapid modernization of BI is taking place, the business use cases driving it, and whats essential in next-generation solutions.
IoE, IoT, and big data  three topics you hear and read about often in our various industries. Lets quickly look at these market and technology dynamics, and see how they are each in their own way democratizing data access and analysis, resulting in new businesses, technologies, and improved community solutions throughout the world.
Are creative businesses the last battleground for data-driven decision making? Drawing lessons from successes and failures in the music industry, book publishing, and TV, David Boyle will argue for a negotiated settlement in the war between data and creative, and show how long-term and mutually beneficial peace can work.
Katherine will discuss recent behavioral science research suggesting how a number of simple, inexpensive tools can be used to encourage improved decisions.
Jeff Jonas, IBM Fellow; Chief Scientist, Context Computing
In his ten-minute keynote, CIA Chief Information Officer Douglas Wolfe discusses how data science is a true team sport, and how the rapid evolution of this field continually improves the impact of the CIA mission.
Strata + Hadoop World Program Chairs Roger Magoulas, Doug Cutting, and Alistair Croll welcome you to the second day of keynotes.
It is easy to make "false discoveries" when analyzing big data.  It is harder to draw causal conclusions that are reliable and reproducible, especially when private or proprietary information is involved.  Recent mathematical ideas, like differential privacy, offer new ways of reaching robust conclusions while provably protecting personal information.
The big data dividend refers to the ongoing, significant profits that are derived by running data-driven applications.  This session will include examples of applications by leading companies, and provide insights into how developers and organizations can realize big data dividends from a new class of scalable applications with continuous analytics.
The traditional BI and analytics tools of the last decade have made it difficult for users to work directly with their data. With the latest innovations in big data discovery platforms, a new role has emerged: the citizen data scientist. In this keynote, Ben will share Platforas research behind the importance of this emerging role so that companies can become truly data-driven.
Imagine the possibilities of having all of your data in one place  at a reasonable cost  with the computing potential to learn from relationships between data in all domains.  Advanced analytics and Hadoop are changing the way organizations approach big data. Hear tips from the future and learn about key patterns emerging from a wide cross section of Hadoop journeys.
Farrah Bostic, Founder, The Difference Engine
IBM fellow and director, Watson Content Services, IBM
Jake Porway, founder and executive director of DataKind, unveils five keys for successful data science for good projects, based on the organization's three years of work rallying thousands of volunteers worldwide to give back.
Big data is a bit like nuclear energy: while full of promise, it generates residue that is difficult to dispose of, poses risks for those who store it, and leaves the industry one major incident away from scaring the public off the technology entirely.
What do you do when you find a momentary break in your otherwise endless barrage of tasks? In this talk, Maria argues for the vital importance of recapturing the seeming nothingness of boredom, of harnessing the pauses of life for their creative potential. It is in boredom that the truly deep questions and discoveries lie.
Joy Johnson, VP, Mobile, AudioCommon
Program Chairs Roger Magoulas, Doug Cutting, and Alistair Croll, close out the Strata + Hadoop World keynotes.
DJ Patil, U.S. Chief Data Scientist at White House Office of Science and Technology Policy
Ben Lorica, Program Director, O'Reilly Media.
Program Chairs Roger Magoulas, Doug Cutting, and Alistair Croll welcome you to the first day of keynotes.
Mike Olson, CSO and Chairman, Cloudera
Unusual collaborations can often lead to new ways of taking, and analyzing data. This talk looks at lessons learned from working with chefs, circus performers, and preschoolers.
Join Microsofts Joseph Sirosh for a behind-the-scenes sneak peek into the creation of the viral phenomenon How-Old.net. He'll cover how it got to 50 million users in 7 days, the unexpected big data challenges that came with it, and the surprising learnings they had about people and systems.
Even in this era of intense medical breakthroughs, many illnesses still evade accurate and timely diagnosis.  Clinicians' must often rely on static diagnostic guidelines, that result in late care and too many false alarms. Half of all heart failure patients can go undiagnosed.
This keynote unveils why rapid modernization of BI is taking place, the business use cases driving it, and whats essential in next-generation solutions.
IoE, IoT, and big data  three topics you hear and read about often in our various industries. Lets quickly look at these market and technology dynamics, and see how they are each in their own way democratizing data access and analysis, resulting in new businesses, technologies, and improved community solutions throughout the world.
Are creative businesses the last battleground for data-driven decision making? Drawing lessons from successes and failures in the music industry, book publishing, and TV, David Boyle will argue for a negotiated settlement in the war between data and creative, and show how long-term and mutually beneficial peace can work.
Katherine will discuss recent behavioral science research suggesting how a number of simple, inexpensive tools can be used to encourage improved decisions.
Jeff Jonas, IBM Fellow; Chief Scientist, Context Computing
In his ten-minute keynote, CIA Chief Information Officer Douglas Wolfe discusses how data science is a true team sport, and how the rapid evolution of this field continually improves the impact of the CIA mission.
Strata + Hadoop World Program Chairs Roger Magoulas, Doug Cutting, and Alistair Croll welcome you to the second day of keynotes.
It is easy to make "false discoveries" when analyzing big data.  It is harder to draw causal conclusions that are reliable and reproducible, especially when private or proprietary information is involved.  Recent mathematical ideas, like differential privacy, offer new ways of reaching robust conclusions while provably protecting personal information.
The big data dividend refers to the ongoing, significant profits that are derived by running data-driven applications.  This session will include examples of applications by leading companies, and provide insights into how developers and organizations can realize big data dividends from a new class of scalable applications with continuous analytics.
The traditional BI and analytics tools of the last decade have made it difficult for users to work directly with their data. With the latest innovations in big data discovery platforms, a new role has emerged: the citizen data scientist. In this keynote, Ben will share Platforas research behind the importance of this emerging role so that companies can become truly data-driven.
Imagine the possibilities of having all of your data in one place  at a reasonable cost  with the computing potential to learn from relationships between data in all domains.  Advanced analytics and Hadoop are changing the way organizations approach big data. Hear tips from the future and learn about key patterns emerging from a wide cross section of Hadoop journeys.
Farrah Bostic, Founder, The Difference Engine
IBM fellow and director, Watson Content Services, IBM
Jake Porway, founder and executive director of DataKind, unveils five keys for successful data science for good projects, based on the organization's three years of work rallying thousands of volunteers worldwide to give back.
Big data is a bit like nuclear energy: while full of promise, it generates residue that is difficult to dispose of, poses risks for those who store it, and leaves the industry one major incident away from scaring the public off the technology entirely.
What do you do when you find a momentary break in your otherwise endless barrage of tasks? In this talk, Maria argues for the vital importance of recapturing the seeming nothingness of boredom, of harnessing the pauses of life for their creative potential. It is in boredom that the truly deep questions and discoveries lie.
Joy Johnson, VP, Mobile, AudioCommon
Program Chairs Roger Magoulas, Doug Cutting, and Alistair Croll, close out the Strata + Hadoop World keynotes.
DJ Patil, U.S. Chief Data Scientist at White House Office of Science and Technology Policy
Ben Lorica, Program Director, O'Reilly Media.
We as an industry are collecting more data every year. IoT, web, and mobile applications send torrents of bits to our data centers that have to be processed and stored, while users expect an always-on experienceleaving little room for error. Patrick McFadin explores how successful companies do this every day with powerful data pipelines built with SMACK: Spark, Mesos, Akka, Cassandra, and Kafka.
Join expert Jerry Overton as he explains how to make the business and technical aspects of your data strategy work together for best results.
We as an industry are collecting more data every year. IoT, web, and mobile applications send torrents of bits to our data centers that have to be processed and stored, while users expect an always-on experienceleaving little room for error. Patrick McFadin explores how successful companies do this every day with powerful data pipelines built with SMACK: Spark, Mesos, Akka, Cassandra, and Kafka.
Join expert Jerry Overton as he explains how to make the business and technical aspects of your data strategy work together for best results.
Yonik Seeley explores recent Apache Solr features in the areas of faceting and analytics, including parallel SQL, streaming expressions, distributed join, and distributed graph queries, as well as the trade-offs of different approaches and strategies for maximizing scalability.
Brian Granger, Sylvain Corlay, and Jason Grout offer an overview of JupyterLab, the next-generation user interface for Project Jupyter that puts Jupyter Notebooks within a powerful user interface that allows the building blocks of interactive computing to be assembled to support a wide range of interactive workflows used in data science.
Geospatial analysis can provide deep insights into many datasets. Unfortunately the key tools to unlocking these insightsgeospatial statistics, machine learning, and meaningful cartographyremain inaccessible to nontechnical audiences. Stuart Lynn and Andy Eschbacher explore the design challenges in making these tools accessible and integrated in an intuitive location intelligence platform.
In pursuit of speed, big data is evolving toward columnar execution. The solid foundation laid by Arrow and Parquet for a shared columnar representation across the ecosystem promises a great future. Julien Le Dem and Jacques Nadeau discuss the future of columnar and the hardware trends it takes advantage of, like RDMA, SSDs, and nonvolatile memory.
Himanshu Gupta explains why Yahoo has been increasingly investing in interactive analytics and how it leverages Druid to power a variety of internal- and external-facing data applications.
The Netflix data platform is constantly evolving, but fundamentally it's an all-cloud platform at a massive scale (40+ PB and over 700 billion new events per day) focused on empowering developers. Kurt Brown dives into the current technology landscape at Netflix and offers some thoughts on what the future holds.
Netflix is exploring new avenues for data processing where traditional approaches fail to scale. Ryan Blue explains how Netflix is building on Parquet to enhance its 40+ petabyte warehouse, combining Parquet's features with Presto and Spark to boost ETL and interactive queries. Information about tuning Parquet is hard to find. Ryan shares what he's learned, creating the missing guide you need.
Tyler Akidau offers a whirlwind tour of the conceptual building blocks of massive-scale data processing systems over the last decade, comparing and contrasting systems at Google with popular open source systems in use today.
Ever wondered what it takes to scale Kafka, Samza, and Druid to handle complex, heterogeneous analytics workloads at petabyte size? Xavier Laut discusses his experience scaling Metamarkets's real-time processing to over 3 million events per second and shares the challenges encountered and lessons learned along the way.
Smart data allows fire services to better protect the people they serve and keep their firefighters safe. The combination of open and nonpublic data used in a smart way generates new insights both in preparation and operations. Bart van Leeuwen discusses how the fire service is benefiting from open standards and best practices.
Cluster computing frameworks such as Hadoop or Spark are tremendously beneficial in processing and deriving insights from data. However, long query latencies make these frameworks suboptimal choices to power interactive applications. Fangjin Yang discusses using Druid for analytics and explains why the architecture is well suited to power analytic dashboards.
Vartika Singh and Jayant Shekhar walk you through techniques for building and tuning machine-learning apps using Spark MLlib and Spark ML Pipelines and graph processing with GraphX.
Martin Wicke and Josh Gordon offer hands-on experience training and deploying a machine-learning system using TensorFlow, a popular open source library. You'll learn how to build machine-learning systems from simple classifiers to complex image-based models as well as how to deploy models in production using TensorFlow Serving.
Yonik Seeley explores recent Apache Solr features in the areas of faceting and analytics, including parallel SQL, streaming expressions, distributed join, and distributed graph queries, as well as the trade-offs of different approaches and strategies for maximizing scalability.
Brian Granger, Sylvain Corlay, and Jason Grout offer an overview of JupyterLab, the next-generation user interface for Project Jupyter that puts Jupyter Notebooks within a powerful user interface that allows the building blocks of interactive computing to be assembled to support a wide range of interactive workflows used in data science.
Geospatial analysis can provide deep insights into many datasets. Unfortunately the key tools to unlocking these insightsgeospatial statistics, machine learning, and meaningful cartographyremain inaccessible to nontechnical audiences. Stuart Lynn and Andy Eschbacher explore the design challenges in making these tools accessible and integrated in an intuitive location intelligence platform.
In pursuit of speed, big data is evolving toward columnar execution. The solid foundation laid by Arrow and Parquet for a shared columnar representation across the ecosystem promises a great future. Julien Le Dem and Jacques Nadeau discuss the future of columnar and the hardware trends it takes advantage of, like RDMA, SSDs, and nonvolatile memory.
Himanshu Gupta explains why Yahoo has been increasingly investing in interactive analytics and how it leverages Druid to power a variety of internal- and external-facing data applications.
The Netflix data platform is constantly evolving, but fundamentally it's an all-cloud platform at a massive scale (40+ PB and over 700 billion new events per day) focused on empowering developers. Kurt Brown dives into the current technology landscape at Netflix and offers some thoughts on what the future holds.
Netflix is exploring new avenues for data processing where traditional approaches fail to scale. Ryan Blue explains how Netflix is building on Parquet to enhance its 40+ petabyte warehouse, combining Parquet's features with Presto and Spark to boost ETL and interactive queries. Information about tuning Parquet is hard to find. Ryan shares what he's learned, creating the missing guide you need.
Tyler Akidau offers a whirlwind tour of the conceptual building blocks of massive-scale data processing systems over the last decade, comparing and contrasting systems at Google with popular open source systems in use today.
Ever wondered what it takes to scale Kafka, Samza, and Druid to handle complex, heterogeneous analytics workloads at petabyte size? Xavier Laut discusses his experience scaling Metamarkets's real-time processing to over 3 million events per second and shares the challenges encountered and lessons learned along the way.
Smart data allows fire services to better protect the people they serve and keep their firefighters safe. The combination of open and nonpublic data used in a smart way generates new insights both in preparation and operations. Bart van Leeuwen discusses how the fire service is benefiting from open standards and best practices.
Cluster computing frameworks such as Hadoop or Spark are tremendously beneficial in processing and deriving insights from data. However, long query latencies make these frameworks suboptimal choices to power interactive applications. Fangjin Yang discusses using Druid for analytics and explains why the architecture is well suited to power analytic dashboards.
Whether we're talking about spam emails, merging records, or investigating clusters, there are many times when having a measure of how alike things are makes them easier to work with (e.g., with unstructured data that isn't incorporated into your data models). Melissa Santos offers a practical approach to creating a distance metric and validating with business owners that it provides value.
Data visualizations using interactive holograms help us make smarter decisions and explore ideas faster by inspecting every vantage point of our data and interacting with it in new, more personal and human ways. There are new rules for the new world. Join Brad Sarsfield as he explores and experiments with the possibilities of the next generation of data visualization experiences.
Traditional ways of visualizing data lineage provide static mapping source datasets to various targets or outputs. As the breadth of analysis occurring in schema-on-read environments increases, tracking how elements of the data were derived is critical. Sean Kandel introduces a new way to visualize data lineage allowing stakeholders a transparent view into their data.
Which suppliers are most likely to have delivery or quality issues? Does service, product placement, or price make the biggest difference in customer sentiment? Text data from sources like email and social media can give answers. Mark Turner explains how to see the associations between any two variables in text data by combining text analytics and the bipartite graph visualization technique.
Visual analysis is changing in the era of GPU clusters. Now that scale compute is easier, the bottleneck is mapping data to visualizations and intelligently interacting with them. Using datasets uploaded to Graphistry, Leo Meyerovich provides a glimpse into the emerging workflows for graph and linked event analysis and offers common tricks for success.
Airbnb developed Caravel to provide all employees with interactive access to data while minimizing friction. Caravel's main goal is to make it easy to slice, dice, and visualize data. Maxime Beauchemin explains how Caravel empowers each and every employee to perform analytics at the speed of thought.
Though visualization is used in data science to understand the shape of the data, it's not widely used for statistical models, which are evaluated based on numerical summaries. Amit Kapoor explores model visualization, which aids in understanding the shape of the model, the impact of parameters and input data on the model, the fit of the model, and where it can be improved.
Stephen Pratt, the CEO of Noodle.ai and former head of Watson for IBM GBS, presents a shareholder value perspective on why enterprise artificial intelligence (eAI) will be the single largest competitive differentiator in business over the next five yearsand what you can do to end up on top.
Data should be something you can see, feel, hear, taste, and touch. Drawing on real-world examples, Cameron Turner, Brad Sarsfield, Hanna Kang-Brown, and Evan Macmillan cover the emerging field of sensory data visualization, including data sonification, and explain where it's headed in the future.
Tianhui Li and Robert Schroll of the Data Incubator offer a foundation in building intelligent business applications using machine learning, walking you through all the steps to prototyping and productiondata cleaning, feature engineering, model building and evaluation, and deploymentand diving into an application for anomaly detection and a personalized recommendation engine.
Whether we're talking about spam emails, merging records, or investigating clusters, there are many times when having a measure of how alike things are makes them easier to work with (e.g., with unstructured data that isn't incorporated into your data models). Melissa Santos offers a practical approach to creating a distance metric and validating with business owners that it provides value.
Data visualizations using interactive holograms help us make smarter decisions and explore ideas faster by inspecting every vantage point of our data and interacting with it in new, more personal and human ways. There are new rules for the new world. Join Brad Sarsfield as he explores and experiments with the possibilities of the next generation of data visualization experiences.
FINRA ingests over 50 billion records of stock market trading data daily into multipetabyte databases. Janaki Parameswaran and Kishore Ramachandran explain how FINRA technology integrates data feeds from disparate systems to provide analytics and visuals for regulating equities, options, and fixed-income markets.
Traditional ways of visualizing data lineage provide static mapping source datasets to various targets or outputs. As the breadth of analysis occurring in schema-on-read environments increases, tracking how elements of the data were derived is critical. Sean Kandel introduces a new way to visualize data lineage allowing stakeholders a transparent view into their data.
Which suppliers are most likely to have delivery or quality issues? Does service, product placement, or price make the biggest difference in customer sentiment? Text data from sources like email and social media can give answers. Mark Turner explains how to see the associations between any two variables in text data by combining text analytics and the bipartite graph visualization technique.
Visual analysis is changing in the era of GPU clusters. Now that scale compute is easier, the bottleneck is mapping data to visualizations and intelligently interacting with them. Using datasets uploaded to Graphistry, Leo Meyerovich provides a glimpse into the emerging workflows for graph and linked event analysis and offers common tricks for success.
Airbnb developed Caravel to provide all employees with interactive access to data while minimizing friction. Caravel's main goal is to make it easy to slice, dice, and visualize data. Maxime Beauchemin explains how Caravel empowers each and every employee to perform analytics at the speed of thought.
Uma Raghavan explains why you're about to see companies whose business models depend on using their customers' data, like Facebook, Google, and many others, scramble to keep up with the flood of new and evolving laws on data privacy.
Though visualization is used in data science to understand the shape of the data, it's not widely used for statistical models, which are evaluated based on numerical summaries. Amit Kapoor explores model visualization, which aids in understanding the shape of the model, the impact of parameters and input data on the model, the fit of the model, and where it can be improved.
Stephen Pratt, the CEO of Noodle.ai and former head of Watson for IBM GBS, presents a shareholder value perspective on why enterprise artificial intelligence (eAI) will be the single largest competitive differentiator in business over the next five yearsand what you can do to end up on top.
Data should be something you can see, feel, hear, taste, and touch. Drawing on real-world examples, Cameron Turner, Brad Sarsfield, Hanna Kang-Brown, and Evan Macmillan cover the emerging field of sensory data visualization, including data sonification, and explain where it's headed in the future.
The road to a data-driven business is paved with hard-won lessons, painful mistakes, and clever insights. We're introducing a new Tutorial Day track packed with case studies, where you can hear from practitioners across a wide range of industries.
Sridhar Alla and Kiran Muglurmath explain how real-time analytics on Comcast Xfinity set-top boxes (STBs) help drive several customer-facing and internal data-science-oriented applications and how Comcast uses Kudu to fill the gaps in batch and real-time storage and computation needs, allowing Comcast to process the high-speed data without the elaborate solutions needed till now.
Neha Narkhede explains how Apache Kafka serves as a foundation to streaming data applications that consume and process real-time data streams and introduces Kafka Connect, a system for capturing continuous data streams, and Kafka Streams, a lightweight stream processing library. Neha also describes the lessons companies like LinkedIn learned building massive streaming data architectures.
Watermarks are a system for measuring progress and completeness in out-of-order streaming systems and are utilized to emit correct results in a timely manner. Given the trend toward out-of-order processing in existing streaming systems, watermarks are an increasingly important tool when designing streaming pipelines. Slava Chernyak explains watermarks and explores real-world applications.
Triggers specify when a stage of computation should emit output. With a small language of primitive conditions, triggers provide the flexibility to tailor a streaming pipeline to a variety of use cases and data sources. Kenneth Knowles delves into the details of language- and runner-independent semantics for triggers in Apache Beam and explores real-world implementations in Google Cloud Dataflow.
Modern cars produce data. Lots of data. And Formula 1 cars produce more than their fair share. Ted Dunning presents a demo of how data streaming can be applied to the analytics problems posed by modern motorsports. Although he won't be bringing Formula 1 cars to the talk, Ted demonstrates a physics-based simulator to analyze realistic data from simulated cars.
You may have successfully made the transition from single machines and one-off solutions to large, distributed stream infrastructures in your data center. But what if one data center is not enough? Ewen Cheslack-Postava explores resilient multi-data-center architecture with Apache Kafka, sharing best practices for data replication and mirroring as well as disaster scenarios and failure handling.
Opportunities in the industrial world are expected to outpace consumer business cases. Time series data is growing exponentially as new machines get connected. Venkatesh Sivasubramanian and Luis Ramos explain how GE makes it faster and easier for systems to access (using a common layer) and perform analytics on a massive volume of time series data by combining Apache Apex, Spark, and Kudu.
Moty Fania shares Intels IT experience implementing an on-premises IoT platform for internal use cases. The platform was designed as a multitenant platform with built-in analytical capabilities and based on open source big data technologies and containers. Moty highlights the lessons learned from this journey with a thorough review of the platforms architecture.
The road to a data-driven business is paved with hard-won lessons, painful mistakes, and clever insights. We're introducing a new Tutorial Day track packed with case studies, where you can hear from practitioners across a wide range of industries.
Sridhar Alla and Kiran Muglurmath explain how real-time analytics on Comcast Xfinity set-top boxes (STBs) help drive several customer-facing and internal data-science-oriented applications and how Comcast uses Kudu to fill the gaps in batch and real-time storage and computation needs, allowing Comcast to process the high-speed data without the elaborate solutions needed till now.
Neha Narkhede explains how Apache Kafka serves as a foundation to streaming data applications that consume and process real-time data streams and introduces Kafka Connect, a system for capturing continuous data streams, and Kafka Streams, a lightweight stream processing library. Neha also describes the lessons companies like LinkedIn learned building massive streaming data architectures.
Watermarks are a system for measuring progress and completeness in out-of-order streaming systems and are utilized to emit correct results in a timely manner. Given the trend toward out-of-order processing in existing streaming systems, watermarks are an increasingly important tool when designing streaming pipelines. Slava Chernyak explains watermarks and explores real-world applications.
Triggers specify when a stage of computation should emit output. With a small language of primitive conditions, triggers provide the flexibility to tailor a streaming pipeline to a variety of use cases and data sources. Kenneth Knowles delves into the details of language- and runner-independent semantics for triggers in Apache Beam and explores real-world implementations in Google Cloud Dataflow.
Time series and event data form the basis for real-time insights about the performance of businesses such as ecommerce, the IoT, and web services, but gaining these insights involves designing a learning system that scales to millions and billions of data streams. Ira Cohen outlines a system that performs real-time machine learning and analytics on streams at massive scale.
Modern cars produce data. Lots of data. And Formula 1 cars produce more than their fair share. Ted Dunning presents a demo of how data streaming can be applied to the analytics problems posed by modern motorsports. Although he won't be bringing Formula 1 cars to the talk, Ted demonstrates a physics-based simulator to analyze realistic data from simulated cars.
Jim Scott outlines the core tenets of a message-driven architecture and explains its importance in real-time big data-enabled distributed systems within the realm of finance.
You may have successfully made the transition from single machines and one-off solutions to large, distributed stream infrastructures in your data center. But what if one data center is not enough? Ewen Cheslack-Postava explores resilient multi-data-center architecture with Apache Kafka, sharing best practices for data replication and mirroring as well as disaster scenarios and failure handling.
Opportunities in the industrial world are expected to outpace consumer business cases. Time series data is growing exponentially as new machines get connected. Venkatesh Sivasubramanian and Luis Ramos explain how GE makes it faster and easier for systems to access (using a common layer) and perform analytics on a massive volume of time series data by combining Apache Apex, Spark, and Kudu.
Yaron Haviv explains how to design real-time IoT and FSI applications, leveraging Spark with advanced data frame acceleration. Yaron then presents a detailed, practical use case, diving deep into the architectural paradigm shift that makes the powerful processing of millions of events both efficient and simple to program.
Moty Fania shares Intels IT experience implementing an on-premises IoT platform for internal use cases. The platform was designed as a multitenant platform with built-in analytical capabilities and based on open source big data technologies and containers. Moty highlights the lessons learned from this journey with a thorough review of the platforms architecture.
Apache Spark is written in Scala. Hence, many if not most data engineers adopting Spark are also adopting Scala, while most data scientists continue to use Python and R. Dean Wampler offers an overview of the core features of Scala you need to use Spark effectively, using hands-on exercises with the Spark APIs.
Fifteen years ago, Webvan spectacularly failed to bring grocery delivery online. Speculation has been high that the current wave of on-demand grocery delivery startups will meet similar fates. Jeremy Stanley explains why this time the story will be differentdata science is the key.
In a panel discussion, top-tier VCs look over the horizon and consider the big trends in big data, explaining what they think the field will look like a few years (or more) down the road. Join us to hear about the trends that everyone is seeing and areas for investment that they find exciting.
Uber, Netflix, LinkedIn, Tesla, Stitch Fix, Earnestthe list of digital disruptors using data to steal customers grows every month. But is it just that these firms are data driven? Is because they have smart data scientists and Hadoop? The secret to their success is that these firms go further in order to be insight driven. Brian Hopkins explains what they're doing and how to join them.
At Strata + Hadoop World 2012, Amy O'Connor and her daughter Danielle Dean shared how they learned and built data science skills at Nokia. This year, Amy and Danielle explore how the landscape in the world of data science has changed in the past four years and explain how to be successful deriving value from data today.
With the advent of smart grid technology, the quantity of data collected by electrical utilities has increased by 35 orders of magnitude. To make full use of this data, utilities must expand their analytical capabilities and develop new analytical techniques. Kim Montgomery discusses some ways that big data tools are advancing the practice of preventative maintenance in the utility industry.
Apache Spark is written in Scala. Hence, many if not most data engineers adopting Spark are also adopting Scala, while most data scientists continue to use Python and R. Dean Wampler offers an overview of the core features of Scala you need to use Spark effectively, using hands-on exercises with the Spark APIs.
How do you reconcile the business opportunity of big data and data science with the sea of possible technologies? Fundamentally, data should serve the strategic imperatives of a businessthose key aspirations that define an organizations future vision. Edd Wilder-James and Colette Glaeser explain how to create a modern data strategy that powers data-driven business.
Fifteen years ago, Webvan spectacularly failed to bring grocery delivery online. Speculation has been high that the current wave of on-demand grocery delivery startups will meet similar fates. Jeremy Stanley explains why this time the story will be differentdata science is the key.
In a panel discussion, top-tier VCs look over the horizon and consider the big trends in big data, explaining what they think the field will look like a few years (or more) down the road. Join us to hear about the trends that everyone is seeing and areas for investment that they find exciting.
Uber, Netflix, LinkedIn, Tesla, Stitch Fix, Earnestthe list of digital disruptors using data to steal customers grows every month. But is it just that these firms are data driven? Is because they have smart data scientists and Hadoop? The secret to their success is that these firms go further in order to be insight driven. Brian Hopkins explains what they're doing and how to join them.
Leading companies that are getting the most out of their data are not focusing on queries and data lakes; they are actively integrating analytics into their operations. Jack Norris reviews three customer case studies in ad/media, financial services, and healthcare to show how a focus on real-time data streams can transform the development, deployment, and future agility of applications.
Daniel Mintz dives into case studies from three companiesThredUp, Twilio, and Warby Parkerthat use data to generate sustainable competitive advantages in their industries.
In 1853, Britains workshops built 90 new gunboats for the Royal Navy in just 90 daysan astonishing feat of engineering made possible by industrial standardization. Snowplow's Alexander Dean argues that data-sophisticated corporations need a new standardization of their own, in the form of schema registries like Confluent Schema Registry or Snowplows own Iglu.
The history of the digital age is being written in photographs. To innovate in the visual age, we have to crack the visual code. Susan Etlinger explores why the ability to understand why one photo resonates and one doesnt can make or break reputations, spark new products or lines of business, and make or save millions of dollars.
Todays online storefronts are good at procuring transactions but poor in managing customers. Rupert Steffner explains why online retailers must build a complementary intelligence to perceive and reason on customer signals to better manage opportunities and risks along the customer journey. Individually managed customer experience is retailers' next challenge, and fueling AI is the right answer.
At Strata + Hadoop World 2012, Amy O'Connor and her daughter Danielle Dean shared how they learned and built data science skills at Nokia. This year, Amy and Danielle explore how the landscape in the world of data science has changed in the past four years and explain how to be successful deriving value from data today.
Given the recent demand for data analytics and data science skills, adequately testing and qualifying candidates can be a daunting task. Interviewing hundreds of individuals of varying experience and skill levels requires a standardized approach. Tanya Cashorali explores strategies, best practices, and deceptively simple interviewing techniques for data analytics and data science candidates.
With the advent of smart grid technology, the quantity of data collected by electrical utilities has increased by 35 orders of magnitude. To make full use of this data, utilities must expand their analytical capabilities and develop new analytical techniques. Kim Montgomery discusses some ways that big data tools are advancing the practice of preventative maintenance in the utility industry.
Tim Williamson and Emil Eifrem explain how organizations can use graph databases to operationalize insights from big data, drawing on the real-life example of Monsantos use of graph databases to conduct real-time graph analysis of the companys data to transform the business in ways that were previously impossible.
While other industries have embraced the digital era, healthcare is still playing catch-up. Kaiser Permanente has been a leader in healthcare technology and first started using computing to improve healthcare results in the 1960s. Taposh Roy, Rajiv Synghal, and Sabrina Dahlgren offer an overview of Kaisers big data strategy and explain how other organizations can adopt similar strategies.
Visa, the worlds largest electronic payments network, is transforming the way it manages data: database appliances are giving way to Hadoop and HBase; proprietary ETL technologies are being replaced by Spark; and enterprise warehouse data models will be complemented by flexible data schemas. Nandu Jayakumar explores the adoption of big data practices at a conservative, financial enterprise.
Shirshanka Das and Yael Garten describe how LinkedIn redesigned its data analytics ecosystem in the face of a significant product rewrite, covering the infrastructure changes, such as client-side activity tracking, a unified reporting platform, and data virtualization techniques to simplify migration, that enable LinkedIn to roll out future product innovations with minimal downstream impact.
Siva Raghupathy demonstrates how to use Hadoop innovations in conjunction with Amazon Web Services (cloud) innovations.
Henry Robinson and Justin Erickson explain how to best take advantage of the flexibility and cost-effectiveness of the cloud with your BI and SQL analytic workloads using Apache Hadoop and Apache Impala (incubating), covering the architectural considerations, best practices, tuning, and functionality available when deploying or migrating BI and SQL analytic workloads to the cloud.
Machine-learning tools promise to help solve data curation problems. While the principles are well understood, the engineering details in configuring and deploying ML techniques are the biggest hurdle. Ihab Ilyas explains why leveraging data semantics and domain-specific knowledge is key in delivering the optimizations necessary for truly scalable ML curation solutions.
Many initiatives for running applications inside containers have been scoped to run on a single host. Using Docker containers for large-scale environments poses new challenges, especially for big data applications like Hadoop. Thomas Phelan shares lessons learned and some tips and tricks on how to Dockerize your big data applications in a reliable, scalable, and high-performance environment.
Rick McFarland explains how the Hearst Corporation utilizes big data and analytics tools like Spark and Kinesis to stream click data in real-time from its 300+ websites worldwide. This streaming process feeds an editorial tool called Buzzing@Hearst, which provides instant feedback to authors on what is trending across the Hearst network.
Society is standing at the gates of what promises to be a profound transformation in the nature of work, the role of data, and the future of the world's major industries. Intelligent machines will play a variety of roles in every sector of the economy. David Beyer explores a number of key industries and their idiosyncratic journeys on the way to adopting AI.
Scikit-learn, which provides easy-to-use interfaces to perform advances analysis and build powerful predictive models, has emerged as one of the most popular open source machine-learning toolkits. Using scikit-learn and Python as examples, Andreas Mueller offers an overview of basic concepts of machine learning, such as supervised and unsupervised learning, cross-validation, and model selection.
Bryan Van de Ven and Sarah Bird demonstrate how to build intelligent apps in a week with Bokeh, Python, and optimization.
Tim Williamson and Emil Eifrem explain how organizations can use graph databases to operationalize insights from big data, drawing on the real-life example of Monsantos use of graph databases to conduct real-time graph analysis of the companys data to transform the business in ways that were previously impossible.
While other industries have embraced the digital era, healthcare is still playing catch-up. Kaiser Permanente has been a leader in healthcare technology and first started using computing to improve healthcare results in the 1960s. Taposh Roy, Rajiv Synghal, and Sabrina Dahlgren offer an overview of Kaisers big data strategy and explain how other organizations can adopt similar strategies.
Visa, the worlds largest electronic payments network, is transforming the way it manages data: database appliances are giving way to Hadoop and HBase; proprietary ETL technologies are being replaced by Spark; and enterprise warehouse data models will be complemented by flexible data schemas. Nandu Jayakumar explores the adoption of big data practices at a conservative, financial enterprise.
Shirshanka Das and Yael Garten describe how LinkedIn redesigned its data analytics ecosystem in the face of a significant product rewrite, covering the infrastructure changes, such as client-side activity tracking, a unified reporting platform, and data virtualization techniques to simplify migration, that enable LinkedIn to roll out future product innovations with minimal downstream impact.
Siva Raghupathy demonstrates how to use Hadoop innovations in conjunction with Amazon Web Services (cloud) innovations.
Henry Robinson and Justin Erickson explain how to best take advantage of the flexibility and cost-effectiveness of the cloud with your BI and SQL analytic workloads using Apache Hadoop and Apache Impala (incubating), covering the architectural considerations, best practices, tuning, and functionality available when deploying or migrating BI and SQL analytic workloads to the cloud.
Machine-learning tools promise to help solve data curation problems. While the principles are well understood, the engineering details in configuring and deploying ML techniques are the biggest hurdle. Ihab Ilyas explains why leveraging data semantics and domain-specific knowledge is key in delivering the optimizations necessary for truly scalable ML curation solutions.
Twitter generates billions and billions of events per day. Analyzing these events in real time presents a massive challenge. Karthik Ramasamy offers an overview of the end-to-end real-time stack Twitter designed in order to meet this challenge, consisting of DistributedLog (the distributed and replicated messaging system) and Heron (the streaming system for real-time computation).
Many initiatives for running applications inside containers have been scoped to run on a single host. Using Docker containers for large-scale environments poses new challenges, especially for big data applications like Hadoop. Thomas Phelan shares lessons learned and some tips and tricks on how to Dockerize your big data applications in a reliable, scalable, and high-performance environment.
Rick McFarland explains how the Hearst Corporation utilizes big data and analytics tools like Spark and Kinesis to stream click data in real-time from its 300+ websites worldwide. This streaming process feeds an editorial tool called Buzzing@Hearst, which provides instant feedback to authors on what is trending across the Hearst network.
Society is standing at the gates of what promises to be a profound transformation in the nature of work, the role of data, and the future of the world's major industries. Intelligent machines will play a variety of roles in every sector of the economy. David Beyer explores a number of key industries and their idiosyncratic journeys on the way to adopting AI.
JeanCarlo Bonilla, Susan Sun, and Caitlin Augustin explore how DataKind volunteer teams navigate the road to social impact by automating evidence collection for conservationists and helping expand the reach of mobile surveys so that more voices can be heard.
Alex Bordei walks you through the steps required to build a data lake in the cloud and connect it to on-premises environments, covering best practices in architecting cloud data lakes and key aspects such as performance, security, data lineage, and data maintenance. The technologies presented range from basic HDFS storage to real-time processing with Spark Streaming.
Companies making data-driven decisions must consider critical legal obligations that may apply to the collection and use of data. Failing to do so has landed many tech stars and startups in hot legal water. Attorneys Kristi Wolff and Crystal Skelton discuss privacy, data security, and other legal considerations for using data across several industry types.
How can we usher in a future of data-driven decision making that is characterized by morenot lessaccountability and accessibility? Brett Goldstein discusses the imperative to couple new developments in data science with a renewed commitment to transparency and open sourcewith a particular focus on open source models to optimize deployment of policing resources.
How are users meant to interpret the influence of big data and personalization in their targeted experiences? What signals do we have to show us how our data is used, how it improves or constrains our experience? Sara Watson explains that in order to develop normative opinions to shape policy and practice, users need means to guide their experiencethe personalization spectrum.
Enterprises are increasingly demanding real-time analytics and insights. Tony Ng offers an overview of Pulsar, an open source real-time streaming system used at eBay. Tony explains how Pulsar integrates Kafka, Kylin, and Druid to provide flexibility and scalability in event and metrics consumption.
The value of online user accounts has led to a significant increase in account takeover (ATO) attacks. Cyber criminals create armies of compromised accounts to perform attacks including fraudulent transactions, bank withdrawals, reward program theft, and more. Fang Yu explains how the latest in big data technology is helping turn the tide on ATO campaigns.
Cybersecurity has become a data problem and thus needs the best-in-breed big data tools. Joshua Patterson, Michael Wendt, and Keith Kraus explain how Accenture Labs's Cybersecurity team is using Apache Kafka, Spark, and Flink to stream data into Blazegraph and Datastax Graph to accelerate cyber defense.
With Apache Kakfa 0.9, the community has introduced a number of features to make data streams secure. Jun Rao explains the motivation for making these changes, discusses the design of Kafka security, and demonstrates how to secure a Kafka cluster. Jun also covers common pitfalls in securing Kafka and talks about ongoing security work.
Li Li and Hao Hao elaborate the architecture of Apache Sentry + RecordService for Hadoop in the cloud, which provides unified, fine-grained authorization via role- and attribute-based access control, to encourage attendees to adopt Apache Sentry and RecordService to protect sensitive data on the multitenant cloud across the Hadoop ecosystem.
Apache Kudu was first announced as a public beta release at Strata NYC 2015 and recently reached 1.0. This conference marks its one year anniversary as a public open source project. Todd Lipcon offers a very brief refresher on the goals and feature set of the Kudu storage engine, covering the development that has taken place over the last year.
Public cloud usage for Hadoop workloads is accelerating. Consequently, Hadoop components have adapted to leverage cloud infrastructure. Andrei Savu, Vinithra Varadharajan, Matthew Jacobs, and Jennifer Wu explore best practices for Hadoop deployments in the public cloud and provide detailed guidance for deploying, configuring, and managing Hive, Spark, and Impala in the public cloud.
Public cloud usage for Hadoop workloads is accelerating. Consequently, Hadoop components have adapted to leverage cloud infrastructure. Andrei Savu, Vinithra Varadharajan, Matthew Jacobs, and Jennifer Wu explore best practices for Hadoop deployments in the public cloud and provide detailed guidance for deploying, configuring, and managing Hive, Spark, and Impala in the public cloud.
Data 101 introduces you to core principles of data architecture, teaches you how to build and manage successful data teams, and inspires you to do more with your data through real-world applications. Setting the foundation for deeper dives on the following days of Strata + Hadoop World, Data 101 reinforces data fundamentals and helps you focus on how data can solve your business problems.
Spark is white-hot at the moment, but why does it matter? Developers are usually the first to understand why some technologies cause more excitement than others. Edd Wilder-James relates this insider knowledge, providing a tour through the hottest emerging data technologies of 2016 to explain why theyre exciting in terms of both new capabilities and the new economies they bring.
Ben Sharma uses popular cloud-based use cases to explore how to effectively and safely leverage big data in the cloud to achieve business goals. Now is the time to get the jump on this trend before your competition gets the upper hand.
In the age of big data analytics, smart monitoring and predicting abnormal behavior of corporation mission-critical systems can save large amounts of time and money. Drawing on a real-world case study from EMC, Amihai Savir examines the winding path from idea to viable solution in a corporate environment and walks you through challenges encountered and lessons learned.
In chess, we might think of strategy as finding the patterns that put us in a better position to win. The same holds true for winning with data. Jerry Overton explains how to build and execute real data strategies, sharing basic methods for building strategic maps and launching data projects that will shorten the time it takes to gain insight into your most important business questions.
Data scientists use statistics to reach meaningful conclusions about data. Unfortunately, statistical tools are often misapplied, resulting in errors that cost both time and money. Deborah Berebichez presents examples of egregious misuses of statistics in business, technology, science, and the media and outlines the simple steps that can reduce the chance of being fooled by statistics.
Julie Rodriguez introduces new visualization methods that provide greater clarity to your data, demonstrating how to show associations and links between datasets to understand the impact one value has on another, how to communicate time-lapsed data to understand the context of an event, and how to display multiple variables to analyze and compare attributes.
Garrett Grolemund and Nathan Stephens explore the new sparklyr package by RStudio, which provides a familiar interface between the R language and Apache Spark and communicates with the Spark SQL and the Spark ML APIs so R users can easily manipulate and analyze data at scale.
Visualizations are a key part of conveying any dataset. D3 is the most popular, easiest, and most extensible way to get your data online in an interactive way. Brian Suda outlines best practices for good data visualizations and explains how you can build them using D3.
Picking the best data format depends on what kind of data you have and how you plan to use it. Owen O'Malley outlines the performance differences between formats in different use cases and offers an overview of the advantages and disadvantages of each to help you improve the performance of your applications.
Performance tuning your SQL-on-Hadoop deployment may seem overwhelming at times, especially for BI workloads that need interactive response times with high concurrency. Marcel Kornacker and Mostafa Mokhtar simplify the process and cover top performance optimizations for Apache Impala (incubating), from schema design and memory optimization to query tuning.
Adam Bordelon and Mohit Soni demonstrate how projects like Apache Myriad (incubating) can install Hadoop on Mesosphere DC/OS alongside other data center-scale applications, enabling efficient resource sharing and isolation across a variety of distributed applications while sharing the same cluster resources and hence breaking silos.
The new erasure coding feature in Apache Hadoop (HDFS-EC) reduces the storage cost by ~50% compared with 3x replication. Zhe Zhang and Uma Maheswara Rao G present the first-ever performance study of HDFS-EC and share insights on when and how to use the feature.
Crystal Valentine explains how the large graph-processing frameworks that run on Hadoop can be used to detect significantly mutated protein signaling pathways in cancer genomes through a probabilistic analysis of large protein-protein interaction networks, using techniques similar to those used in social network analysis algorithms.
Our ability to extract meaning from unstructured text data has not kept pace with our ability to produce and store it, but recent breakthroughs in recurrent neural networks are allowing us to make exciting progress in computer understanding of language. Building on these new ideas, Michael Williams explores three ways to summarize text and presents prototype products for each approach.
Terry Mcfadden and Priyank Patel discuss Procter and Gamble's three-year journey to enable production applications with on-cluster BI technology, exploring in detail the architecture challenges and choices made by the team along this journey.
Garrett Grolemund and Nathan Stephens explore the new sparklyr package by RStudio, which provides a familiar interface between the R language and Apache Spark and communicates with the Spark SQL and the Spark ML APIs so R users can easily manipulate and analyze data at scale.
Visualizations are a key part of conveying any dataset. D3 is the most popular, easiest, and most extensible way to get your data online in an interactive way. Brian Suda outlines best practices for good data visualizations and explains how you can build them using D3.
Picking the best data format depends on what kind of data you have and how you plan to use it. Owen O'Malley outlines the performance differences between formats in different use cases and offers an overview of the advantages and disadvantages of each to help you improve the performance of your applications.
Performance tuning your SQL-on-Hadoop deployment may seem overwhelming at times, especially for BI workloads that need interactive response times with high concurrency. Marcel Kornacker and Mostafa Mokhtar simplify the process and cover top performance optimizations for Apache Impala (incubating), from schema design and memory optimization to query tuning.
Adam Bordelon and Mohit Soni demonstrate how projects like Apache Myriad (incubating) can install Hadoop on Mesosphere DC/OS alongside other data center-scale applications, enabling efficient resource sharing and isolation across a variety of distributed applications while sharing the same cluster resources and hence breaking silos.
The new erasure coding feature in Apache Hadoop (HDFS-EC) reduces the storage cost by ~50% compared with 3x replication. Zhe Zhang and Uma Maheswara Rao G present the first-ever performance study of HDFS-EC and share insights on when and how to use the feature.
Crystal Valentine explains how the large graph-processing frameworks that run on Hadoop can be used to detect significantly mutated protein signaling pathways in cancer genomes through a probabilistic analysis of large protein-protein interaction networks, using techniques similar to those used in social network analysis algorithms.
Our ability to extract meaning from unstructured text data has not kept pace with our ability to produce and store it, but recent breakthroughs in recurrent neural networks are allowing us to make exciting progress in computer understanding of language. Building on these new ideas, Michael Williams explores three ways to summarize text and presents prototype products for each approach.
Terry Mcfadden and Priyank Patel discuss Procter and Gamble's three-year journey to enable production applications with on-cluster BI technology, exploring in detail the architecture challenges and choices made by the team along this journey.
Xiangrui Meng explores recent community efforts to extend SparkR for scalable advanced analyticsincluding summary statistics, single-pass approximate algorithms, and machine-learning algorithms ported from Spark MLliband shows how to integrate existing R packages with SparkR to accelerate existing R workflows.
Amitai Armon and Nir Lotan outline a new, free software tool that enables the creation of deep learning models quickly and easily. The tool is based on existing deep learning frameworks and incorporates extensive optimizations that provide high performance on standard CPUs.
Many areas of applied machine learning require models optimized for rare occurrences, such as class imbalances, and users actively attempting to subvert the system (adversaries). Brendan Herger offers an overview of multiple published techniques that specifically attempt to address these issues and discusses lessons learned by the Data Innovation Lab at Capital One.
Jeff Carpenter describes how data modeling can be a key enabler of microservice architectures for transactional and analytics systems, including service identification, schema design, and event streaming.
The need to find efficiencies in healthcare is becoming paramount as our society and the global population continue to grow and live longer. Navdeep Alam shares his experience and reviews current and emerging technologies in the marketplace that handle working with unbounded, de-identified patient datasets in the billions of rows in an efficient and scalable way.
Many challenges exist in designing an SQL-on-Hadoop cluster for production in a multiuser environment with heterogeneous and concurrent query workloads. Jun Liu and Zhaojuan Bian draw on their personal experience to address these challenges, explaining how to determine the right size of your cluster with different combinations of hardware and software resources using a simulation-based approach.
Todd Lipcon and Marcel Kornacker explain how to simplify Hadoop-based data-centric applications with the CRUD (create, read, update, and delete) and interactive analytic functionality of Apache Impala (incubating) and Apache Kudu (incubating).
Kaushik Deka and Phil Jarymiszyn discuss the benefits of a Spark-based feature store, a library of reusable features that allows data scientists to solve business problems across the enterprise. Kaushik and Phil outline three challenges they facedsemantic data integration within a data lake, high-performance feature engineering, and metadata governanceand explain how they overcame them.
Zillow pioneered providing access to unprecedented information about the housing market. Long gone are the days when you needed an agent to get comparables and prior sale and listing data. And with more data, data science has enabled more use cases. Jasjeet Thind explains how Zillow uses Spark and machine learning to transform real estate.
Bas Geerdink offers an overview of the evolution that the Hadoop ecosystem has taken at ING. Since 2013, ING has invested heavily in a central data lake and data management practice. Bas shares historical lessons and best practices for enterprises that are incorporating Hadoop into their infrastructure landscape.
We're likely just at the beginning of data science. The people and things that are starting to be equipped with sensors will enable entirely new classes of problems that will have to be approached more scientifically. Mike Stringer outlines some of the issues that may arise for business, for data scientists, and for society.
Radish Lab teamed up with science news nonprofit Climate Central to transform temperature data from 1,001 US cities into a compelling, simple interactive that received more than 1 million views within three days of launch. Alana Range and Brian Kahn offer an overview of the process of creating a viral, interactive data visualization with teams that regularly produce powerful data stories.
Apache Flink has seen incredible growth during the last year, both in development and usage, driven by the fundamental shift from batch to stream processing. Kostas Tzoumas demonstrates how Apache Flink enables real-time decisions, makes infrastructure less complex, and enables extremely efficient, accurate, and fault-tolerant streaming applications.
The largest challenge for deep learning is scalability. Google has built a large-scale neural network in the cloud and is now sharing that power. Kazunori Sato introduces pretrained ML services, such as the Cloud Vision API and the Speech API, and explores how TensorFlow and Cloud Machine Learning can accelerate custom model training 10x40x with Google's distributed training infrastructure.
Roy Ben-Alta explores the Amazon Kinesis platform in detail and discusses best practices for scaling your core streaming data ingestion pipeline as well as real-world customer use cases and design pattern integration with Amazon Elasticsearch, AWS Lambda, and Apache Spark.
Come learn the basics of stream processing via a guided walkthrough of the most sophisticated and portable stream processing model on the planetApache Beam (incubating). Tyler Akidau and Jesse Anderson cover the basics of robust stream processing (windowing, watermarks, and triggers) with the option to execute exercises on top of the runner of your choiceFlink, Spark, or Google Cloud Dataflow.
Ian Wrigley demonstrates how to leverage the capabilities of Apache Kafka to collect, manage, and process stream data for both big data projects and general-purpose enterprise data integration. Ian covers system architecture, use cases, and how to write applications that publish data to, and subscribe to data from, Kafkano prior knowledge of Kafka required.
Come learn the basics of stream processing via a guided walkthrough of the most sophisticated and portable stream processing model on the planetApache Beam (incubating). Tyler Akidau and Jesse Anderson cover the basics of robust stream processing (windowing, watermarks, and triggers) with the option to execute exercises on top of the runner of your choiceFlink, Spark, or Google Cloud Dataflow.
Ian Wrigley demonstrates how to leverage the capabilities of Apache Kafka to collect, manage, and process stream data for both big data projects and general-purpose enterprise data integration. Ian covers system architecture, use cases, and how to write applications that publish data to, and subscribe to data from, Kafkano prior knowledge of Kafka required.
Despite widespread adoption, machine-learning models remain mostly black boxes, making it very difficult to understand the reasons behind a prediction. Such understanding is fundamentally important to assess trust in a model before we take actions based on a prediction or choose to deploy a new ML service. Carlos Guestrin offers a general approach for explaining predictions made by any ML model.
Data science has always been a focus at eHarmony, but recently more business units have needed data-driven models. Jonathan Morra introduces Aloha, an open source project that allows the modeling group to quickly deploy type-safe accurate models to production, and explores how eHarmony creates models with Apache Spark and how it uses them.
Clustering algorithms produce vectors of information, which are almost surely difficult to interpret. These are then laboriously translated by data scientists into insights for influencing product and executive decisions. June Andrews offers an overview of a human-in-the-loop method used at Pinterest and LinkedIn that has lead to fast, accurate, and pertinent human-readable insights.
Predicting which stories will become popular is an invaluable tool for newsrooms. Eui-Hong Han and Shuguang Wang explain how the Washington Post predicts what stories on its site will be popular with readers and share the challenges they faced in developing the tool and metrics on how they refined the tool to increase accuracy.
Can machines be creative? Josh Patterson and David Kale offer a practical demonstrationan interactive Twitter bot that users can ping to receive a response dynamically generated by a conditional recurrent neural net implemented using DL4Jthat suggests the answer may be yes.
Much of the success of deep learning in recent years can be attributed to scalebigger datasets and more computing powerbut scale can quickly become a problem. Distributed, asynchronous computing in heterogenous environments is complex, hard to debug, and hard to profile and optimize. Martin Wicke demonstrates how to automate or abstract away such complexity, using TensorFlow as an example.
Deep learning has taken us a few steps further toward achieving AI for a man-machine interface. However, deep learning technologies like speech recognition and natural language processing remain a mystery to many. Yishay Carmiel reviews the history of deep learning, the impact it's made, recent breakthroughs, interesting solved and open problems, and what's in store for the future.
David Talby and Claudiu Branzan lead a live demo of an end-to-end system that makes nontrivial clinical inferences from free-text patient records. Infrastructure components include Kafka, Spark Streaming, Spark, Titan, and Elasticsearch; data science components include custom UIMA annotators, curated taxonomies, machine-learned dynamic ontologies, and real-time inferencing.
Amir Hajian, Khaled Ammar, and Alex Constandache offer an approach to mining a large dataset to predict the electability of hypothetical candidates in the US presidential election race, using machine learning, natural language processing, and deep learning on an infrastructure that includes Spark and Elasticsearch, which serves as the backbone of the mobile game White House Run.
In the realm of predictive maintenance, the event of interest is an equipment failure. In real scenarios, this is usually a rare event. Unless the data collection has been taking place over a long period of time, the data will have very few of these events or, in the worst case, none at all. Danielle Dean and Shaheen Gauher discuss the various ways of building and evaluating models for such data.
How can the value of a patent be quantified? Josh Lemaitre explores how Thomson Reuters Labs approached this problem by applying machine learning to the patent corpus in an effort to predict those most likely to be enforced via litigation. Josh covers infrastructure, methods, challenges, and opportunities for future research.
What are the essential components of a data platform? John Akred, Mauricio Vacas, and Stephen O'Sullivan explain how the various parts of the Hadoop, Spark, and big data ecosystems fit together in production to create a data platform supporting batch, interactive, and real-time analytical workloads.
Jonathan Seidman, Gwen Shapira, Mark Grover, and Ted Malaska demonstrate how to architect a modern, real-time big data platform and explain how to leverage components like Kafka, Impala, Kudu, Spark Streaming, and Spark SQL with Hadoop to enable new forms of data processing and analytics such as real-time ETL, change data capture, and machine learning.
Despite widespread adoption, machine-learning models remain mostly black boxes, making it very difficult to understand the reasons behind a prediction. Such understanding is fundamentally important to assess trust in a model before we take actions based on a prediction or choose to deploy a new ML service. Carlos Guestrin offers a general approach for explaining predictions made by any ML model.
Data science has always been a focus at eHarmony, but recently more business units have needed data-driven models. Jonathan Morra introduces Aloha, an open source project that allows the modeling group to quickly deploy type-safe accurate models to production, and explores how eHarmony creates models with Apache Spark and how it uses them.
Clustering algorithms produce vectors of information, which are almost surely difficult to interpret. These are then laboriously translated by data scientists into insights for influencing product and executive decisions. June Andrews offers an overview of a human-in-the-loop method used at Pinterest and LinkedIn that has lead to fast, accurate, and pertinent human-readable insights.
Predicting which stories will become popular is an invaluable tool for newsrooms. Eui-Hong Han and Shuguang Wang explain how the Washington Post predicts what stories on its site will be popular with readers and share the challenges they faced in developing the tool and metrics on how they refined the tool to increase accuracy.
Can machines be creative? Josh Patterson and David Kale offer a practical demonstrationan interactive Twitter bot that users can ping to receive a response dynamically generated by a conditional recurrent neural net implemented using DL4Jthat suggests the answer may be yes.
Much of the success of deep learning in recent years can be attributed to scalebigger datasets and more computing powerbut scale can quickly become a problem. Distributed, asynchronous computing in heterogenous environments is complex, hard to debug, and hard to profile and optimize. Martin Wicke demonstrates how to automate or abstract away such complexity, using TensorFlow as an example.
Deep learning has taken us a few steps further toward achieving AI for a man-machine interface. However, deep learning technologies like speech recognition and natural language processing remain a mystery to many. Yishay Carmiel reviews the history of deep learning, the impact it's made, recent breakthroughs, interesting solved and open problems, and what's in store for the future.
David Talby and Claudiu Branzan lead a live demo of an end-to-end system that makes nontrivial clinical inferences from free-text patient records. Infrastructure components include Kafka, Spark Streaming, Spark, Titan, and Elasticsearch; data science components include custom UIMA annotators, curated taxonomies, machine-learned dynamic ontologies, and real-time inferencing.
Amir Hajian, Khaled Ammar, and Alex Constandache offer an approach to mining a large dataset to predict the electability of hypothetical candidates in the US presidential election race, using machine learning, natural language processing, and deep learning on an infrastructure that includes Spark and Elasticsearch, which serves as the backbone of the mobile game White House Run.
In the realm of predictive maintenance, the event of interest is an equipment failure. In real scenarios, this is usually a rare event. Unless the data collection has been taking place over a long period of time, the data will have very few of these events or, in the worst case, none at all. Danielle Dean and Shaheen Gauher discuss the various ways of building and evaluating models for such data.
How can the value of a patent be quantified? Josh Lemaitre explores how Thomson Reuters Labs approached this problem by applying machine learning to the patent corpus in an effort to predict those most likely to be enforced via litigation. Josh covers infrastructure, methods, challenges, and opportunities for future research.
The real power and value proposition of Apache Spark is in building a unified use case that combines ETL, batch analytics, real-time stream analysis, machine learning, graph processing, and visualizations. Through hands-on examples, Zoltan Toth explores various Wikipedia datasets to illustrate a variety of ideal programming paradigms.
Ram Sriharsha reviews major developments in Apache Spark 2.0 and discusses future directions for the project to make Spark faster and easier to use for a wider array of workloads, with an emphasis on API evolution, single-node performance (Project Tungsten Phase 3), and Structured Streaming.
Ted Malaska and Mark Grover cover the top five things that prevent Spark developers from getting the most out of their Spark clusters. When these issues are addressed, it is not uncommon to see the same job running 10x or 100x faster with the same clusters and the same data, using just a different approach.
Spark's efficiency and speed can help reduce the TCO of existing clusters. This is because Spark's performance advantages allow it to complete processing in drastically shorter batch windows with higher performance per dollar. Raj Krishnamurthy offers a detailed walk-through of an alternating least squares-based matrix factorization workload able to improve runtimes by a factor of 2.22.
Praveen Murugesan explains how Uber leverages Hadoop and Spark as the cornerstones of its data infrastructure. Praveen details the current data architecture at Uber and outlines some of the unique challenges with data processing Uber faced as well as its approach to solving some key issues in order to continue to power Uber's real-time marketplace.
Swisscom, the leading mobile service provider in Switzerland, also provides data-driven intelligence through the analysis of its mobile network. Its Mobility Insights team works to help administrators understand the flow of people through their location of interest. Franois Garillot explores the platform, tooling, and choices that help achieve this service and some challenges the team has faced.
Drawing on his experiences across 150+ production deployments, Neelesh Srinivas Salian focuses on five common issues observed in a cluster environment setup with Apache Spark (Core, Streaming, and SQL) to help you improve the usability and supportability of Apache Spark and avoid such issues in future deployments.
Structured Streaming is a new effort in Apache Spark to make stream processing simple without the need to learn a new programming paradigm or system. Ram Sriharsha offers an overview of Structured Streaming, discussing its support for event-time, out-of-order/delayed data, sessionization, and integration with the batch data stack to show how it simplifies building powerful continuous applications.
Through collaboration with some of the top payments companies around the world, Intel has developed an end-to-end solution for building fraud detection applications. Yuhao Yang explains how Intel used and extended Spark DataFrames and ML Pipelines to build the tool chain for financial fraud detection and shares the lessons learned during development.
Holden Karau and Seth Hendrickson demonstrate how to do streaming machine learning using Spark's new Structured Streaming and walk you through creating your own streaming model.
Narasimhan Sampath and Avinash Ramineni share how Choice Hotels International used Spark Streaming, Kafka, Spark, and Spark SQL to create an advanced analytics platform that enables business users to be self-reliant by accessing the data they need from a variety of sources to generate customer insights and property dashboards and enable data-driven decisions with minimal IT engagement.
Although Spark gets a lot of attention, we only think about two languages being supportedPython and Scala. Jesse Anderson proves that Java works just as well. With lambdas, we even get syntax comparable to Scala, so Java developers get the best of both worlds without having to learn Scala.
The real power and value proposition of Apache Spark is in building a unified use case that combines ETL, batch analytics, real-time stream analysis, machine learning, graph processing, and visualizations. Through hands-on examples, Zoltan Toth explores various Wikipedia datasets to illustrate a variety of ideal programming paradigms.
Ram Sriharsha reviews major developments in Apache Spark 2.0 and discusses future directions for the project to make Spark faster and easier to use for a wider array of workloads, with an emphasis on API evolution, single-node performance (Project Tungsten Phase 3), and Structured Streaming.
Ted Malaska and Mark Grover cover the top five things that prevent Spark developers from getting the most out of their Spark clusters. When these issues are addressed, it is not uncommon to see the same job running 10x or 100x faster with the same clusters and the same data, using just a different approach.
Spark's efficiency and speed can help reduce the TCO of existing clusters. This is because Spark's performance advantages allow it to complete processing in drastically shorter batch windows with higher performance per dollar. Raj Krishnamurthy offers a detailed walk-through of an alternating least squares-based matrix factorization workload able to improve runtimes by a factor of 2.22.
Praveen Murugesan explains how Uber leverages Hadoop and Spark as the cornerstones of its data infrastructure. Praveen details the current data architecture at Uber and outlines some of the unique challenges with data processing Uber faced as well as its approach to solving some key issues in order to continue to power Uber's real-time marketplace.
Swisscom, the leading mobile service provider in Switzerland, also provides data-driven intelligence through the analysis of its mobile network. Its Mobility Insights team works to help administrators understand the flow of people through their location of interest. Franois Garillot explores the platform, tooling, and choices that help achieve this service and some challenges the team has faced.
Drawing on his experiences across 150+ production deployments, Neelesh Srinivas Salian focuses on five common issues observed in a cluster environment setup with Apache Spark (Core, Streaming, and SQL) to help you improve the usability and supportability of Apache Spark and avoid such issues in future deployments.
Structured Streaming is a new effort in Apache Spark to make stream processing simple without the need to learn a new programming paradigm or system. Ram Sriharsha offers an overview of Structured Streaming, discussing its support for event-time, out-of-order/delayed data, sessionization, and integration with the batch data stack to show how it simplifies building powerful continuous applications.
Through collaboration with some of the top payments companies around the world, Intel has developed an end-to-end solution for building fraud detection applications. Yuhao Yang explains how Intel used and extended Spark DataFrames and ML Pipelines to build the tool chain for financial fraud detection and shares the lessons learned during development.
Holden Karau and Seth Hendrickson demonstrate how to do streaming machine learning using Spark's new Structured Streaming and walk you through creating your own streaming model.
Narasimhan Sampath and Avinash Ramineni share how Choice Hotels International used Spark Streaming, Kafka, Spark, and Spark SQL to create an advanced analytics platform that enables business users to be self-reliant by accessing the data they need from a variety of sources to generate customer insights and property dashboards and enable data-driven decisions with minimal IT engagement.
Although Spark gets a lot of attention, we only think about two languages being supportedPython and Scala. Jesse Anderson proves that Java works just as well. With lambdas, we even get syntax comparable to Scala, so Java developers get the best of both worlds without having to learn Scala.
Learn how Spark and Hadoop enable data scientists to help companies reduce costs, increase profits, improve products, retain customers, and identify new opportunities. Through in-class simulations and exercises, Bruce Martin walks you through applying data science methods to real-world challenges in different industries, offering preparation for data scientist roles in the field.
Learn how Spark and Hadoop enable data scientists to help companies reduce costs, increase profits, improve products, retain customers, and identify new opportunities. Through in-class simulations and exercises, Bruce Martin walks you through applying data science methods to real-world challenges in different industries, offering preparation for data scientist roles in the field.
Learn how Spark and Hadoop enable data scientists to help companies reduce costs, increase profits, improve products, retain customers, and identify new opportunities. Through in-class simulations and exercises, Bruce Martin walks you through applying data science methods to real-world challenges in different industries, offering preparation for data scientist roles in the field.
Mark Grover, Jonathan Seidman, and Ted Malaska, the authors of Hadoop Application Architectures, participate in an open Q&A session on considerations and recommendations for the architecture and design of applications using Hadoop. Come with questions about your use case and its big data architecture or just listen in on the conversation.
John Akred, Stephen O'Sullivan, and Julie Steele will field a wide range of detailed questions about developing a modern data strategy, architecting a data platform, and best practices for CDO and its evolving role. Even if you dont have a specific question, join in to hear what others are asking.
Martin Wicke and Josh Gordon field questions related to their tutorial, Deep Learning with TensorFlow.
Join Apache Beam and Google Cloud Dataflow engineers to ask all of your questions about stream processing. They'll answer everything from general streaming questions about concepts, semantics, capabilities, limitations, etc. to questions specifically related to Apache Beam, Google Cloud Dataflow, and other common streaming systems (Flink, Spark, Storm, etc.).
Join Xiangrui Meng and Ram Sriharsha to discuss the state of Spark.
The real power and value proposition of Apache Spark is in building a unified use case that combines ETL, batch analytics, real-time stream analysis, machine learning, graph processing, and visualizations. Brian Clapper employs hands-on exercises using various Wikipedia datasets to illustrate the variety of ideal programming paradigms Spark makes possible.
The real power and value proposition of Apache Spark is in building a unified use case that combines ETL, batch analytics, real-time stream analysis, machine learning, graph processing, and visualizations. Brian Clapper employs hands-on exercises using various Wikipedia datasets to illustrate the variety of ideal programming paradigms Spark makes possible.
The real power and value proposition of Apache Spark is in building a unified use case that combines ETL, batch analytics, real-time stream analysis, machine learning, graph processing, and visualizations. Brian Clapper employs hands-on exercises using various Wikipedia datasets to illustrate the variety of ideal programming paradigms Spark makes possible.
In today's world, executives need to be the drivers for data science solutions. Data analysis has moved from the domain of data scientists to the forefront of core strategic initiatives. Are you empowering your team to identify and execute on every opportunity to optimize business with machine learning? In this session, you will learn how executives are transforming business with machine learning.
Data lakes provide large-scale data processing and storage at low cost but struggle to deliver real-time analytics without investment in large clusters. If you need subsecond analytic response on streaming data, consider a GPU database. Amit Vij and Mark Brooks outline the dramatic performance benefits a GPU database offers and explain how to integrate it with Hadoop.
VoltDB promises full ACID with strong serializability in a fault-tolerant, distributed SQL platform, as well as higher throughput than other systems that promise much less. But why should users believe this? John Hugg discusses VoltDB's internal testing and support processes, its work with Kyle Kingsbury on the VoltDB Jepsen testing project, and where VoltDB will continue to improve.
A panel of practitioners from from Dell, National Instruments, and Citicompanies that are gaining real value from big data analyticsexplore their companies' big data journeys, explaining how analytics can answer groundbreaking new questions about business and create a path to becoming a data-driven organization.
Analytic discovery is a team sport; the lone hero data scientist is a thing of the past. John Akred of Silicon Valley Data Science leads a panel of analytics and data experts from Pfizer, the City of San Diego, and Neustar that explores how these businesses were changed through analytic collaboration.
The self-service YP Analytics application allows advertisers to understand their digital presence and ROI. Richard Langlois explains how Yellow Pages used this expertise for an internal use case that delivers real-time analytics with Tableau, using OLAP on Hadoop and enabled by its stack, which includes HDFS, Parquet, Hive, Impala, and AtScale, for fast, real-time analytics and data exploration.
Hear the Chief Data Platform Architect of Dell Technologies outline streaming principles.
Join data experts from Citi, Standard Charter Bank, and Polaris for a panel discussion moderated by Shankar Ganapathy. Learn about the principles, technologies, and processes they have used to design a highly efficient information management pipeline architected around the Hadoop ecosystem.
Creating production-ready analytical pipelines can be a messy, error-prone undertaking. Kyle Ambert explores the Trusted Analytics Platform, an open source-based platform that enables data scientists to ask bigger questions of their data and carry out principled data science experimentsall while engaging in iterative, collaborative development of production solutions with application developers.
Haoyuan Li offers an overview of Alluxio (formerly Tachyon), a memory-speed virtual distributed storage system. In the past year, the Alluxio project experienced a tremendous improvement in performance and scalability and was extended with key new features. This year, the goal is to make Alluxio accessible to an even wider set of users through a focus on security, new language bindings, and APIs.
Starting from first principles, Vinayak Borkar defines the requirements for a modern operational data store and explores some possible architectures to support those requirements.
Juliet Hougland and Sean Owen offer a practical overview of the basics of using Python data tools with a Hadoop cluster, covering HDFS connectivity and dealing with raw data files, running SQL queries with a SQL-on-Hadoop system like Apache Hive or Apache Impala (incubating), and using Apache Spark to write more complex analytical jobs.
Many Hadoop clusters lack even basic security controls. Michael Yoder, Ben Spivey, Mark Donsky, and Mubashir Kazia walk you through securing a Hadoop cluster. You'll start with a cluster with no security and then add security features related to authentication, authorization, encryption of data at rest, encryption of data in transit, and complete data governance.
Juliet Hougland and Sean Owen offer a practical overview of the basics of using Python data tools with a Hadoop cluster, covering HDFS connectivity and dealing with raw data files, running SQL queries with a SQL-on-Hadoop system like Apache Hive or Apache Impala (incubating), and using Apache Spark to write more complex analytical jobs.
More data exists than ever before and in more disparate silos. Getting the insights you need, sifting through data, and answering new questions have all been complex, hairy tasks that only data jocks have been able to do. Andrew Yeung and Scott Anderson explore new ways to challenge the status quo and speed insights on diverse sources and demonstrate real customer use cases.
When building your data stack, the architecture could be your biggest challenge. Yet it could also be the best predictor for success. With so many elements to consider and no proven playbook, where do you begin to assemble best practices for a scalable data architecture? Ben Sharma offers lessons learned from the field to get you started.
Building, running, and governing a data lake on Hadoop is often a difficult process filled with slow development cycles and painful operations. Jonathan Gray proposes a modern, unified integration architecture that helps IT mitigate these issues while enabling businesses to reduce time to insights and make decisions faster through a modern self-service environment.
Although Python and R promise powerful data science insights, they can also be complex to manage and deploy with Hadoop infrastructure. Peter Wang distills the vast array of Hadoop and data science tools and architectures down to the essentials that deliver a powerful and lightweight stack quickly so that you can accelerate time to value while meeting your data science, governance, and IT needs.
What's the point at which Hadoop tips from a Swiss-army knife of use cases to a new foundation that rearranges how the financial services marketplace turns data into profit and competitive advantage? This panel of expert practitioners looks into the near future to see if the inflection point is at hand.
The amount of cutting-edge technology that Azure puts at your fingertips is incredible. Artificial intelligence is no exception. Azure enables sophisticated capabilities in artificial intelligence, machine learning, deep learning, cognitive services, and advanced analytics. Rimma Nehme explains why Azure is the next AI supercomputer and how this vision is being implemented in reality.
Running Hadoop, Spark, and Presto can be as fast and inexpensive as ordering a latte at your favorite coffee shop. Jonathan Fritz explains how organizations are deploying these and other big data frameworks with Amazon Web Services (AWS) and how you too can quickly and securely run Spark and Presto on AWS. Jonathan shows you how to get started and shares best practices and common use cases.
BigQuery provides petabyte-scale data warehousing with consistently high performance for all users. However, users coming from traditional enterprise data warehousing platforms often have questions about how best to adapt their workloads for BigQuery. Chad Jennings explores best practices and integration with BigQuery with special emphasis on loading and transforming data for BigQuery.
Crystal Valentine draws on lessons learned from companies like Uber and Ericsson to outline the key principles to developing a microservices application. Along the way, Crystal describes how certain next-gen application areassuch as machine learningare particularly well suited to implementation in a microservices architecture rather than a legacy application paradigm.
Join Apache Kafka cocreator and PMC chair Jun Rao and Apache Kafka committer and architect of Kafka Connect Ewen Cheslack-Postava for a Q&A session about Apache Kafka. Bring your questions about Kafka internals or key considerations for developing your data pipeline and architecture, designing your applications, and running in production with Kafka.
Join Max Shron, former consultant on data science and current head of Warby Parker's data science team, for a Q&A all about data science consulting. Bring your questions about getting into the data science consulting business (or your questions about how to transition from consulting to something new). Even if you don't have questions, join in to hear what others are asking.
Finance is information. From analyzing risk and detecting fraud to predicting payments and improving customer experience, data technologies are transforming the financial industry. And we're diving deep into this change with a new day of data-meets-finance talks, tailored for Strata + Hadoop World events in the world's financial hubs.
Finance is information. From analyzing risk and detecting fraud to predicting payments and improving customer experience, data technologies are transforming the financial industry. And we're diving deep into this change with a new day of data-meets-finance talks, tailored for Strata + Hadoop World events in the world's financial hubs.
The IoT is fundamentally transforming industries and reconfiguring the technology landscape, but challenges exist for enterprises to effectively realize the value from this next wave of information and opportunity. Cheryl Wiebe explores how leading companies harness the IoT by putting IoT data in context, fostering collaboration between IT and OT and enabling a new breed of scalable analytics.
Reiner Kappenberger explores the new standards and innovations enabling architects and developers to take a build it in approach to security in early design phases for big data and IoT systems, explaining why emerging technologies such as format-preserving encryption are rapidly delivering more trusted big data and IoT ecosystems without altering application behavior or device functionality.
Jonathon Whitton details how PRGX is using Talend and Cloudera to load two million annual client flat files into a Hadoop cluster and perform recovery audit services in order to help clients detect, find, and fix leakage in their procurement and payment processes.
The trend of deploying Hadoop on virtual infrastructure is rapidly increasing. Martin Yip explores the benefits of virtualizing Hadoop through the lens of three real-world examples. You'll leave with the confidence to deploy your Hadoop clusters using virtualization.
Thomas Place explores the big data journey of the worlds biggest payment processor, which came dangerously close to building a data swamp before pivoting to embrace governance and quality-first patterns. This case study includes patterns, partners, successes, failures, and lessons learned to date and reviews the journey ahead.
Rajesh Shroff reviews the big data and analytics landscape, lessons learned in enterprise over the last few years, and some of the key considerations while designing a big data system.
Big data is a critical part of the enterprise data fabric and must meet the critical enterprise criteria of correctness, quality, consistency, compliance, and traceability. Michael Eacrett explains how companies are using big data infrastructures, asynchronously and in real time, to actively solve information governance and data-quality challenges.
Sharing your valuable data internally or with third-party consumers can be risky due to data privacy regulations and IP considerations, but sharing can also generate revenue or help nonprofits succeed at world-changing missions. Steve Touw explores real-world examples of how a proper data architecture enables philanthropic missions and offers ideas for how to better share your data.
Viral Shah explains how enterprises like Asurion Services are leveraging big data management solutions to accelerate enterprise data lake initiatives for business value.
Join DJ Patil and Lynn Overmann to ask your questions about data science at the White House.
The flux capacitor was the core component that made time travel possible in Back to the Future, processing garbage as a power source. Did you know that you can achieve the same affect in machine learning? Ingo Mierswa demonstrates how you can power through your analytics faster than ever before using the knowledge of 250K data scientists.
Its hard to get data into a data lake. Organizations hand-code their way through this, but with hundreds of data sources, it soon becomes unmanageable. Chuck Yarbrough offers a solution that uses metadata to autogenerate ingestion processes. Teams can drive hundreds of Hadoop onboarding processes through just a few templates, reducing development time and risk.
Customers are looking to extend the benefits beyond big data with the power of the deep learning and accelerated analytics ecosystems. Jim McHugh explains how customers are leveraging deep learning and accelerated analytics to turn insights into AI-driven knowledge and covers the growing ecosystem of solutions and technologies that are delivering on this promise.
Scott Gnau provides unique insights into the tipping point for data, how enterprises are now rethinking everything from their IT architecture and software strategies to data governance and security, and the cultural shifts CIOs must grapple with when supporting a business using real-time data to scale and grow.
Joe Goldberg explores how companies like GoPro, Produban, Navistar, and others have taken a platform approach to managing their workflows; how they are using workflows to power data ingest, ETL, and data integration processing; how an end-to-end view of workflows has reduced issue resolution time; and how these companies are achieving success in their data warehouse modernization projects.
Big data and analytics is a team sport empowering companies of all kinds to achieve business outcomes faster and with greater levels of success. Carey James explains how the formation of Dell Technologies and Dell EMC can help you on your data analytics journey and how you can turn actionable insights into new business opportunities.
Mastercard's Nick Curcuru hosts an interactive fireside chat with Anthony Dina from Dell to explore how the flexibility, scalability, and agility of Hadoop big data solutions allow one of the worlds leading organizations to innovate, enable, and enhance the customer experience while still expanding emerging opportunities.
Antonio Rosales offers an overview of Juju, an open source method to distill the best practices and operations needed to use interconnected big data solutions. By providing an open source means to describe services and solutions, users can focus on using the science, and developers can focus on delivering best practices.
Machine data is growing at an exponential rate, and a key driver for this growth is the Internet of Things (IoT) revolution. Johan Bjerke explains how to find value in and make use of the unstructured machine data that plays an important role in the new connected world.
Current data warehouse technologies are increasingly challenged to handle the growth in data volume, new data types, and multiple analytics types. Hadoop has the potential to address these issues, but you need to solve several complexities before you can realize its full benefits. Amar Arsikere showcases the business and technical aspects of augmenting and modernizing data warehouses on Hadoop.
Joe Caserta explores how a leading membership interest group is utilizing a data lake to track its members path-to-purchase touch points across multiple channels by matching and mastering individuals using Spark GraphFrames and stitching together website, marketing, email, and transaction data to discover the most effective way to attract new members and retain existing high-value members.
Mariusz Gdarowski offers an overview of Neptune, deepsense.ios new IT platform-based machine-learning experiment management solution for data scientists. Neptune enhances the management of machine-learning tasks such as dependent computational processes, code versioning, comparing achieved results, monitoring tasks and progress, sharing infrastructure among teammates, and many others.
Connor Carreras offers an in-depth review of the most popular use cases for data wrangling solutions among enterprise organizations, drawing on real customer deployments to explain how data wrangling has enabled them to accelerate analysis and uncover new sources of business value.
Jake Dolezal shares research into the performance of data quality and data management workloads on Hadoop clusters. Jake discusses a YARN-based approach to data management and outlines highly effective IT resource utilization techniques to achieve extreme agility for organizations and performance gains in Hadoop.
Ready to take a deeper look at how Hadoop and its ecosystem has a widespread impact on analytics? Douglas Liming explains where SAS fits into the open ecosystem, why you no longer have to choose between analytics languages like Python, R, or SAS, and how a single, unified open analytics architecture empowers you to literally have it all.
Jack Gudenkauf explores how organizations have successfully deployed tiered hyperscale architecture for real-time streaming with Spark, Kafka, Hadoop, and Vertica and discusses how advancements in hardware technologies such as nonvolatile memory, SSDs, and accelerators are changing the role of big data and big analytics platforms in an overall enterprise-data-platform strategy.
Launched in late 2015, Cigna's enterprise data lake project is taking the company on a data governance journey. Sherri Adame offers an overview of the project, providing insights into some of the business pain points and key drivers, how it has led to organizational change, and the best practices associated with Cignas new data governance process.
Guy Levy-Yurista explains the unexpected consequences of making big data processing significantly more agile than ever before and the impact it's having on human insight consumption.
OReilly Media and DataStax have partnered to create a 2-day developer certification course for Apache Cassandra. Get certified as a Cassandra developer at Strata + Hadoop World in New York and be recognized for your NoSQL expertise.
OReilly Media and DataStax have partnered to create a 2-day developer certification course for Apache Cassandra. Get certified as a Cassandra developer at Strata + Hadoop World in New York and be recognized for your NoSQL expertise.
OReilly Media and DataStax have partnered to create a 2-day developer certification course for Apache Cassandra. Get certified as a Cassandra developer at Strata + Hadoop World in New York and be recognized for your NoSQL expertise.
Strata + Hadoop World program chairs Roger Magoulas, Doug Cutting, and Alistair Croll welcome you to the first day of keynotes.
Strata + Hadoop World program chairs Roger Magoulas, Doug Cutting, and Alistair Croll welcome you to the second day of keynotes.
Strata + Hadoop World program chairs Roger Magoulas, Doug Cutting, and Alistair Croll welcome you to the first day of keynotes.
Strata + Hadoop World program chairs Roger Magoulas, Doug Cutting, and Alistair Croll welcome you to the second day of keynotes.
Since its inception, big data solutions have best been known for their ability to master the complexity of the volume, variety, and velocity of data. But as we enter the era of data democratization, theres a new set of concerns to consider. Mike Olson discusses the new dynamics of big data and how a renewed approach focused on where, who, and why can lead to cutting-edge solutions.
During election season, were tasked with considering the next four years and comparing platforms across candidates. Whats good for the country is good for your data. Consider what the next four years will look like for your organization. How will you lower costs and deliver innovation? Jack Norris reviews the requirements for a winning data platform, such as speed, scale, and agility.
Susan Woodward discusses venture outcomeswhat fraction make lots of money, which just barely return capital, and which fraction fail completely. Susan uses updated figures on the fraction of entrepreneurs who succeed, including some interesting details on female founders of venture companies.
How do we discover what we're not looking for? In the age of big data and bioinformatics, the answer is more relevant than ever. We develop new tools to help us spot clues in mountains of information, and yet, serendipity remains a very human art. Pagan Kennedy discusses the origins of the word serendipity and qualities of mind that lead to successful searches in the deep unknown.
The power of artificial intelligence and advanced analytics emerges from the ability to analyze and compute large, disparate datasets from varied devices and locations, such as predictive medicine and automated cars, at lightning-fast speed. Martin Hall explains why collaboration and openness are the key elements driving innovation in AI.
Theres been much discussion on open source versus commercial; CIOs and CTOs are increasingly interested in solutions that blend the benefits of both worlds. Ron Bodkin explains how Teradata drives open source adoption inside enterprises through a range of initiatives: direct contributions to open source projects, building orchestration software, and providing technical expertise.
Healthcare, a $3 trillion industry, is ripe for disruption through data science. However, there are many challenges in the journey to make healthcare a truly transparent, consumer-centric, data-driven industry. Sriram Vishwanath shares some myths and facts about data science's impact on healthcare.
Data, your most precious commodity, is increasing at an alarming rate. At the same time, an emerging business imperative has made this data a component of your deepest insights, allowing you to focus on your business outcomes. Patricia Florissi explains why the recent formation of Dell EMC ensures that your analytics capabilities will be stronger than ever.
Cloudera CEO Tom Reilly and James Powell, global CTO of Nielsen, discuss the dynamics of Hadoop in the cloud, what to consider at the start of the journey, and how to implement a solution that delivers flexibility while meeting key enterprise requirements.
When Hollywood portrays artificial intelligence, it's either a demon or a savior. But the reality is that AI is far more likely to be an extension of ourselves. Strata program chair Alistair Croll looks at the sometimes surprising ways that machine learning is insinuating itself into our everyday lives.
Will machine learning give us better eyesight? Join Joseph Sirosh for a surprising story about how machine learning, population data, and the cloud are coming together to fundamentally reimagine eye care in one of the worlds most populous countries, India.
The Panama Papers investigation revealed the offshore holdings and connections of dozens of politicians and prominent public figures around the world and led to high-profile resignations, police raids, and official investigations. Almost 500 journalists had to sift through 2.6 terabytes of datathe biggest leak in the history of journalism. Mar Cabra explains how technology made it all possible.
The need to quickly acquire, process, prepare, store, and analyze data has never been greater. The need for performance crosses the big data ecosystem toofrom the edge to the server to the analytics software, speed matters. Raghunath Nambiar shares a few use cases that have had significant organizational impact where performance was key.
Keynote by DJ Patil and Lynn Overmann
Data has long stopped being structured and flat, but the results of our analysis are still rendered as flat bar charts and scatter plots. We live in a 3D world, and we need to be able to enable data interaction from all perspectives. Robert Thomas offers an overview of Immersive Visualizationintegrated with notebooks and powered by Sparkwhich helps bring insights to life.
Hadoop and its ecosystem have changed analytics profoundly. Paul Kent offers an overview of SAS's participation in open platforms and introduces SAS Viya, a new unified and open analytics architecture that lets you scale analytics in the cloud and code as you choose.
Gary Marcus explores the gap between what machines do well and what people do well and what needs to happen before machines can match the flexibility and power of human cognition.
Although 2016 is a highly unusual political year, elections and public opinion follow predictable statistical properties. Sam Wang explains how the presidential, Senate, and House races can be tracked and forecast from freely available polling data using tools from statistics and machine learning.
American politics is adrift in a sea of polls. This year, that sea is deeper than ever beforeand darker. Data science is upending the public opinion industry. But to what end? In a brief, illustrated history of the field, Jill Lepore demonstrates how pollsters rose to prominence by claiming that measuring public opinion is good for democracy and asks, "But what if its bad?"
Chad W. Jennings demonstrates the power of BigQuery through an exciting demo and announces several new features that will make BigQuery a better home for your enterprise big data workloads.
This tutorial will be valuable for developers, architects, or project leads who are already knowledgeable about Hadoop and are now looking for more insight into how it can be leveraged to implement real-world applications.
We're always talking about "innovation", but - says Tim Harford - there are really two very different kinds of innovation. Using stories from sport, science, music and military history, Tim will make you think different about where good ideas come from and how they should be encouraged.
Ivory is a new open-source, Hadoop-based data store that focuses on changing the way we approach the critical and time-consuming activity of scalable feature engineering. It both simplifies and adds rigour to data science pipelines, aiding in their transition from the lab to production environments.
Building and deploying predictive applications require knowing how to evaluate, test, and track the performance of machine learning models over time. Using available off-the-shelf tools, this talk engages potential application builders on topics such as common evaluation metrics, A/B testing set up, tracking model performance, tracking usage via real-time feedback, and updating models.
How big is the human genome? What tools can we use to manage and understand it? Turns out the same tools used for traditional purposes (Hadoop, Spark, BigQuery, Dataflow, and SQL) can be applied to genomics. In this session we'll introduce the basics of managing genomes with our favorite big data tools, and draw parallels with more traditional use cases like analyzing view logs.
A/B testing is easy; it's just an application of hypothesis testing, taught in every first year stats course. My goal in this talk is to convince you that this view is wrong. There is a lot of subtlety in creating a meaningful test, and this subtlety is important in practice. I'll cover issues from methodology to epistemology, giving insights and tools directly applicable to practice.
Offering benefits is a classic and important strategy for acquisition of new customers and churn management. For measuring benefits with data, this model combines multivariate testing like A/B testing and Bayesian time series prediction modeling. The model is implemented in an R and CausalImpact package. This presentation will demonstrate the model structure and provide a case study.
We often face the need to analyze the count of discrete events which occur at a specific time and place, whether they are crime events, taxi requests, or phone calls.  Forecasting these space-time events brings particular challenges: finding suitable tools for geographic processing, and techniques for modeling the data.  The session will cover the lessons learned in building such a system.
Deep learning is a promising machine learning technique with a high barrier to entry. In this talk, we provide an easy entry into this field via "deep features" from pre-trained models. These features can be trained on one data set for one task and used to obtain good predictions on a different task, on a different data set. No prior experience is necessary.
Apache Spark has a lot to like for the data scientist: natively distributed, REPL, Scala and Python APIs, and a machine learning library, MLlib. Spark 1.2 includes an implementation of random decision forests, an important classifier/regressor algorithm. This talk will introduce Spark, Scala, and random decision forests, and demonstrate the process of analyzing a real-world data set with them.
While the data management side of Big Data has seen tremendous progress in the past few years, bringing technologies like Hadoop or Spark together with advanced machine learning and data analysis methods is still a major challenge. In this talk, I will discuss recent advances, approaches, and patterns which are used to build truly scalable machine learning solutions.
Live demo using Python open-source libraries to build a hybrid machine-learning model for fraud detection, combining features from natural language processing, topic modeling, time series analysis, link analysis, heuristic rules, and anomaly detection. Well then show how we scaled to billions of events using Spark, and what it took to make the system perform and ready for production.
Context relevant has defined the next-generation of financial information capabilities by applying rapid automated predictive analytics software to solve Wall Streets toughest problems. The big data 2.0 era of automated, intelligent, and scalable systems allows Wall Street banks to finally take advantage of the massive value of the data they hold and better serve and protect their customers.
Hadoop, Storm, and Spark are fantastic frameworks for processing massive amounts of data in parallel. Every now and then, there is a one-off data science task that could really use some speeding up. For those kinds of tasks, it's probably not worthwhile to set up large frameworks. This presentation demonstrates GNU Parallel, which allows you to easily parallelize and distribute such tasks.
Apache Spark is a powerful, unified data processing engine offering a number of APIs, from batch/SQL over streaming to manipulations over graphs. The core architecture of Spark has not necessarily been designed with a multi-user environment in mind. We will review existing and emerging approaches how to use Spark in multi-user environments, such as the Tachyon project.
Mobile gaming is a fast-moving field and needs metrics like daily active users or revenue in real-time to be able to fine-tune quickly. Approximation is needed to count those metrics, as the data volume would be too large to process exactly in real-time. We will demonstrate how to use Spark Streaming and probabilistic data structures to achieve a low error rate, even for many millions of users.
This session demonstrates using open source tools and techniques for visually exploring massive node-link graphs in a web browser by visualizing all the data. Seeing all the data reveals informative patterns and provides important context to understanding insights. Examples will highlight large scale graph analysis of social networks, customer purchase history, and health care industry data.
In the second (afternoon) half of the Architecture Day tutorial, attendees will apply the best practices they learned in the morning session to build a data application for sessionizing user data.
This tutorial will be valuable for developers, architects, or project leads who are already knowledgeable about Hadoop and are now looking for more insight into how it can be leveraged to implement real-world applications.
We're always talking about "innovation", but - says Tim Harford - there are really two very different kinds of innovation. Using stories from sport, science, music and military history, Tim will make you think different about where good ideas come from and how they should be encouraged.
Ivory is a new open-source, Hadoop-based data store that focuses on changing the way we approach the critical and time-consuming activity of scalable feature engineering. It both simplifies and adds rigour to data science pipelines, aiding in their transition from the lab to production environments.
Building and deploying predictive applications require knowing how to evaluate, test, and track the performance of machine learning models over time. Using available off-the-shelf tools, this talk engages potential application builders on topics such as common evaluation metrics, A/B testing set up, tracking model performance, tracking usage via real-time feedback, and updating models.
How big is the human genome? What tools can we use to manage and understand it? Turns out the same tools used for traditional purposes (Hadoop, Spark, BigQuery, Dataflow, and SQL) can be applied to genomics. In this session we'll introduce the basics of managing genomes with our favorite big data tools, and draw parallels with more traditional use cases like analyzing view logs.
A/B testing is easy; it's just an application of hypothesis testing, taught in every first year stats course. My goal in this talk is to convince you that this view is wrong. There is a lot of subtlety in creating a meaningful test, and this subtlety is important in practice. I'll cover issues from methodology to epistemology, giving insights and tools directly applicable to practice.
Offering benefits is a classic and important strategy for acquisition of new customers and churn management. For measuring benefits with data, this model combines multivariate testing like A/B testing and Bayesian time series prediction modeling. The model is implemented in an R and CausalImpact package. This presentation will demonstrate the model structure and provide a case study.
We often face the need to analyze the count of discrete events which occur at a specific time and place, whether they are crime events, taxi requests, or phone calls.  Forecasting these space-time events brings particular challenges: finding suitable tools for geographic processing, and techniques for modeling the data.  The session will cover the lessons learned in building such a system.
Deep learning is a promising machine learning technique with a high barrier to entry. In this talk, we provide an easy entry into this field via "deep features" from pre-trained models. These features can be trained on one data set for one task and used to obtain good predictions on a different task, on a different data set. No prior experience is necessary.
Apache Spark has a lot to like for the data scientist: natively distributed, REPL, Scala and Python APIs, and a machine learning library, MLlib. Spark 1.2 includes an implementation of random decision forests, an important classifier/regressor algorithm. This talk will introduce Spark, Scala, and random decision forests, and demonstrate the process of analyzing a real-world data set with them.
While the data management side of Big Data has seen tremendous progress in the past few years, bringing technologies like Hadoop or Spark together with advanced machine learning and data analysis methods is still a major challenge. In this talk, I will discuss recent advances, approaches, and patterns which are used to build truly scalable machine learning solutions.
Live demo using Python open-source libraries to build a hybrid machine-learning model for fraud detection, combining features from natural language processing, topic modeling, time series analysis, link analysis, heuristic rules, and anomaly detection. Well then show how we scaled to billions of events using Spark, and what it took to make the system perform and ready for production.
Context relevant has defined the next-generation of financial information capabilities by applying rapid automated predictive analytics software to solve Wall Streets toughest problems. The big data 2.0 era of automated, intelligent, and scalable systems allows Wall Street banks to finally take advantage of the massive value of the data they hold and better serve and protect their customers.
Hadoop, Storm, and Spark are fantastic frameworks for processing massive amounts of data in parallel. Every now and then, there is a one-off data science task that could really use some speeding up. For those kinds of tasks, it's probably not worthwhile to set up large frameworks. This presentation demonstrates GNU Parallel, which allows you to easily parallelize and distribute such tasks.
Apache Spark is a powerful, unified data processing engine offering a number of APIs, from batch/SQL over streaming to manipulations over graphs. The core architecture of Spark has not necessarily been designed with a multi-user environment in mind. We will review existing and emerging approaches how to use Spark in multi-user environments, such as the Tachyon project.
Mobile gaming is a fast-moving field and needs metrics like daily active users or revenue in real-time to be able to fine-tune quickly. Approximation is needed to count those metrics, as the data volume would be too large to process exactly in real-time. We will demonstrate how to use Spark Streaming and probabilistic data structures to achieve a low error rate, even for many millions of users.
This session demonstrates using open source tools and techniques for visually exploring massive node-link graphs in a web browser by visualizing all the data. Seeing all the data reveals informative patterns and provides important context to understanding insights. Examples will highlight large scale graph analysis of social networks, customer purchase history, and health care industry data.
In the second (afternoon) half of the Architecture Day tutorial, attendees will apply the best practices they learned in the morning session to build a data application for sessionizing user data.
Spark Camp, organized by the creators of the Apache Spark project at Databricks, will be a day-long, hands-on introduction to the Spark platform, including Spark Core, the Spark Shell, Spark Streaming, Spark SQL, MLlib, and more.
In this talk, attendees will learn about Impalas approach to on-the-fly, automatic data transformation, which in conjunction with the ability to handle nested structures such as JSON and XML documents, addresses the needs of at-source analytics  including direct querying of your input schema, immediate querying of data as it lands in HDFS, and high performance on par with specialized engines.
Cloudera Impala can be considered as an alternative solution to a relational database for data warehouse-like workloads. The CERN database community did a close evaluation of the Impala engine in respect to CERN's needs. In this presentation we will discuss our experience with the technology, and will report on a queries performance in comparison to data access using an Oracle RDBMS.
Apache Kylin is an open source distributed analytics engine contributed by eBay Inc. that provides SQL interface and multi-dimensional analysis (OLAP) on Hadoop, supporting extremely large datasets. It was accepted as an Apache Incubator Project on Nov 25, 2014. Website: http://kylin.io
SQL-on-Hadoop systems that support business intelligence (BI) use cases must handle hundreds or even thousands of concurrent users. We will talk about how to scale your SQL-on-Hadoop system to a large number of concurrent users, and how to verify that your system can support BI.
With hundreds of developers from a variety of organizations participating, Hadoop moves quickly. This talk will survey the important changes admins and users should be aware of and their impacts on various use cases.
Compare the Markets senior project manager Neil Martin will present the lessons learned whilst delivering a successful yet complex multifaceted project to reinvigorate the organizations data infrastructure.
Goldman Sachs is a leading global investment banking, securities, and investment management firm that provides a wide range of financial services. Goldman executes hundreds of millions of financial transactions per day across nearly every market in the world. Learn how Goldman is harnessing knowledge, data, and compute power to maintain and increase its competitive edge.
The Hadoop ecosystem makes it possible to build an enterprise data hub capable of storing and analysing a wide variety of data. However, a platform with such broad capability triggers a question: how to organise the myriad data sets in a way that allows users to explore and access the data they need? This session will propose an information architecture for Hadoop that enables this.
As the volume of data and number of applications moving to Apache Hadoop has increased, so has the need to secure that data and those applications. In this presentation, we'll take a brief look at where Hadoop security is today and then peer into the future.
Encryption is a requirement for many business sectors dealing with confidential information. To meet these requirements, transparent, end-to-end encryption was added to HDFS. This protects data while it is in-flight and at-rest, and can be used compatibly with existing Hadoop apps. We will cover the design and implementation of transparent encryption in HDFS, as well as performance results.
Starting in Hive 0.14, insert values, update, and delete have been added to Hive SQL. In addition, ACID compliant transactions have been added so users get a consistent view of data while reading and writing. This talk will cover the intended use cases, architecture, and performance of insert, update, and delete in Hive.
What if Big Data technologies would be like Lego blocks that can be clicked together to create complete solutions. You could add continuous deployment, e.g. Pig or Storm topologies. Integrate SSO. You can add sentiment analysis or real-time dashboards. You can integrate any data source. All Open Source. We are ready to demo this so Big Data solutions in minutes is reality not marketing.
Spark Camp, organized by the creators of the Apache Spark project at Databricks, will be a day-long, hands-on introduction to the Spark platform, including Spark Core, the Spark Shell, Spark Streaming, Spark SQL, MLlib, and more.
In this talk, attendees will learn about Impalas approach to on-the-fly, automatic data transformation, which in conjunction with the ability to handle nested structures such as JSON and XML documents, addresses the needs of at-source analytics  including direct querying of your input schema, immediate querying of data as it lands in HDFS, and high performance on par with specialized engines.
Cloudera Impala can be considered as an alternative solution to a relational database for data warehouse-like workloads. The CERN database community did a close evaluation of the Impala engine in respect to CERN's needs. In this presentation we will discuss our experience with the technology, and will report on a queries performance in comparison to data access using an Oracle RDBMS.
Apache Kylin is an open source distributed analytics engine contributed by eBay Inc. that provides SQL interface and multi-dimensional analysis (OLAP) on Hadoop, supporting extremely large datasets. It was accepted as an Apache Incubator Project on Nov 25, 2014. Website: http://kylin.io
SQL-on-Hadoop systems that support business intelligence (BI) use cases must handle hundreds or even thousands of concurrent users. We will talk about how to scale your SQL-on-Hadoop system to a large number of concurrent users, and how to verify that your system can support BI.
With hundreds of developers from a variety of organizations participating, Hadoop moves quickly. This talk will survey the important changes admins and users should be aware of and their impacts on various use cases.
Compare the Markets senior project manager Neil Martin will present the lessons learned whilst delivering a successful yet complex multifaceted project to reinvigorate the organizations data infrastructure.
Goldman Sachs is a leading global investment banking, securities, and investment management firm that provides a wide range of financial services. Goldman executes hundreds of millions of financial transactions per day across nearly every market in the world. Learn how Goldman is harnessing knowledge, data, and compute power to maintain and increase its competitive edge.
The Hadoop ecosystem makes it possible to build an enterprise data hub capable of storing and analysing a wide variety of data. However, a platform with such broad capability triggers a question: how to organise the myriad data sets in a way that allows users to explore and access the data they need? This session will propose an information architecture for Hadoop that enables this.
As the volume of data and number of applications moving to Apache Hadoop has increased, so has the need to secure that data and those applications. In this presentation, we'll take a brief look at where Hadoop security is today and then peer into the future.
Encryption is a requirement for many business sectors dealing with confidential information. To meet these requirements, transparent, end-to-end encryption was added to HDFS. This protects data while it is in-flight and at-rest, and can be used compatibly with existing Hadoop apps. We will cover the design and implementation of transparent encryption in HDFS, as well as performance results.
Starting in Hive 0.14, insert values, update, and delete have been added to Hive SQL. In addition, ACID compliant transactions have been added so users get a consistent view of data while reading and writing. This talk will cover the intended use cases, architecture, and performance of insert, update, and delete in Hive.
What if Big Data technologies would be like Lego blocks that can be clicked together to create complete solutions. You could add continuous deployment, e.g. Pig or Storm topologies. Integrate SSO. You can add sentiment analysis or real-time dashboards. You can integrate any data source. All Open Source. We are ready to demo this so Big Data solutions in minutes is reality not marketing.
This presentation will talk about the motivation, design principles, architecture, challenges, and current status of the community project to make Spark a new back-end processing engine for Hive.
Apache Spark is a popular engine for fast and efficient data processing. This talk will cover recent feature additions to Spark, such as the elastic scaling support, new algorithms in MLlib, and the Spark SQL datasources API. It will also outline the Spark roadmap for upcoming months. Since this talk is not until May, specific roadmap details will be determined close to the talk itself.
Data is only useful if you can process it, analyse it, and create valuable products from it. If you have an idea for a new data-driven product, how long does it take you to get it into production? In this talk, we'll discuss Apache Kafka and Samza, open source tools created at LinkedIn with the goal of helping teams implement data products and ship them to production rapidly.
The Zeta Architecture is the enterprise architecture that describes how to move your business to the next generation. It combines a data center-wide resource manager, a rock-solid distributed file system, containerization, a big data processing platform, stream processing, a real-time data store, independent application architectures, and custom enterprise applications.
This presentation will provide a brief technical overview of Google Bigtable and the global problems we solve internally at Google with this revolutionary architecture. We'll discuss some of our innovations since the original paper was released, what weve been working on with HBase, and include announcements on where we're headed next!
Technical overview of how Apache Drill enables high performance analysis of complex and dynamic data. Will discuss the role of self-describing data in a modern distributed data lake, the requirement for adaptive execution, and how to expose dynamic schema using SQL.
Search is more than typing words into a box. It's evolved into the backbone for todays analytics demands and is an asset for businesses to ask the right questions in order to make sense of their data. Versatile, agile search and analytics can uncover the uncommonly common trends within, giving businesses real-time insights and setting them up to make the right data-driven decisions.
We share lessons learned the hard way while building a real-time search, analytics, and trends pipeline over social media posts, using Elasticsearch, Azure, and Spark Streaming. Topics cover building an end-to-end pipeline including stream processing, applying natural language processing tools, scaling and performance tuning, search relevance, and applications like TV trends.
Nowadays, all kinds of businesses need to deal with real-time information in order to successfully deliver their core services. SPARKTA was born to meet this demand. Thanks to this technology, real-time analysis is readily available for every use case with absolutely no coding. SPARKTA is easy to deploy, and also open source, fast, scalable, and fault-tolerant.
Spark is often seen as a replacement for MapReduce in Hadoop systems, but Spark clusters can also be deployed and managed by Mesos. This talk explains how to use Mesos for Spark applications. Using example applications, we'll examine the pros and cons of using Mesos vs. Hadoop YARN as a data platform and discuss practical issues when running Spark on Mesos.
Learn what it takes to ditch your Big Data batch pipelines and go all-streaming-all-the-time, without compromising latency, correctness, or the flexibility to deal with changes in upstream data.
Apache Flink is a data analysis engine designed to match Hadoop in reliability and Spark in performance. Flink introduces novel features such as cost-based optimization for Java and Scala programs, native iterative processing, unification of streaming and batch processing, and efficient hybrid in-memory/on-disk processing. Flink has more than 70 contributors from industry and academia.
This presentation will talk about the motivation, design principles, architecture, challenges, and current status of the community project to make Spark a new back-end processing engine for Hive.
Apache Spark is a popular engine for fast and efficient data processing. This talk will cover recent feature additions to Spark, such as the elastic scaling support, new algorithms in MLlib, and the Spark SQL datasources API. It will also outline the Spark roadmap for upcoming months. Since this talk is not until May, specific roadmap details will be determined close to the talk itself.
Data is only useful if you can process it, analyse it, and create valuable products from it. If you have an idea for a new data-driven product, how long does it take you to get it into production? In this talk, we'll discuss Apache Kafka and Samza, open source tools created at LinkedIn with the goal of helping teams implement data products and ship them to production rapidly.
The Zeta Architecture is the enterprise architecture that describes how to move your business to the next generation. It combines a data center-wide resource manager, a rock-solid distributed file system, containerization, a big data processing platform, stream processing, a real-time data store, independent application architectures, and custom enterprise applications.
This presentation will provide a brief technical overview of Google Bigtable and the global problems we solve internally at Google with this revolutionary architecture. We'll discuss some of our innovations since the original paper was released, what weve been working on with HBase, and include announcements on where we're headed next!
Technical overview of how Apache Drill enables high performance analysis of complex and dynamic data. Will discuss the role of self-describing data in a modern distributed data lake, the requirement for adaptive execution, and how to expose dynamic schema using SQL.
Search is more than typing words into a box. It's evolved into the backbone for todays analytics demands and is an asset for businesses to ask the right questions in order to make sense of their data. Versatile, agile search and analytics can uncover the uncommonly common trends within, giving businesses real-time insights and setting them up to make the right data-driven decisions.
We share lessons learned the hard way while building a real-time search, analytics, and trends pipeline over social media posts, using Elasticsearch, Azure, and Spark Streaming. Topics cover building an end-to-end pipeline including stream processing, applying natural language processing tools, scaling and performance tuning, search relevance, and applications like TV trends.
Nowadays, all kinds of businesses need to deal with real-time information in order to successfully deliver their core services. SPARKTA was born to meet this demand. Thanks to this technology, real-time analysis is readily available for every use case with absolutely no coding. SPARKTA is easy to deploy, and also open source, fast, scalable, and fault-tolerant.
Spark is often seen as a replacement for MapReduce in Hadoop systems, but Spark clusters can also be deployed and managed by Mesos. This talk explains how to use Mesos for Spark applications. Using example applications, we'll examine the pros and cons of using Mesos vs. Hadoop YARN as a data platform and discuss practical issues when running Spark on Mesos.
Learn what it takes to ditch your Big Data batch pipelines and go all-streaming-all-the-time, without compromising latency, correctness, or the flexibility to deal with changes in upstream data.
Apache Flink is a data analysis engine designed to match Hadoop in reliability and Spark in performance. Flink introduces novel features such as cost-based optimization for Java and Scala programs, native iterative processing, unification of streaming and batch processing, and efficient hybrid in-memory/on-disk processing. Flink has more than 70 contributors from industry and academia.
Interested in time series use cases? Need a database that can scale with your application? Apache Cassandra has proven to be one of the best solutions for storing and retrieving time series data at high velocity and high volume. This tutorial will provide an in-depth introduction to Cassandra data modeling internals and finish with an example application.
D3.js has a very steep learning curve for learning how to create interactive visualizations. However, there are three main concepts that, once you get your head around them, will make the climb much easier. Focusing on these three main concepts, we will walk through many examples to teach the fundamental building blocks of creating D3.js based interactive visualizations.
How to make data and analytics valuable to a business. How to improve a business with data and analytics. I'm a business person first, and an analyst second. I have seen many excellent data scientists fail to implement their ideas. I have also seen many excellent business people fail to generate value from data.
Creating value from data needs a new mindset. To fully exploit new big data tools and architectures, we need a new way of thinking: data as the raw material of growth. How do you share this understanding in your company, and how do you plan for success?
Most health systems have been slow to embrace new techniques and technologies, despite the dire need to get more value from their data, more quickly. In this talk, I will discuss the state of health informatics; bringing new tools and processes to healthcare; navigating politics; and collaborating with industry and startups.
At Barclays, we have succeeded in building initial info-led propositions based on the ability to analyze terabytes of data in seconds. We overcame the challenges of bringing together structured and unstructured data by employing innovative solutions.
While many companies are struggling to adopt big data and unlock its potential, facing challenges of visualization and democratization of insight, a number of industry leaders are leapfrogging big data adoption to circumvent the analyst bottleneck by going straight to automation of core business processes. This requires overcoming a set of tough cultural, technical, and scientific challenges.
This session will look at the journey Marks & Spencer have been on and have ahead in using data to help turn it from a traditional British high street retailer into a global, multi-channel retailer. It will explore the impact on culture, technology, ways of working, and capabilities needed to drive the change.
In this talk, John, Nate, and Hallie from Accenture's Technology Labs will explain their perspective on existing approaches to data strategy, and how a devoted data innovation lab can pull together open source technology and open data, create a visualization, and mock up a prototype, to help organizations pave the way for exploration of new data frontiers.
Curiosity is one of the most valued skills for people working in Data Science. But how can we train it? Einstein said that "Curiosity is an important trait of a genius". Lets explore how we can develop our curiosity with three exercises in the session: how to find pleasure in uncertainty; question the question were asking; and find a beginner's mind. With direct application to data science.
As the necessity of having a data strategy is sinking in, the chief data officer (CDO) has emerged as a new member of the executive team focused on creating and implementing that strategy. This talk describes what that looks like across a variety of industries and organizations, and shares some best practices for getting the most out of your business data.
I will talk about how we are using data science to help transform OpenTable into a local dining expert who knows you very well, and can help you and others find the best dining experience wherever you travel. This entails a whole slew of tools from natural language processing, recommendation system engineering, sentiment analysis to predictions based on internal and external signals!
IDEO's Hybrid team brings all the design tools from IDEO's product design process to work with clients on data oriented projects. The team will share elements of their process and case studies to show how incorporating human-centered techniques from design can improve data as an input to decision making.
How much does prostitution contribute to the UK economy? According to the UKs Office of National Statistics the answer is 5bn, or 0.4% of GDP.  But how did they calculate that number? With 10-year-old survey data and lots of assumptions, that's how. In this talk Andrew Fogg shows how he was able to add 3bn to the UK economy using some statistical sleuthing and modern web data techniques.
Interested in time series use cases? Need a database that can scale with your application? Apache Cassandra has proven to be one of the best solutions for storing and retrieving time series data at high velocity and high volume. This tutorial will provide an in-depth introduction to Cassandra data modeling internals and finish with an example application.
D3.js has a very steep learning curve for learning how to create interactive visualizations. However, there are three main concepts that, once you get your head around them, will make the climb much easier. Focusing on these three main concepts, we will walk through many examples to teach the fundamental building blocks of creating D3.js based interactive visualizations.
How to make data and analytics valuable to a business. How to improve a business with data and analytics. I'm a business person first, and an analyst second. I have seen many excellent data scientists fail to implement their ideas. I have also seen many excellent business people fail to generate value from data.
Creating value from data needs a new mindset. To fully exploit new big data tools and architectures, we need a new way of thinking: data as the raw material of growth. How do you share this understanding in your company, and how do you plan for success?
Most health systems have been slow to embrace new techniques and technologies, despite the dire need to get more value from their data, more quickly. In this talk, I will discuss the state of health informatics; bringing new tools and processes to healthcare; navigating politics; and collaborating with industry and startups.
At Barclays, we have succeeded in building initial info-led propositions based on the ability to analyze terabytes of data in seconds. We overcame the challenges of bringing together structured and unstructured data by employing innovative solutions.
While many companies are struggling to adopt big data and unlock its potential, facing challenges of visualization and democratization of insight, a number of industry leaders are leapfrogging big data adoption to circumvent the analyst bottleneck by going straight to automation of core business processes. This requires overcoming a set of tough cultural, technical, and scientific challenges.
This session will look at the journey Marks & Spencer have been on and have ahead in using data to help turn it from a traditional British high street retailer into a global, multi-channel retailer. It will explore the impact on culture, technology, ways of working, and capabilities needed to drive the change.
In this talk, John, Nate, and Hallie from Accenture's Technology Labs will explain their perspective on existing approaches to data strategy, and how a devoted data innovation lab can pull together open source technology and open data, create a visualization, and mock up a prototype, to help organizations pave the way for exploration of new data frontiers.
Curiosity is one of the most valued skills for people working in Data Science. But how can we train it? Einstein said that "Curiosity is an important trait of a genius". Lets explore how we can develop our curiosity with three exercises in the session: how to find pleasure in uncertainty; question the question were asking; and find a beginner's mind. With direct application to data science.
As the necessity of having a data strategy is sinking in, the chief data officer (CDO) has emerged as a new member of the executive team focused on creating and implementing that strategy. This talk describes what that looks like across a variety of industries and organizations, and shares some best practices for getting the most out of your business data.
I will talk about how we are using data science to help transform OpenTable into a local dining expert who knows you very well, and can help you and others find the best dining experience wherever you travel. This entails a whole slew of tools from natural language processing, recommendation system engineering, sentiment analysis to predictions based on internal and external signals!
IDEO's Hybrid team brings all the design tools from IDEO's product design process to work with clients on data oriented projects. The team will share elements of their process and case studies to show how incorporating human-centered techniques from design can improve data as an input to decision making.
How much does prostitution contribute to the UK economy? According to the UKs Office of National Statistics the answer is 5bn, or 0.4% of GDP.  But how did they calculate that number? With 10-year-old survey data and lots of assumptions, that's how. In this talk Andrew Fogg shows how he was able to add 3bn to the UK economy using some statistical sleuthing and modern web data techniques.
Three-hour hands-on introductory workshop on predictive modeling and machine learning with open source tools from the Python community such as scikit-learn and IPython.
Learn how to combine the best ideas of reproducible research into a simple, easy-to-use workflow with R. The Packrat, R Markdown, and Shiny packages let you (a) embed your code into reports to create a reproducible record of your work, (b) rerun the code to generate a new report as data and ideas change, and (c) export your reports into multiple formats, including pdfs and interactive web apps.
Trains today are complex systems of many embedded subsystems. Market demands are changing and data analytics is now absolutely required to deliver market leading offerings to customers. What value is hidden in train data? We will discuss different approaches and explore opportunities to reduce the cost of rolling stock fleet maintenance, minimize train downtime and improve fleet availability.
We present a concrete case study of a situation where it was necessary to have different data models (documents and graphs) in the same database engine using a common query language. A single aircraft already contains some 6,000,000 parts, not counting components. Any single data model inevitably leads to inefficient queries, though queries are nevertheless crucial for the application.
How do you negotiate with your house? This is a tale of home automation, machine learning, and a new way of programming. Find out how I used machine learning, micro-services and image recognition to light my house, mainly in the dark.
Just when we thought the last mile problem was solved, the Internet of Things is turning the last mile problem of the consumer Internet into the first mile problem of the industrial Internet. This inversion impacts every aspect of the design of networked applications. I will show how to use existing Hadoop ecosystem tools, such as Spark, Drill and others, to deal successfully with this inversion.
Intelligent prediction of driving behavior has a wide range of applications. For this session, we will explore a connected car. We will cover the architecture used for streaming real-time sensor data from a car, and the machine learning techniques utilized to predict route and range. Coming out of this session, youll understand how open source technologies can serve as a platform for the IoT.
Algorithms define the meaning we get from data. Arbitrary decisions are regularly built into our analytics by chosen method, setting parameters, or dealing with missing values. These value judgments are not present in the privacy discussion or business point of view. However, they may be much more important than the more obvious data collection or secure storage.
In Barcelona we saw that being good is hard. Being evil is fun and gets you paid more. Over the last 18 months examples have been embarrassingly easy to find. We review the field of doing high-impact evil with data and analysis. Make the maximum (negative) impact on your friends, your business, and the world in this updated version of the best talk from 2013.
Better data collaboration is vital for every organization. For the UN's Humanitarian division it is particularly hard--they work in hundreds of countries, in emergencies and natural disasters. This talk describes the Humanitarian Data Exchange, answering such questions as: what motivates busy, front-line staff to share data? How do you measure the success of a data collaboration platform?
'Connected' refers more and more to a human-machine relationship that requires understanding and trust; personalisation and respect for what is personal. More about this changing relationship, six basic concepts that apply to user experience design as well as privacy, and tips for delivering both understanding and trust.
Data is hard. Old thinking and old tools play their part, but the worst offenders are bad data citizens. This talk calls out the bad behavior and old thinking. Then it covers new thinking, a better way of approaching tools, and, most importantly, how to make data easy by being a good data citizen.
The election results page for the 2014 Indian general elections was hosted on CNN-IBN and bing.com. The focus was on real-time analysis of results for users and TV anchors. With over 540 million voters and 100 million viewers, the volume and complexity of data both provide a design challenge. This talk focuses on the techniques behind this design. http://blog.gramener.com/1755
Creating programmable cities, cars, and homes using sensors and data.  Explore a number of IoT use cases and aims to give some practical implementation experience reports of building connected products in the context of cities, cars, and homes.  We will also show what happens when you make it super-easy to plug sensor data into multiple systems, creating a new type of programmable world.
In Japan, Kaiten (conveyer belt) sushi is a fast business. With 360 stores, SushiRo is one of the largest Kaiten operators in the world. Faced with 24-hour BI cycles and lost data due to daily batch reporting, SushiRo turned to AWS for streaming analytics. Learn how Amazon Kinesis and Redshift help SushiRo capture real-time data from Sushi plates and convert it into business insights.
Three-hour hands-on introductory workshop on predictive modeling and machine learning with open source tools from the Python community such as scikit-learn and IPython.
Learn how to combine the best ideas of reproducible research into a simple, easy-to-use workflow with R. The Packrat, R Markdown, and Shiny packages let you (a) embed your code into reports to create a reproducible record of your work, (b) rerun the code to generate a new report as data and ideas change, and (c) export your reports into multiple formats, including pdfs and interactive web apps.
Trains today are complex systems of many embedded subsystems. Market demands are changing and data analytics is now absolutely required to deliver market leading offerings to customers. What value is hidden in train data? We will discuss different approaches and explore opportunities to reduce the cost of rolling stock fleet maintenance, minimize train downtime and improve fleet availability.
We present a concrete case study of a situation where it was necessary to have different data models (documents and graphs) in the same database engine using a common query language. A single aircraft already contains some 6,000,000 parts, not counting components. Any single data model inevitably leads to inefficient queries, though queries are nevertheless crucial for the application.
How do you negotiate with your house? This is a tale of home automation, machine learning, and a new way of programming. Find out how I used machine learning, micro-services and image recognition to light my house, mainly in the dark.
Just when we thought the last mile problem was solved, the Internet of Things is turning the last mile problem of the consumer Internet into the first mile problem of the industrial Internet. This inversion impacts every aspect of the design of networked applications. I will show how to use existing Hadoop ecosystem tools, such as Spark, Drill and others, to deal successfully with this inversion.
Intelligent prediction of driving behavior has a wide range of applications. For this session, we will explore a connected car. We will cover the architecture used for streaming real-time sensor data from a car, and the machine learning techniques utilized to predict route and range. Coming out of this session, youll understand how open source technologies can serve as a platform for the IoT.
Algorithms define the meaning we get from data. Arbitrary decisions are regularly built into our analytics by chosen method, setting parameters, or dealing with missing values. These value judgments are not present in the privacy discussion or business point of view. However, they may be much more important than the more obvious data collection or secure storage.
In Barcelona we saw that being good is hard. Being evil is fun and gets you paid more. Over the last 18 months examples have been embarrassingly easy to find. We review the field of doing high-impact evil with data and analysis. Make the maximum (negative) impact on your friends, your business, and the world in this updated version of the best talk from 2013.
Better data collaboration is vital for every organization. For the UN's Humanitarian division it is particularly hard--they work in hundreds of countries, in emergencies and natural disasters. This talk describes the Humanitarian Data Exchange, answering such questions as: what motivates busy, front-line staff to share data? How do you measure the success of a data collaboration platform?
'Connected' refers more and more to a human-machine relationship that requires understanding and trust; personalisation and respect for what is personal. More about this changing relationship, six basic concepts that apply to user experience design as well as privacy, and tips for delivering both understanding and trust.
Data is hard. Old thinking and old tools play their part, but the worst offenders are bad data citizens. This talk calls out the bad behavior and old thinking. Then it covers new thinking, a better way of approaching tools, and, most importantly, how to make data easy by being a good data citizen.
The election results page for the 2014 Indian general elections was hosted on CNN-IBN and bing.com. The focus was on real-time analysis of results for users and TV anchors. With over 540 million voters and 100 million viewers, the volume and complexity of data both provide a design challenge. This talk focuses on the techniques behind this design. http://blog.gramener.com/1755
Creating programmable cities, cars, and homes using sensors and data.  Explore a number of IoT use cases and aims to give some practical implementation experience reports of building connected products in the context of cities, cars, and homes.  We will also show what happens when you make it super-easy to plug sensor data into multiple systems, creating a new type of programmable world.
In Japan, Kaiten (conveyer belt) sushi is a fast business. With 360 stores, SushiRo is one of the largest Kaiten operators in the world. Faced with 24-hour BI cycles and lost data due to daily batch reporting, SushiRo turned to AWS for streaming analytics. Learn how Amazon Kinesis and Redshift help SushiRo capture real-time data from Sushi plates and convert it into business insights.
This session will look at Ciscos Data Warehouse Optimisation (DWO) Solution, and explores how it reduces ever-growing data warehouse management costs, whilst delivering agreater variety and volume of data that can can be ingested and stored to derive new business insights.
This presentation shares how SAS can help spread the use of Hadoop to less technical audiences, showcasing some of the end-user technologies already implemented at SAS customers that can help across the spectrum of data ingestion and management, visualization, and analytics.
This session will describe a method of data replication between Hadoop in multiple locations that achieves 100% utilization of globally-distributed infrastructure, while maintaining data consistency in the face of network, process, and machine failures. The session will also describe how computation is executed across sites, and provide example architectures using MapReduce and Hive.
Most organizations nowadays see the massive value potential in (big) data analytics. What most of them still fear is that starting an analytics initiative will result in a massive IT project that will take 12-18 months before first analytical results are achieved  and deploying the results to generate business value will take another 12-18 months. . .
As Hadoop adoption continues to grow, the number of workflows and their complexity increases. This session describes how organizations are managing Hadoop and big data workflows with an enterprise workflow solution that provides a graphical user interface for managing all of the complex components of the enterprise application fabric.
Big data and analytics continue to be a disruptive business force. Are we entering another phase  real-time digital business transformation, where businesses are realizing that the time to adjust to market and customer opportunities and threats is shrinking quickly?
HP has integrated Hadoop into the core of our Big Data platform, solutions and services. We will introduce the HP Big Data Reference Architecture and a series of services that will accelerate your adoption of Hadoop. HP's new BDRA for Hadoop offers an extremely flexible and powerful platform, when HP Haven Big Data Software solutions can be used to augment Hadoop and build a smarter Data Lake.
Join Intel and HP as they discuss new innovations in new server designs powered by Intel Architecture that are enabling customers to adopt and grow their big data environments with confidence.
Big data.  Is this the real life?  Is this just fantasy?  Caught in a landslide.  No escape from reality.  A brief walk through some of the developments that are happening now with Big Data at the core.
Presentation focusing on the opportunities and challenges presented by big data, featuring a technical overview of how Informatica can help deliver trusted and timely data and including examples of customer best practice.
Apache Atlas proposes to provide governance capabilities in Hadoop that use both a prescriptive and forensic models enriched by business taxonomical metadata.
This session will look at Ciscos Data Warehouse Optimisation (DWO) Solution, and explores how it reduces ever-growing data warehouse management costs, whilst delivering agreater variety and volume of data that can can be ingested and stored to derive new business insights.
This presentation shares how SAS can help spread the use of Hadoop to less technical audiences, showcasing some of the end-user technologies already implemented at SAS customers that can help across the spectrum of data ingestion and management, visualization, and analytics.
This session will describe a method of data replication between Hadoop in multiple locations that achieves 100% utilization of globally-distributed infrastructure, while maintaining data consistency in the face of network, process, and machine failures. The session will also describe how computation is executed across sites, and provide example architectures using MapReduce and Hive.
Most organizations nowadays see the massive value potential in (big) data analytics. What most of them still fear is that starting an analytics initiative will result in a massive IT project that will take 12-18 months before first analytical results are achieved  and deploying the results to generate business value will take another 12-18 months. . .
As Hadoop adoption continues to grow, the number of workflows and their complexity increases. This session describes how organizations are managing Hadoop and big data workflows with an enterprise workflow solution that provides a graphical user interface for managing all of the complex components of the enterprise application fabric.
Big data and analytics continue to be a disruptive business force. Are we entering another phase  real-time digital business transformation, where businesses are realizing that the time to adjust to market and customer opportunities and threats is shrinking quickly?
HP has integrated Hadoop into the core of our Big Data platform, solutions and services. We will introduce the HP Big Data Reference Architecture and a series of services that will accelerate your adoption of Hadoop. HP's new BDRA for Hadoop offers an extremely flexible and powerful platform, when HP Haven Big Data Software solutions can be used to augment Hadoop and build a smarter Data Lake.
Join Intel and HP as they discuss new innovations in new server designs powered by Intel Architecture that are enabling customers to adopt and grow their big data environments with confidence.
Big data.  Is this the real life?  Is this just fantasy?  Caught in a landslide.  No escape from reality.  A brief walk through some of the developments that are happening now with Big Data at the core.
Presentation focusing on the opportunities and challenges presented by big data, featuring a technical overview of how Informatica can help deliver trusted and timely data and including examples of customer best practice.
Apache Atlas proposes to provide governance capabilities in Hadoop that use both a prescriptive and forensic models enriched by business taxonomical metadata.
Join the authors of "Hadoop Application Architectures" for an open Q/A session on considerations and recommendations for architecture and design of applications using Hadoop. Talk to us about your use-case and its big data architecture, or just come to listen in.
Pragmatic approach to data science, data science pipeline, and machine learning for different vertical applications. How to train existing staff to take on data science and engineering challenges. Where are the unicorns? How to find and hire great data scientists. How to evaluate their skillset. How to build and manage an innovative data...
Join the Spark team for an informal question and answer session. Spark committers from Databricks will be on hand to field a wide range of detailed questions. Even if you don't have a specific question, join in to hear what others are asking.
Join the authors of "Hadoop Application Architectures" for an open Q/A session on considerations and recommendations for architecture and design of applications using Hadoop. Talk to us about your use-case and its big data architecture, or just come to listen in.
Pragmatic approach to data science, data science pipeline, and machine learning for different vertical applications. How to train existing staff to take on data science and engineering challenges. Where are the unicorns? How to find and hire great data scientists. How to evaluate their skillset. How to build and manage an innovative data...
Join the Spark team for an informal question and answer session. Spark committers from Databricks will be on hand to field a wide range of detailed questions. Even if you don't have a specific question, join in to hear what others are asking.
This three-day curriculum features advanced lectures and hands-on technical exercises for advanced Spark usage in data exploration, analysis, and building big data applications. Course materials emphasize architectural design patterns and best practices for leveraging Spark in the context of other popular, complementary frameworks for building and managing enterprise data workflows.
This three-day curriculum features advanced lectures and hands-on technical exercises for advanced Spark usage in data exploration, analysis, and building big data applications. Course materials emphasize architectural design patterns and best practices for leveraging Spark in the context of other popular, complementary frameworks for building and managing enterprise data workflows.
This three-day curriculum features advanced lectures and hands-on technical exercises for advanced Spark usage in data exploration, analysis, and building big data applications. Course materials emphasize architectural design patterns and best practices for leveraging Spark in the context of other popular, complementary frameworks for building and managing enterprise data workflows.
This three-day curriculum features advanced lectures and hands-on technical exercises for advanced Spark usage in data exploration, analysis, and building big data applications. Course materials emphasize architectural design patterns and best practices for leveraging Spark in the context of other popular, complementary frameworks for building and managing enterprise data workflows.
This three-day curriculum features advanced lectures and hands-on technical exercises for advanced Spark usage in data exploration, analysis, and building big data applications. Course materials emphasize architectural design patterns and best practices for leveraging Spark in the context of other popular, complementary frameworks for building and managing enterprise data workflows.
This three-day curriculum features advanced lectures and hands-on technical exercises for advanced Spark usage in data exploration, analysis, and building big data applications. Course materials emphasize architectural design patterns and best practices for leveraging Spark in the context of other popular, complementary frameworks for building and managing enterprise data workflows.
Cloudera University's one-day essentials course presents an overview of Apache Hadoop and how it can help decision-makers meet business goals, providing a fundamental introduction to the main components of Hadoop and its use cases in various industries. This course is a good starting point for any role or set of objectives and is part of the data analyst learning path.
This intensive two-day course will provide you with a condensed introduction to the key concepts and techniques of machine learning. It will allow you to know what is and is not possible with these exciting new tools, and understand how they can benefit your organization. It will give you the language and framework to talk to both experts and executives.
This intensive two-day course will provide you with a condensed introduction to the key concepts and techniques of machine learning. It will allow you to know what is and is not possible with these exciting new tools, and understand how they can benefit your organization. It will give you the language and framework to talk to both experts and executives.
Cloudera University's one-day essentials course presents an overview of Apache Hadoop and how it can help decision-makers meet business goals, providing a fundamental introduction to the main components of Hadoop and its use cases in various industries. This course is a good starting point for any role or set of objectives and is part of the data analyst learning path.
This intensive two-day course will provide you with a condensed introduction to the key concepts and techniques of machine learning. It will allow you to know what is and is not possible with these exciting new tools, and understand how they can benefit your organization. It will give you the language and framework to talk to both experts and executives.
This intensive two-day course will provide you with a condensed introduction to the key concepts and techniques of machine learning. It will allow you to know what is and is not possible with these exciting new tools, and understand how they can benefit your organization. It will give you the language and framework to talk to both experts and executives.
Program Chairs Roger Magoulas, Doug Cutting, and Alistair Croll welcome you to the first day of keynotes.
In this presentation we will guide you through the data adventure that we have embarked on to deliver corporate goals. This digital transformation will help us to achieve customer experience and satisfaction improvement whilst adhering to security and data governance challenges to protect our customers.
After five years of enterprise adoption, Hadoop is now a critical data asset in your analytic and data platform strategy. Some companies, however, are struggling with making Hadoop work for their enterprise needs. . .
Cait O'Riordan, VP of product, music, and platforms, Shazam
Healthcare is in the early stages of a revolution, as almost everything that determines our health is now becoming knowable. Data-driven healthcare represents an unheralded opportunity to make a huge leap forward. At this pivotal moment in medical history, we need to overcome an attitudinal aversion to utilizing the promise of data analysis to provide medical insight and save lives.
At Google, few things are so pervasive as Bigtable, the famous wide-column NoSQL database. It lies behind nearly every major Google product (Gmail, YouTube, Google Analytics), with its own class of internal memes, and a resource footprint unmatched anywhere else in the world.
Julie Meyer, chairman, CEO, and founder, Ariadne Capital
We're always talking about "innovation", but - says Tim Harford - there are really two very different kinds of innovation. Using stories from sports, science, music, and military history, Tim will make you think different about where good ideas come from and how they should be encouraged.
Program Chairs Roger Magoulas, Doug Cutting, and Alistair Croll welcome you to the second day of keynotes.
In this session, Phill Radley (Chief Data Architect at British Telecom) gives an overview of BT's internal multi-tenant hadoop platform. He explains  their first production use case (master data management of BT UK Business Customer data) and gives a flavour of their use case pipeline.
Big data and analytics continue to be a disruptive business force. Are we entering another phase  real-time digital business transformation, where businesses are realizing that the time to adjust to market and customer opportunities and threats is shrinking quickly?
Jet engines, lifelike movie monsters, cancer-fighting nanorobots, and bespoke products. We live in a world where everything around us is designed by someone. The pace of innovation is escalating and with new methods of manufacturing, such as 3D printing, the demands placed on designers and design technology are increasing.
Join SASs Tamara Dull as she compares bike riding to current trends in big data adoption and explains why newer technologies like Hadoop arent always to blame.
As the internet of things and connected car programs across the globe gain momentum and broaden in scope, check out this world record attempt; racing from North Cape, Norway to Cape Agulhas, South Africa. . .
Joanne Hannaford, Partner, Goldman Sachs.
Christine Flounders, Bloomberg
We are being watched  by companies, by the government, by our neighbors. Technology has made powerful surveillance tools available to everyone. And now some of us are investing in counter-surveillance techniques and tactics.
Program Chairs Roger Magoulas, Doug Cutting, and Alistair Croll welcome you to the first day of keynotes.
In this presentation we will guide you through the data adventure that we have embarked on to deliver corporate goals. This digital transformation will help us to achieve customer experience and satisfaction improvement whilst adhering to security and data governance challenges to protect our customers.
After five years of enterprise adoption, Hadoop is now a critical data asset in your analytic and data platform strategy. Some companies, however, are struggling with making Hadoop work for their enterprise needs. . .
Cait O'Riordan, VP of product, music, and platforms, Shazam
Healthcare is in the early stages of a revolution, as almost everything that determines our health is now becoming knowable. Data-driven healthcare represents an unheralded opportunity to make a huge leap forward. At this pivotal moment in medical history, we need to overcome an attitudinal aversion to utilizing the promise of data analysis to provide medical insight and save lives.
At Google, few things are so pervasive as Bigtable, the famous wide-column NoSQL database. It lies behind nearly every major Google product (Gmail, YouTube, Google Analytics), with its own class of internal memes, and a resource footprint unmatched anywhere else in the world.
Julie Meyer, chairman, CEO, and founder, Ariadne Capital
We're always talking about "innovation", but - says Tim Harford - there are really two very different kinds of innovation. Using stories from sports, science, music, and military history, Tim will make you think different about where good ideas come from and how they should be encouraged.
Program Chairs Roger Magoulas, Doug Cutting, and Alistair Croll welcome you to the second day of keynotes.
In this session, Phill Radley (Chief Data Architect at British Telecom) gives an overview of BT's internal multi-tenant hadoop platform. He explains  their first production use case (master data management of BT UK Business Customer data) and gives a flavour of their use case pipeline.
Big data and analytics continue to be a disruptive business force. Are we entering another phase  real-time digital business transformation, where businesses are realizing that the time to adjust to market and customer opportunities and threats is shrinking quickly?
Jet engines, lifelike movie monsters, cancer-fighting nanorobots, and bespoke products. We live in a world where everything around us is designed by someone. The pace of innovation is escalating and with new methods of manufacturing, such as 3D printing, the demands placed on designers and design technology are increasing.
Join SASs Tamara Dull as she compares bike riding to current trends in big data adoption and explains why newer technologies like Hadoop arent always to blame.
As the internet of things and connected car programs across the globe gain momentum and broaden in scope, check out this world record attempt; racing from North Cape, Norway to Cape Agulhas, South Africa. . .
Joanne Hannaford, Partner, Goldman Sachs.
Christine Flounders, Bloomberg
We are being watched  by companies, by the government, by our neighbors. Technology has made powerful surveillance tools available to everyone. And now some of us are investing in counter-surveillance techniques and tactics.
Python has become an increasingly important part of the data-engineer and analytic-tool landscapes. PyData at Strata provides in-depth coverage of the tools and techniques gaining traction with the data audience, including IPython Notebook, NumPy/matplotlib, SciPy, and scikit-learn, and explores how to scale Python performance, including handling large, distributed datasets.
Robert Nishihara offers an overview of SparkNet, a framework for training deep networks in Spark using existing deep learning libraries (such as Caffe) for the backend. SparkNet gets an order of magnitude speedup from distributed training relative to Caffe on a single GPU, even in the regime in which communication is extremely expensive.
Recommender systems use machine-learning algorithms to surface relevant products to consumers. While they are extremely effective, they cannot fully replace human interpretation. The two have very different capabilities that are additive. Eric Colson shows what's possible when the unique contributions of machines are combined with those of human experts to create a truly personalized experience.
At Eventbrite, users can serendipitously discover events they will love. But making this possible isn't easy. Events are short lived, and by the time Eventbrite can build an adequate collaborative-filtering model, the event is already over. John Berryman explains how Eventbrite overcomes these technical challenges with a combination of collaborative-filtering and content-based methods.
There is a big difference between running a machine-learning algorithm manually from time to time and building a production system that runs thousands of machine-learning algorithms each day on petabytes of data, while also dealing with all the edge cases that arise. Robert Grossman discusses some of the lessons learned when building such a system and explores the tools that made the job easier.
Best practices from scientific research can significantly increase the pace and quality of data science projects. Erik Andrejko discusses the benefits and challenges of reproducibility and collaboration, including review and inter-team communication, for data science work at the Climate Corporation.
Weve all heard that rare breed the data scientist described as a unicorn. In building your DS team, should you hold out for that unicorn or create groups of specialists who can work together? Michael Dauber, Yael Garten, Monica Rogati, and Daniel Tunkelang discuss the pros and cons of various team models to help you decide what works best for your particular situation and organization.
Chi-Yi Kuan, Weidong Zhang, and Yongzheng Zhang explain how LinkedIn has built a "voice of member" platform to analyze hundreds of millions of text documents. Chi-Yi, Weidong, and Yongzheng illustrate the critical components of this platform and showcase how LinkedIn leverages it to derive insights such as customer value propositions from an enormous amount of unstructured data.
Despite Python's popularity throughout the data-engineering and data science workflow, the principles behind its performance and scaling behavior are less understood. Travis Oliphant explains best practices and modern tools to scale Python to larger-than-memory and distributed workloads without sacrificing its ease of use or being forced to adopt heavyweight frameworks.
Marcel Kornacker explains how to use nested data structures to increase analytic productivity. Marcel uses the well-known TPC-H schema to demonstrate how to simplify analytic workloads with nested schemas.
Siddha Ganju explains how CERN uses machine-learning models to predict which datasets will become popular over time. This helps to replicate the datasets that are most heavily accessed, which improves the efficiency of physics analysis in CMS. Analyzing this data leads to useful information about the physical processes.
Scott Draves gives an overview of the Beaker notebook, a new open source tool for data scientists. Beaker was designed to be polyglot: a single notebook may contain cells from multiple languages that communicate with one another through a unique feature called autotranslation. Scott discusses motivations for the design, reviews the architecture, and gives a demo of Beaker in action.
Python has become an increasingly important part of the data-engineer and analytic-tool landscapes. PyData at Strata provides in-depth coverage of the tools and techniques gaining traction with the data audience, including IPython Notebook, NumPy/matplotlib, SciPy, and scikit-learn, and explores how to scale Python performance, including handling large, distributed datasets.
Robert Nishihara offers an overview of SparkNet, a framework for training deep networks in Spark using existing deep learning libraries (such as Caffe) for the backend. SparkNet gets an order of magnitude speedup from distributed training relative to Caffe on a single GPU, even in the regime in which communication is extremely expensive.
Recommender systems use machine-learning algorithms to surface relevant products to consumers. While they are extremely effective, they cannot fully replace human interpretation. The two have very different capabilities that are additive. Eric Colson shows what's possible when the unique contributions of machines are combined with those of human experts to create a truly personalized experience.
At Eventbrite, users can serendipitously discover events they will love. But making this possible isn't easy. Events are short lived, and by the time Eventbrite can build an adequate collaborative-filtering model, the event is already over. John Berryman explains how Eventbrite overcomes these technical challenges with a combination of collaborative-filtering and content-based methods.
There is a big difference between running a machine-learning algorithm manually from time to time and building a production system that runs thousands of machine-learning algorithms each day on petabytes of data, while also dealing with all the edge cases that arise. Robert Grossman discusses some of the lessons learned when building such a system and explores the tools that made the job easier.
Best practices from scientific research can significantly increase the pace and quality of data science projects. Erik Andrejko discusses the benefits and challenges of reproducibility and collaboration, including review and inter-team communication, for data science work at the Climate Corporation.
Weve all heard that rare breed the data scientist described as a unicorn. In building your DS team, should you hold out for that unicorn or create groups of specialists who can work together? Michael Dauber, Yael Garten, Monica Rogati, and Daniel Tunkelang discuss the pros and cons of various team models to help you decide what works best for your particular situation and organization.
Chi-Yi Kuan, Weidong Zhang, and Yongzheng Zhang explain how LinkedIn has built a "voice of member" platform to analyze hundreds of millions of text documents. Chi-Yi, Weidong, and Yongzheng illustrate the critical components of this platform and showcase how LinkedIn leverages it to derive insights such as customer value propositions from an enormous amount of unstructured data.
Despite Python's popularity throughout the data-engineering and data science workflow, the principles behind its performance and scaling behavior are less understood. Travis Oliphant explains best practices and modern tools to scale Python to larger-than-memory and distributed workloads without sacrificing its ease of use or being forced to adopt heavyweight frameworks.
Marcel Kornacker explains how to use nested data structures to increase analytic productivity. Marcel uses the well-known TPC-H schema to demonstrate how to simplify analytic workloads with nested schemas.
Siddha Ganju explains how CERN uses machine-learning models to predict which datasets will become popular over time. This helps to replicate the datasets that are most heavily accessed, which improves the efficiency of physics analysis in CMS. Analyzing this data leads to useful information about the physical processes.
Scott Draves gives an overview of the Beaker notebook, a new open source tool for data scientists. Beaker was designed to be polyglot: a single notebook may contain cells from multiple languages that communicate with one another through a unique feature called autotranslation. Scott discusses motivations for the design, reviews the architecture, and gives a demo of Beaker in action.
From advanced visualization, collaboration, and reproducibility to big data, R Day at Strata covers a raft of current topics that analysts and R users need to pay attention to. The R Day tutorials come from leading luminaries and R committersthe folks keeping the R ecosystem apace of the challenges facing analysts and others who work with data.
Panoramix makes it easy to slice, dice, and visualize your data. Point it to Druid (or almost any other database) and navigate through your data at the speed of thought. Maxime Beauchemin outlines the features and use cases for Panoramix.
Assuming breach led to centralizing all logs (SIEMs), but incident response and forensics are still behind on the analytics side. Leo Meyerovich, Mike Wendt, and Joshua Patterson share how Graphistry and Accenture Technology Labs are rethinking data engineering and data analysis and modernizing end-to-end architectures.
Nick Turner offers a case study of Markerstudy, an insurance and insurance-related-services company based in the UK that recreated their data platform around Hadoop. Dubbed the Big Data Insight project, the new platform features near real-time reporting and self-service exploration and has resulted in reduced claims costs, better fraud detection, and increased customer-retention rates.
Most people think of data visualizations as charts or graphs with perhaps some interactivity. Christopher Nguyen and Anh Trinh present a new approach that considers visualizations to be first-class objects that also act as data sources and sinks. This enables powerful collaboration where thousands of users can build on the work of one another by sharing these visualization objects.
Data visualization is everywhereit communicates meaningful data, finds insights through exploratory interfaces, and informs people through data-driven content. More and more, consumers expect to interact with the data, not just consume it. Irene Ros explains how to employ techniques from user-centered design to build better data-visualization interfaces.
Big data is great for feeding ML algorithms, but you quickly face a bandwidth issue when interfacing with humans. The brain is a fantastic information-processing machine and has an unparalleled, innate ability to detect patterns. Sbastien Pierre explains what designers can teach engineers about creating new ways to make large volumes of data understandable at the human level.
Noah Iliinsky surveys the state of visualization, outlines the major trends in the field, and explores the directions that visualization is headed. Noah also dives into the assorted tool domainsfrom enterprise to desktop to code-basedand discusses the pros and cons and use cases of each.
In his 20+ years of applying machine learning and data analysis to a wide range of industries, Jeremy Howard never felt that his work really changed anyone's life in a deep and positive way, so he spent a year researching ways he might effect real change. Jeremy outlines the impact that deep learning is going to make on the world and explains how you too can make a difference.
Seemingly harmless choices in visualization, design, and content selection can distort your data and lead to false conclusions. Aneesh Karve presents a framework for identifying and overcoming these distortions by drawing upon research in human perception, focus and context, and mobile design.
With more than 1.4 billion smartphones and at least half that many tablets in use, there is a tremendous need for responsive web design in the data-visualization sphere. Bill Hinderman explains the principles of responsive data visualization, which allows you to respond to screen conditions as well as data conditions.
Vowpal Wabbit (VW) is a fast out-of-core learning system that pushes the frontier of machine learning. Jeroen Janssens offers a practical introduction to VW from both RStudio and the Unix command line and demonstrates how it can be used to perform tasks such as classification, regression, matrix factorization, and topic modeling.
From advanced visualization, collaboration, and reproducibility to big data, R Day at Strata covers a raft of current topics that analysts and R users need to pay attention to. The R Day tutorials come from leading luminaries and R committersthe folks keeping the R ecosystem apace of the challenges facing analysts and others who work with data.
Panoramix makes it easy to slice, dice, and visualize your data. Point it to Druid (or almost any other database) and navigate through your data at the speed of thought. Maxime Beauchemin outlines the features and use cases for Panoramix.
Assuming breach led to centralizing all logs (SIEMs), but incident response and forensics are still behind on the analytics side. Leo Meyerovich, Mike Wendt, and Joshua Patterson share how Graphistry and Accenture Technology Labs are rethinking data engineering and data analysis and modernizing end-to-end architectures.
Nick Turner offers a case study of Markerstudy, an insurance and insurance-related-services company based in the UK that recreated their data platform around Hadoop. Dubbed the Big Data Insight project, the new platform features near real-time reporting and self-service exploration and has resulted in reduced claims costs, better fraud detection, and increased customer-retention rates.
Most people think of data visualizations as charts or graphs with perhaps some interactivity. Christopher Nguyen and Anh Trinh present a new approach that considers visualizations to be first-class objects that also act as data sources and sinks. This enables powerful collaboration where thousands of users can build on the work of one another by sharing these visualization objects.
Data visualization is everywhereit communicates meaningful data, finds insights through exploratory interfaces, and informs people through data-driven content. More and more, consumers expect to interact with the data, not just consume it. Irene Ros explains how to employ techniques from user-centered design to build better data-visualization interfaces.
Big data is great for feeding ML algorithms, but you quickly face a bandwidth issue when interfacing with humans. The brain is a fantastic information-processing machine and has an unparalleled, innate ability to detect patterns. Sbastien Pierre explains what designers can teach engineers about creating new ways to make large volumes of data understandable at the human level.
Noah Iliinsky surveys the state of visualization, outlines the major trends in the field, and explores the directions that visualization is headed. Noah also dives into the assorted tool domainsfrom enterprise to desktop to code-basedand discusses the pros and cons and use cases of each.
In his 20+ years of applying machine learning and data analysis to a wide range of industries, Jeremy Howard never felt that his work really changed anyone's life in a deep and positive way, so he spent a year researching ways he might effect real change. Jeremy outlines the impact that deep learning is going to make on the world and explains how you too can make a difference.
Seemingly harmless choices in visualization, design, and content selection can distort your data and lead to false conclusions. Aneesh Karve presents a framework for identifying and overcoming these distortions by drawing upon research in human perception, focus and context, and mobile design.
With more than 1.4 billion smartphones and at least half that many tablets in use, there is a tremendous need for responsive web design in the data-visualization sphere. Bill Hinderman explains the principles of responsive data visualization, which allows you to respond to screen conditions as well as data conditions.
Vowpal Wabbit (VW) is a fast out-of-core learning system that pushes the frontier of machine learning. Jeroen Janssens offers a practical introduction to VW from both RStudio and the Unix command line and demonstrates how it can be used to perform tasks such as classification, regression, matrix factorization, and topic modeling.
The term "data visualization" can mean anything from charts and graphs to infographics to big data and everything in between. Brian Suda explores the basics of how to design with data, specifically using the industry-standard D3 library. By the end of Brian's tutorial, you'll be able to create data visualizations with your own datasets.
Jonathan Seidman, Ted Malaska, Gwen Shapira, and Mark Grover walk participants through building a fraud-detection system, using an end-to-end case study to provide a concrete example of how to architect and implement real-time systems via Apache Hadoop components like Kafka, HBase, Impala, and Spark.
Chris Sanden and Christopher Colburn outline a shared infrastructure for doing anomaly detection. Chris and Christopher explain how their solution addresses both real-time and batch use cases and offer a framework for performance evaluation.
If you consider user click paths a process, you can apply process mining. Process mining models users based on their actual behavior, which allows us to compare new clicks with modeled behavior and report any inconsistencies. Bolke de Bruin and Hylke Hendriksen explain how ING implemented process mining on Spark Streaming, enabling real-time fraud detection.
Data scientists inhabit such an ever-changing landscape of languages, packages, and frameworks that it can be easy to succumb to tool fatigue. If this sounds familiar, you may have missed the increasing popularity of Linux containers in the DevOps world, in particular Docker. Michelangelo D'Agostino demonstrates why Docker deserves a place in every data scientists toolkit.
BayesDB enables rapid prototyping and incremental refinement of statistical models by combining a model-independent declarative query language, BQL, with machine-assisted modeling and compositional models. Richard Tibbetts and Vikash Mansinghka explore the applications of BayesDB for analyzing and understanding developmental economics data in collaboration with the Gates Foundation.
Each year, 15 million people suffer strokes, and at least a fifth of those are due to atrial fibrillation, the most common heart arrhythmia. Brandon Ballinger reports on a collaboration between UCSF cardiologists and ex-Google data scientists that detects atrial fibrillation with deep learning.
Time series data is increasingly ubiquitous with both the adoption of electronic health record (EHR) systems in hospitals and clinics and the proliferation of wearable sensors. Josh Patterson, David Kale, and Zachary Lipton bring the open source deep learning library DL4J to bear on the challenge of analyzing clinical time series using recurrent neural networks (RNNs).
Companies are looking for a single database engine that can address all their varied needsfrom transactional to analytical workloads, against structured, semistructured, and unstructured data, leveraging graph databases, document stores, text search engines, column stores, key value stores, and wide column stores. Rohit Jain discusses the challenges one faces on the path to this nirvana.
Many big data projects work in the lab yet never make it to full-scale production. Lengthy deployments and expertise shortages hinder enterprise adoption. Cloud deployments help but create challenges with security, integration, and costs. Prat Moghe outlines best practices for leveraging the modern big data stack and public cloud infrastructure while maintaining enterprise-grade standards.
Roopa Tangirala details Netflix's migration from Oracle to Cassandra, covering the problems encountered, what worked and what didn't, and lessons learned along the way.
There are (too?) many options for BI on Hadoop. Some are great at exploration, some are great at OLAP, some are fast, and some are flexible. Understanding the options and how they work with Hadoop systems is a key challenge for many organizations. Jacques Nadeau provides a survey of the main options, both traditional (Tableau, Qlik, etc.) and new (Platfora, Datameer, etc.).
Data initiatives are often approached with a feast-or-famine mentality: go big and do it all or go home. Bill Loconzolo explains how established enterprises can build scalable, secure data pipelines that create connections between central data and product teams and enable business results that matter. Learn the framework Bill developed to realize your big data vision.
The term "data visualization" can mean anything from charts and graphs to infographics to big data and everything in between. Brian Suda explores the basics of how to design with data, specifically using the industry-standard D3 library. By the end of Brian's tutorial, you'll be able to create data visualizations with your own datasets.
Jonathan Seidman, Ted Malaska, Gwen Shapira, and Mark Grover walk participants through building a fraud-detection system, using an end-to-end case study to provide a concrete example of how to architect and implement real-time systems via Apache Hadoop components like Kafka, HBase, Impala, and Spark.
Chris Sanden and Christopher Colburn outline a shared infrastructure for doing anomaly detection. Chris and Christopher explain how their solution addresses both real-time and batch use cases and offer a framework for performance evaluation.
If you consider user click paths a process, you can apply process mining. Process mining models users based on their actual behavior, which allows us to compare new clicks with modeled behavior and report any inconsistencies. Bolke de Bruin and Hylke Hendriksen explain how ING implemented process mining on Spark Streaming, enabling real-time fraud detection.
Data scientists inhabit such an ever-changing landscape of languages, packages, and frameworks that it can be easy to succumb to tool fatigue. If this sounds familiar, you may have missed the increasing popularity of Linux containers in the DevOps world, in particular Docker. Michelangelo D'Agostino demonstrates why Docker deserves a place in every data scientists toolkit.
BayesDB enables rapid prototyping and incremental refinement of statistical models by combining a model-independent declarative query language, BQL, with machine-assisted modeling and compositional models. Richard Tibbetts and Vikash Mansinghka explore the applications of BayesDB for analyzing and understanding developmental economics data in collaboration with the Gates Foundation.
Each year, 15 million people suffer strokes, and at least a fifth of those are due to atrial fibrillation, the most common heart arrhythmia. Brandon Ballinger reports on a collaboration between UCSF cardiologists and ex-Google data scientists that detects atrial fibrillation with deep learning.
Time series data is increasingly ubiquitous with both the adoption of electronic health record (EHR) systems in hospitals and clinics and the proliferation of wearable sensors. Josh Patterson, David Kale, and Zachary Lipton bring the open source deep learning library DL4J to bear on the challenge of analyzing clinical time series using recurrent neural networks (RNNs).
Companies are looking for a single database engine that can address all their varied needsfrom transactional to analytical workloads, against structured, semistructured, and unstructured data, leveraging graph databases, document stores, text search engines, column stores, key value stores, and wide column stores. Rohit Jain discusses the challenges one faces on the path to this nirvana.
Many big data projects work in the lab yet never make it to full-scale production. Lengthy deployments and expertise shortages hinder enterprise adoption. Cloud deployments help but create challenges with security, integration, and costs. Prat Moghe outlines best practices for leveraging the modern big data stack and public cloud infrastructure while maintaining enterprise-grade standards.
Roopa Tangirala details Netflix's migration from Oracle to Cassandra, covering the problems encountered, what worked and what didn't, and lessons learned along the way.
There are (too?) many options for BI on Hadoop. Some are great at exploration, some are great at OLAP, some are fast, and some are flexible. Understanding the options and how they work with Hadoop systems is a key challenge for many organizations. Jacques Nadeau provides a survey of the main options, both traditional (Tableau, Qlik, etc.) and new (Platfora, Datameer, etc.).
Data initiatives are often approached with a feast-or-famine mentality: go big and do it all or go home. Bill Loconzolo explains how established enterprises can build scalable, secure data pipelines that create connections between central data and product teams and enable business results that matter. Learn the framework Bill developed to realize your big data vision.
In this full-day tutorial, participants will get an overview of all aspects of successfully managing Hadoop clustersfrom installation to configuration management, service monitoring, troubleshooting, and support integrationwith an emphasis on production systems.
In the era of large-volume security applications, false positives, as Gartner says, can make the difference between building an "indicator machine" and an "answering machine." Ram Shankar and Cody Rioux explore how to suppress false positives in security monitoring systems through use cases from Microsoft and Netflix.
With Apache Kakfa 0.9, the community has introduced a number of features to make data streams secure. Jun Rao explains the motivation for making these changes, discusses the design of Kafka security, and demonstrates how to secure a Kafka cluster. Jun also covers common pitfalls in securing Kafka and ongoing security work.
Pratik Verma and Paulo Pereira share three security architecture principles for Hadoop to protect sensitive data without disrupting users: modifying requests to filter content makes security transparent to users; centralizing data-access decisions and distributing enforcement makes security scalable; and using metadata instead of files or tables ensures systematic protection of sensitive data.
Chao Sun and Alex Leblang explore RecordService, a new solution that provides an API to read data from Hadoop storage managers and return them as canonical records. This eliminates the need for components to support individual file formats, handle security, perform auditing, and implement sophisticated IO scheduling and other common processing that is at the bottom of any computation.
Yinglian Xie describes the anatomy of modern online services, where large armies of malicious accounts hide among legitimate users and conduct a variety of attacks. Yinglian demonstrates how the Spark framework can facilitate early detection of these types of attacks by analyzing billions of user actions.
Bosco Durai offers a top-down view of security in the Hadoop ecosystem. Bosco explores the right way to protect your data based on your enterprise's security requirements, as he covers the available mechanisms to achieve your information security goals.
Figuring out Hadoop is daunting. However, understanding a set of basic yet important principles is all you need to cut through the hype and make intelligent enterprise decisions. Donald Miner breaks down modern Hadoop into 10 important principles you need to know to understand what Hadoop is and how it is different from the old way of doing things.
Over the past decade, machine learning has become intertwined with newer, Internet-born businesses. This despite the fact that the vast majority of global GDP turns on larger, less visible industries like energy and construction. David Beyer explores the ways these backbone industries are adopting machine-intelligent applications and the trends underlying this shift.
Scott Donaldson and Matt Cardillo detail the security measures and system architecture needed to bring alive a multipetabyte data warehouse via interactive analytics and directed graphs from several trillions of market events, using HBase, EMR, Hive, Redshift, and S3 technologies in a cost-efficient manner.
Autodesk's next-gen analytics pipeline, based on SDKs, Kafka, Spark, and containers, will solve the problems of platform and product fragmentation, instrumentation quality, and ease of access to analytics. Yann Landrin and Charlie Crocker explore the features that will enable teams to build reliable, high-quality usage analytics for Autodesk's products, autonomously and in mere minutes.
Typically, 810% of product URLs in ecommerce sites are misclassified. Sreeni Iyer and Anurag Bhardwaj discuss a machine-learning-based solution that relies on an innovative fusion of classifiers that are both text- and image-based, along with human touch to handle edge cases, to automatically classify product URLs according to a canonical taxonomic organization with a high F-score.
In this full-day tutorial, participants will get an overview of all aspects of successfully managing Hadoop clustersfrom installation to configuration management, service monitoring, troubleshooting, and support integrationwith an emphasis on production systems.
In the era of large-volume security applications, false positives, as Gartner says, can make the difference between building an "indicator machine" and an "answering machine." Ram Shankar and Cody Rioux explore how to suppress false positives in security monitoring systems through use cases from Microsoft and Netflix.
With Apache Kakfa 0.9, the community has introduced a number of features to make data streams secure. Jun Rao explains the motivation for making these changes, discusses the design of Kafka security, and demonstrates how to secure a Kafka cluster. Jun also covers common pitfalls in securing Kafka and ongoing security work.
Pratik Verma and Paulo Pereira share three security architecture principles for Hadoop to protect sensitive data without disrupting users: modifying requests to filter content makes security transparent to users; centralizing data-access decisions and distributing enforcement makes security scalable; and using metadata instead of files or tables ensures systematic protection of sensitive data.
Chao Sun and Alex Leblang explore RecordService, a new solution that provides an API to read data from Hadoop storage managers and return them as canonical records. This eliminates the need for components to support individual file formats, handle security, perform auditing, and implement sophisticated IO scheduling and other common processing that is at the bottom of any computation.
Yinglian Xie describes the anatomy of modern online services, where large armies of malicious accounts hide among legitimate users and conduct a variety of attacks. Yinglian demonstrates how the Spark framework can facilitate early detection of these types of attacks by analyzing billions of user actions.
Bosco Durai offers a top-down view of security in the Hadoop ecosystem. Bosco explores the right way to protect your data based on your enterprise's security requirements, as he covers the available mechanisms to achieve your information security goals.
Figuring out Hadoop is daunting. However, understanding a set of basic yet important principles is all you need to cut through the hype and make intelligent enterprise decisions. Donald Miner breaks down modern Hadoop into 10 important principles you need to know to understand what Hadoop is and how it is different from the old way of doing things.
Over the past decade, machine learning has become intertwined with newer, Internet-born businesses. This despite the fact that the vast majority of global GDP turns on larger, less visible industries like energy and construction. David Beyer explores the ways these backbone industries are adopting machine-intelligent applications and the trends underlying this shift.
Scott Donaldson and Matt Cardillo detail the security measures and system architecture needed to bring alive a multipetabyte data warehouse via interactive analytics and directed graphs from several trillions of market events, using HBase, EMR, Hive, Redshift, and S3 technologies in a cost-efficient manner.
Autodesk's next-gen analytics pipeline, based on SDKs, Kafka, Spark, and containers, will solve the problems of platform and product fragmentation, instrumentation quality, and ease of access to analytics. Yann Landrin and Charlie Crocker explore the features that will enable teams to build reliable, high-quality usage analytics for Autodesk's products, autonomously and in mere minutes.
Typically, 810% of product URLs in ecommerce sites are misclassified. Sreeni Iyer and Anurag Bhardwaj discuss a machine-learning-based solution that relies on an innovative fusion of classifiers that are both text- and image-based, along with human touch to handle edge cases, to automatically classify product URLs according to a canonical taxonomic organization with a high F-score.
What are the essential components of a data platform? John Akred, Stephen O'Sullivan, and Gary Dusbabek explain how the various parts of the Hadoop, Spark, and big data ecosystems fit together in production to create a data platform supporting batch, interactive, and real-time analytical workloads.
Big data and data science have great potential for accelerating business, but how do you reconcile the business opportunity with the sea of possible technologies? Conventional data strategy has little to guide us, focusing more on governance than on creating new value. Edd Dumbill and Scott Kurth explain how to create a modern data strategy that powers data-driven business.
The most valuable people in your organization combine business acumen with data savviness. But these data heroes are rare. Denise McInerney describes how she has empowered business users at Intuit to make better decisions with data and explains how you can do the same thing in your organization.
Lior Abraham explores how Tinder reinvented its behavioral analytics approach with Interana to tune matchmaking and business operations. Lior discusses strategies for behavioral analytics and explains how they can be applied at your company to increase conversion, improve engagement, and maximize retention.
In a panel discussion, top-tier VCs look over the horizon and consider the big trends in big data, explaining what they think the field will look like a few years (or more) down the road. Join us as Shivon Zilis, Cack Wilhelm, Michael Dauber, Kristina Bergman, and Roseanne Wincek talk about trends that everyone is seeing and areas for investment that they find exciting.
Data has become a hot career choice, but some fear that a career in data is highly stressful or simply boring. Jin Zhang, Jerry Overton, and Michele Chambers give an overview of the field and its various specializations with the hope that this understanding will eliminate any fear and empower attendees to pursue a career in data.
While many companies struggle to adopt big data, a number of industry leaders are leapfrogging big data adoption by going straight to automating core business processes. Andreas Schmidt presents examples from leading European companies that have overcome cultural, technical, and scientific challenges and unlocked the potential of big data in an entirely different way.
Benedikt Koehler offers approaches to analyzing and visualizing bitcoin dataaccessing and downloading the blockchain, transforming the data into a networked data format, identifying hubs and clusters, and visualizing the results as dynamic network graphsso that typical patterns and anomalies can quickly be identified.
Autodesk's transition to a subscription business model has caused the company to rethink how it interacts with and engages its customers. Adam Sugano details how, in a short period of time, Autodesk has executed numerous data science projects that have enhanced its capabilities to acquire, retain, and provide more value to its customers.
How do you implement Apache Hadoop in a large healthcare company with a mature data-analysis infrastructure? Jeffrey Shmain and Mohammad Quraishi describe Cigna's journey toward big data and Hadoop, including an overview of new Hadoop capabilities like heterogeneous data integration and large-scale machine learning.
Linus Liang and Brad Allen explain how big data is helping Embrace save millions of babies around the world. Embrace invented the world's most affordable infant incubator, but the data it collectsfrom the hardest to reach and most rural parts of the worldwill actually save more lives than the device will.
The Canadian Broadcasting Corporation broadcasts a lot of digital content. And Canadians create a huge amount of data about that content. So how does a public broadcaster, of all entities, broadcast its data exhaust? Christopher Berry details the CBC's early experiments with importing a variant of the lean startup into a 79-year-old institution.
Increasing competition and technological change is impelling the telco industry toward a new model of analytics. Telefnica has been at the front of this change, driving business transformation to a digital telco. John Belchamber and Arturo Canales tell the story of that transformation and detail the pitfalls and challenges faced by teams looking to follow a similar journey.
What are the essential components of a data platform? John Akred, Stephen O'Sullivan, and Gary Dusbabek explain how the various parts of the Hadoop, Spark, and big data ecosystems fit together in production to create a data platform supporting batch, interactive, and real-time analytical workloads.
Big data and data science have great potential for accelerating business, but how do you reconcile the business opportunity with the sea of possible technologies? Conventional data strategy has little to guide us, focusing more on governance than on creating new value. Edd Dumbill and Scott Kurth explain how to create a modern data strategy that powers data-driven business.
The most valuable people in your organization combine business acumen with data savviness. But these data heroes are rare. Denise McInerney describes how she has empowered business users at Intuit to make better decisions with data and explains how you can do the same thing in your organization.
Lior Abraham explores how Tinder reinvented its behavioral analytics approach with Interana to tune matchmaking and business operations. Lior discusses strategies for behavioral analytics and explains how they can be applied at your company to increase conversion, improve engagement, and maximize retention.
In a panel discussion, top-tier VCs look over the horizon and consider the big trends in big data, explaining what they think the field will look like a few years (or more) down the road. Join us as Shivon Zilis, Cack Wilhelm, Michael Dauber, Kristina Bergman, and Roseanne Wincek talk about trends that everyone is seeing and areas for investment that they find exciting.
Data has become a hot career choice, but some fear that a career in data is highly stressful or simply boring. Jin Zhang, Jerry Overton, and Michele Chambers give an overview of the field and its various specializations with the hope that this understanding will eliminate any fear and empower attendees to pursue a career in data.
While many companies struggle to adopt big data, a number of industry leaders are leapfrogging big data adoption by going straight to automating core business processes. Andreas Schmidt presents examples from leading European companies that have overcome cultural, technical, and scientific challenges and unlocked the potential of big data in an entirely different way.
Benedikt Koehler offers approaches to analyzing and visualizing bitcoin dataaccessing and downloading the blockchain, transforming the data into a networked data format, identifying hubs and clusters, and visualizing the results as dynamic network graphsso that typical patterns and anomalies can quickly be identified.
Autodesk's transition to a subscription business model has caused the company to rethink how it interacts with and engages its customers. Adam Sugano details how, in a short period of time, Autodesk has executed numerous data science projects that have enhanced its capabilities to acquire, retain, and provide more value to its customers.
How do you implement Apache Hadoop in a large healthcare company with a mature data-analysis infrastructure? Jeffrey Shmain and Mohammad Quraishi describe Cigna's journey toward big data and Hadoop, including an overview of new Hadoop capabilities like heterogeneous data integration and large-scale machine learning.
Linus Liang and Brad Allen explain how big data is helping Embrace save millions of babies around the world. Embrace invented the world's most affordable infant incubator, but the data it collectsfrom the hardest to reach and most rural parts of the worldwill actually save more lives than the device will.
The Canadian Broadcasting Corporation broadcasts a lot of digital content. And Canadians create a huge amount of data about that content. So how does a public broadcaster, of all entities, broadcast its data exhaust? Christopher Berry details the CBC's early experiments with importing a variant of the lean startup into a 79-year-old institution.
Increasing competition and technological change is impelling the telco industry toward a new model of analytics. Telefnica has been at the front of this change, driving business transformation to a digital telco. John Belchamber and Arturo Canales tell the story of that transformation and detail the pitfalls and challenges faced by teams looking to follow a similar journey.
The real power and value proposition of Apache Spark is in building a unified use case that combines ETL, batch analytics, real-time stream analysis, machine learning, graph processing, and visualizations. Through hands-on examples, Sameer Farooqui explores various Wikipedia datasets to illustrate a variety of ideal programming paradigms.
Netflix is exploring new avenues for data processing where traditional approaches fail to scale. Daniel Weeks explains how Netflix has enhanced its 25+ petabyte warehouse by combining Parquet's features with Presto and Spark to boost both ETL and interactive queries. Daniel explores how these approaches offer new ways to look at the relationship between storage and compute.
Coursera's platform allows 15 million learners to take courses from the best universities. Roshan Sumbaly and Thomas Barthelemy outline the pieces of Coursera's data infrastructure (streaming, data warehouse) that support its growing semi- and unstructured data requirements and explain how this ecosystem allows Coursera to build various instructor- and learner-side data products.
Seshadri Mahalingam and Joe Hellerstein discuss Photon, a high-performance data-transformation engine that provides immediacy to the data-wrangling experience, and demonstrate how to make the most of modern processors from both the browser and the desktop, with a focus on issues specific to the variety of big raw data, including heavy string manipulation and statistical data profiling.
Running distributed systems in production can be tremendously challenging. Fangjin Yang covers common problems and failures with distributed systems and discusses design patterns that can be used to maintain data integrity and availability when everything goes wrong. Fangjin uses Druid as a real-world case study of how these patterns are implemented in an open source technology.
Next-gen UIs will allow people to use plain English to interact with software. However, current published research focuses on abstract understanding, not on translating English into concrete software actions. Joseph Turian and Alex Nisnevich outline UPSHOT's English-to-SQL semantic parser and demonstrate how to build your own English-to-your software application parser.
Organizations do not need a big data strategy. They need a business strategy that incorporates big data. Most organizations lack a roadmap for using big data to uncover new business opportunities. Bill Schmarzo explains how to explore, justify, and plan big data projects with business management.
It is fashionable today to declare doom and gloom for the data lake. Alex Gorelik discusses best practices for Hadoop data lake success and provides real-world examples of successful data lake implementations in a non-vendor-specific talk.
Autofill, spellcheck, and turn-by-turn directions provide just-in-time suggestions. What if guiding users to accurate data were as simple? Debora Seys explains how eBay is delivering self-service analytics by moving from heavily engineered metadata systems to the new world of machine-learned guidance and asynchronous collaboration.
Transamerica built a product recommendation system that can be leveraged across multiple distribution channels to recommend products, serve customer needs, and reduce complexity. Vishal Bamba, Nitin Prabhu, Jeremy Beck, and Amy Wang highlight the machine-learning technology, models, and architecture behind Transamerica's product recommendation platform.
Apache Spark is a versatile big data processing framework, but just because you can program in SQL for Spark does not mean Spark is a database. For an optimal big data infrastructure, you may still need a distributed file system, databases (SQL or NoSQL), message queues, and specialized systems such as ElasticSearch. Vida Ha explains how to design architecture for different use cases.
A data catalog provides context to help data analysts, data scientists, and other data consumers (including those with little technical background) find a relevant dataset, determine if it can be trusted, understand what it means, and utilize it to make better products and better decisions. Aaron Kalb explores how enterprises build interfaces that make sourcing data as easy as shopping on Amazon.
The real power and value proposition of Apache Spark is in building a unified use case that combines ETL, batch analytics, real-time stream analysis, machine learning, graph processing, and visualizations. Through hands-on examples, Sameer Farooqui explores various Wikipedia datasets to illustrate a variety of ideal programming paradigms.
Netflix is exploring new avenues for data processing where traditional approaches fail to scale. Daniel Weeks explains how Netflix has enhanced its 25+ petabyte warehouse by combining Parquet's features with Presto and Spark to boost both ETL and interactive queries. Daniel explores how these approaches offer new ways to look at the relationship between storage and compute.
Coursera's platform allows 15 million learners to take courses from the best universities. Roshan Sumbaly and Thomas Barthelemy outline the pieces of Coursera's data infrastructure (streaming, data warehouse) that support its growing semi- and unstructured data requirements and explain how this ecosystem allows Coursera to build various instructor- and learner-side data products.
Seshadri Mahalingam and Joe Hellerstein discuss Photon, a high-performance data-transformation engine that provides immediacy to the data-wrangling experience, and demonstrate how to make the most of modern processors from both the browser and the desktop, with a focus on issues specific to the variety of big raw data, including heavy string manipulation and statistical data profiling.
Running distributed systems in production can be tremendously challenging. Fangjin Yang covers common problems and failures with distributed systems and discusses design patterns that can be used to maintain data integrity and availability when everything goes wrong. Fangjin uses Druid as a real-world case study of how these patterns are implemented in an open source technology.
Next-gen UIs will allow people to use plain English to interact with software. However, current published research focuses on abstract understanding, not on translating English into concrete software actions. Joseph Turian and Alex Nisnevich outline UPSHOT's English-to-SQL semantic parser and demonstrate how to build your own English-to-your software application parser.
Organizations do not need a big data strategy. They need a business strategy that incorporates big data. Most organizations lack a roadmap for using big data to uncover new business opportunities. Bill Schmarzo explains how to explore, justify, and plan big data projects with business management.
It is fashionable today to declare doom and gloom for the data lake. Alex Gorelik discusses best practices for Hadoop data lake success and provides real-world examples of successful data lake implementations in a non-vendor-specific talk.
Autofill, spellcheck, and turn-by-turn directions provide just-in-time suggestions. What if guiding users to accurate data were as simple? Debora Seys explains how eBay is delivering self-service analytics by moving from heavily engineered metadata systems to the new world of machine-learned guidance and asynchronous collaboration.
Transamerica built a product recommendation system that can be leveraged across multiple distribution channels to recommend products, serve customer needs, and reduce complexity. Vishal Bamba, Nitin Prabhu, Jeremy Beck, and Amy Wang highlight the machine-learning technology, models, and architecture behind Transamerica's product recommendation platform.
Apache Spark is a versatile big data processing framework, but just because you can program in SQL for Spark does not mean Spark is a database. For an optimal big data infrastructure, you may still need a distributed file system, databases (SQL or NoSQL), message queues, and specialized systems such as ElasticSearch. Vida Ha explains how to design architecture for different use cases.
A data catalog provides context to help data analysts, data scientists, and other data consumers (including those with little technical background) find a relevant dataset, determine if it can be trusted, understand what it means, and utilize it to make better products and better decisions. Aaron Kalb explores how enterprises build interfaces that make sourcing data as easy as shopping on Amazon.
Jayant Shekhar, Amandeep Khurana, Krishna Sankar, and Vartika Singh guide participants through techniques for building machine-learning apps using Spark MLlib and Spark ML and demonstrate the principles of graph processing with Spark GraphX.
Joseph Adler, Ewen Cheslack, and Ian Wrigley demonstrate the features of Apache Kafka that make it easy to build fast, secure, and reliable data pipelines and explain how to use Copycat, Kafka Streams, and Kafka Security as they coach you through building a working enterprise data pipeline.
Reynold Xin reviews Sparks adoption and development in 2015. Reynold then looks to the future to outline three major technology trendsthe integration of streaming systems and enterprise data infrastructure, cloud computing and elasticity, and the rise of new hardwarediscuss the major efforts to address these trends, and explore their implications for Spark users.
Vinoth Chandar explains how Uber revamped its foundational data infrastructure with Hadoop as the source-of-truth data lake, sharing lessons from the experience.
The success of Apache Spark is bringing developers to Scala. For big data, the JVM uses memory inefficiently, causing significant GC challenges. Spark's Project Tungsten fixes these problems with custom data layouts and code generation. Dean Wampler gives an overview of Spark, explaining ongoing improvements and what we should do to improve Scala and the JVM for big data.
Want to build models over data every second from millions of sensors? Dig into the histories of millions of financial instruments? Sandy Ryza discusses the unique challenges of time series data and explains how to work with it at scale. Sandy then introduces the open source Spark-Timeseries library, which provides a natural way of munging, manipulating, and modeling time series data.
Alex Silva outlines the implementation of a real-time analytics platform using microservices and a Scala stack that includes Kafka, Spark Streaming, Spray, and Akka. This infrastructure can process vast amounts of streaming data, ranging from video events to clickstreams and logs. The result is a powerful real-time data pipeline capable of flexible data ingestion and fast analysis.
Apache Spark is a fast, general engine for big data processing. As Spark jobs are used for more mission-critical tasks, it is important to have effective tools for testing and validation. Holden Karau details reasonable validation rules for production jobs and best practices for creating effective tests, as well as options for generating test data.
Michael Armbrust explores real-time analytics with Spark from interactive queries to streaming.
Tathagata Das introduces Streaming DataFrames, the next evolution of Spark Streaming. Streaming DataFrames unifies an additional dimension: interactive analysis. In addition, it provides enhanced support for out-of-order (delayed) data, zero-latency decision making and integration with existing enterprise data warehouses.
Spark has been growing in deployments for the past year. Neelesh Srinivas Salian explores common issues observed in a cluster environment setup with Apache Spark and offers guidelines to help setup a real-world environment when planning an Apache Spark deployment in a cluster. Attendees can use these observations to improve the usability and supportability of Apache Spark in their projects.
Schema plays a key role in the Hadoop architecture at Uber. Kelvin Chu and Evan Richards explain why schema is important and how it can make your Hadoop and Spark application more reliable and efficient.
To keep up with the DNA-sequencing-technology revolution, bioinformaticians need more-scalable tools for genomics analysis. Timothy Danford outlines one possible solution in a case study of a cancer genomics analysis pipeline implemented as part of the open source genomics software project, ADAM, which uses Apache Spark-generated abstractions executed on commodity computing infrastructure.
Jayant Shekhar, Amandeep Khurana, Krishna Sankar, and Vartika Singh guide participants through techniques for building machine-learning apps using Spark MLlib and Spark ML and demonstrate the principles of graph processing with Spark GraphX.
Joseph Adler, Ewen Cheslack, and Ian Wrigley demonstrate the features of Apache Kafka that make it easy to build fast, secure, and reliable data pipelines and explain how to use Copycat, Kafka Streams, and Kafka Security as they coach you through building a working enterprise data pipeline.
Reynold Xin reviews Sparks adoption and development in 2015. Reynold then looks to the future to outline three major technology trendsthe integration of streaming systems and enterprise data infrastructure, cloud computing and elasticity, and the rise of new hardwarediscuss the major efforts to address these trends, and explore their implications for Spark users.
Vinoth Chandar explains how Uber revamped its foundational data infrastructure with Hadoop as the source-of-truth data lake, sharing lessons from the experience.
The success of Apache Spark is bringing developers to Scala. For big data, the JVM uses memory inefficiently, causing significant GC challenges. Spark's Project Tungsten fixes these problems with custom data layouts and code generation. Dean Wampler gives an overview of Spark, explaining ongoing improvements and what we should do to improve Scala and the JVM for big data.
Want to build models over data every second from millions of sensors? Dig into the histories of millions of financial instruments? Sandy Ryza discusses the unique challenges of time series data and explains how to work with it at scale. Sandy then introduces the open source Spark-Timeseries library, which provides a natural way of munging, manipulating, and modeling time series data.
Alex Silva outlines the implementation of a real-time analytics platform using microservices and a Scala stack that includes Kafka, Spark Streaming, Spray, and Akka. This infrastructure can process vast amounts of streaming data, ranging from video events to clickstreams and logs. The result is a powerful real-time data pipeline capable of flexible data ingestion and fast analysis.
Apache Spark is a fast, general engine for big data processing. As Spark jobs are used for more mission-critical tasks, it is important to have effective tools for testing and validation. Holden Karau details reasonable validation rules for production jobs and best practices for creating effective tests, as well as options for generating test data.
Michael Armbrust explores real-time analytics with Spark from interactive queries to streaming.
Tathagata Das introduces Streaming DataFrames, the next evolution of Spark Streaming. Streaming DataFrames unifies an additional dimension: interactive analysis. In addition, it provides enhanced support for out-of-order (delayed) data, zero-latency decision making and integration with existing enterprise data warehouses.
Spark has been growing in deployments for the past year. Neelesh Srinivas Salian explores common issues observed in a cluster environment setup with Apache Spark and offers guidelines to help setup a real-world environment when planning an Apache Spark deployment in a cluster. Attendees can use these observations to improve the usability and supportability of Apache Spark in their projects.
Schema plays a key role in the Hadoop architecture at Uber. Kelvin Chu and Evan Richards explain why schema is important and how it can make your Hadoop and Spark application more reliable and efficient.
To keep up with the DNA-sequencing-technology revolution, bioinformaticians need more-scalable tools for genomics analysis. Timothy Danford outlines one possible solution in a case study of a cancer genomics analysis pipeline implemented as part of the open source genomics software project, ADAM, which uses Apache Spark-generated abstractions executed on commodity computing infrastructure.
The world is moving to real-time data, and much of that data flows through Apache Kafka. Jay Kreps explores how Kafka forms the basis for our modern stream-processing architecture. He covers some of the pros and cons of different frameworks and approaches and discusses the recent APIs Kafka has added to allow direct stream processing of Kafka data.
Application messaging isnt newsolutions include IBM MQ, RabbitMQ, and ActiveMQ. Apache Kafka is a high-performance, high-scalability alternative that integrates well with Hadoop. Can modern distributed messaging systems like Kafka be considered a legacy replacement or is it purely complementary? Ted Dunning outlines Kafka's architectural benefits and tradeoffs to find the answer.
Moty Fania shares Intels IT experience implementing an on-premises big data IoT platform for internal use cases. This unique platform was built on top of several open source technologies and enables highly scalable stream analytics with a stack of algorithms such as multisensor change detection, anomaly detection, and more.
Modern houses and robots have a lot in common. Both have a lot of sensors and have to make a lot of decisions. However, unlike houses, robots adapt and perform helpful tasks. Brandon Rohrer details an algorithm specifically designed to help houses, buildings, roads, and stores learn to actively help the people that use them.
Alex Ingerman explains how several AWS services, including Amazon Machine Learning, Amazon Kinesis, AWS Lambda, and Amazon Mechanical Turk, can be tied together to build a predictive application to power a real-time customer-service use case.
Apache Kafka lies at the heart of the largest data pipelines, handling trillions of messages and petabytes of data every day. Learn the right approach for getting the most out of Kafka from the experts at LinkedIn and Confluent. Todd Palino and Gwen Shapira explore how to monitor, optimize, and troubleshoot performance of your data pipelinesfrom producer to consumer, development to production.
Ted Malaska and Jeff Holoman explain how to go from zero to full-on time series and mutable-profile systems in 40 minutes. Ted and Jeff cover code examples of ingestion from Kafka and Spark Streaming and access through SQL, Spark, and Spark SQL to explore the underlying theories and design patterns that will be common for most solutions with Kudu.
You may have successfully made the transition from single machines and one-off solutions to large distributed stream infrastructures in your data center. But what if one data center is not enough? Guozhang Wang offers an overview of best practices for multi-data-center deployments, architecture guidelines for data replication, and disaster scenarios.
Sean Murphy demonstrates how and why the power grid and other legacy industrials built on traditional engineering will be transformed from deterministic machines described by mathematical equations to probabilistic systems requiring streaming data and analytics. Sean demonstrates how to take an agile approach to the scientific method with big data and fuse the two approaches.
DistributedLog is a high-performance replicated log service built on top of Apache BookKeeper that is the foundation of publish-subscribe at Twitter, serving traffic from transactional databases to real-time data analytic pipelines. Sijie Guo offers an overview of DistributedLog, detailing the technical decisions and challenges behind its creation and how it is used at Twitter.
Enterprises are increasingly demanding real-time analytics and insights. Tony Ng offers an overview of Pulsar, an open source real-time streaming system used at eBay, which can scale to millions of events per second with 4GL SQL-like language support. Tony explains how Pulsar integrates Kafka, Kylin, and Druid to provide flexibility and scalability in event and metrics consumption.
The world is moving to real-time data, and much of that data flows through Apache Kafka. Jay Kreps explores how Kafka forms the basis for our modern stream-processing architecture. He covers some of the pros and cons of different frameworks and approaches and discusses the recent APIs Kafka has added to allow direct stream processing of Kafka data.
Application messaging isnt newsolutions include IBM MQ, RabbitMQ, and ActiveMQ. Apache Kafka is a high-performance, high-scalability alternative that integrates well with Hadoop. Can modern distributed messaging systems like Kafka be considered a legacy replacement or is it purely complementary? Ted Dunning outlines Kafka's architectural benefits and tradeoffs to find the answer.
Moty Fania shares Intels IT experience implementing an on-premises big data IoT platform for internal use cases. This unique platform was built on top of several open source technologies and enables highly scalable stream analytics with a stack of algorithms such as multisensor change detection, anomaly detection, and more.
Modern houses and robots have a lot in common. Both have a lot of sensors and have to make a lot of decisions. However, unlike houses, robots adapt and perform helpful tasks. Brandon Rohrer details an algorithm specifically designed to help houses, buildings, roads, and stores learn to actively help the people that use them.
Alex Ingerman explains how several AWS services, including Amazon Machine Learning, Amazon Kinesis, AWS Lambda, and Amazon Mechanical Turk, can be tied together to build a predictive application to power a real-time customer-service use case.
Apache Kafka lies at the heart of the largest data pipelines, handling trillions of messages and petabytes of data every day. Learn the right approach for getting the most out of Kafka from the experts at LinkedIn and Confluent. Todd Palino and Gwen Shapira explore how to monitor, optimize, and troubleshoot performance of your data pipelinesfrom producer to consumer, development to production.
Ted Malaska and Jeff Holoman explain how to go from zero to full-on time series and mutable-profile systems in 40 minutes. Ted and Jeff cover code examples of ingestion from Kafka and Spark Streaming and access through SQL, Spark, and Spark SQL to explore the underlying theories and design patterns that will be common for most solutions with Kudu.
You may have successfully made the transition from single machines and one-off solutions to large distributed stream infrastructures in your data center. But what if one data center is not enough? Guozhang Wang offers an overview of best practices for multi-data-center deployments, architecture guidelines for data replication, and disaster scenarios.
Sean Murphy demonstrates how and why the power grid and other legacy industrials built on traditional engineering will be transformed from deterministic machines described by mathematical equations to probabilistic systems requiring streaming data and analytics. Sean demonstrates how to take an agile approach to the scientific method with big data and fuse the two approaches.
DistributedLog is a high-performance replicated log service built on top of Apache BookKeeper that is the foundation of publish-subscribe at Twitter, serving traffic from transactional databases to real-time data analytic pipelines. Sijie Guo offers an overview of DistributedLog, detailing the technical decisions and challenges behind its creation and how it is used at Twitter.
Enterprises are increasingly demanding real-time analytics and insights. Tony Ng offers an overview of Pulsar, an open source real-time streaming system used at eBay, which can scale to millions of events per second with 4GL SQL-like language support. Tony explains how Pulsar integrates Kafka, Kylin, and Druid to provide flexibility and scalability in event and metrics consumption.
Alistair Croll leads a full day of case studies, panels, and eye-opening presentations that explain how to use data to make better business decisions faster. Tailored to business strategists, marketers, product managers, and entrepreneurs, this fast-paced day focuses on how to solve today's thorniest business problems with big data. It's the missing MBA for a data-driven, always-on business world.
Yahoo uses Druid to provide visibility into the actions of its billions of users and developed a new type of sketch called a Theta Sketch to enable this analysis. Eric Tschetter discusses how Yahoo leverages Druid and Theta Sketches together to enable user-level understanding of their billions of users.
Developers who want both streaming analytics and ad hoc, OLAP-like analysis have often had to develop complex architectures such as Lambda. Helena Edelson and Evan Chan highlight a much simpler approach using the NoLambda stack (Apache Spark/Scala, Mesos, Akka, Cassandra, Kafka) plus FiloDB, a new entrant to the distributed-database world, which combines streaming and ad hoc analytics.
Metadata services are a critical missing piece of the current open source ecosystem for big data. Joe Hellerstein and Vikram Sreekanti give an overview of their vendor-neutral metadata services layer, Ground, through two reference use cases at UC Berkeley: genomics research driven by Spark and courseware using Jupyter Notebooks.
Not all storage resources are equal. Alluxio has developed Alluxio tiered storage to achieve highly efficient utilization of memory, SSDs, and HDDs that is completely transparent to computation frameworks and user applications. Calvin Jia and Jiri Simsa outline the features and use cases of Alluxio tiered storage.
Years of research in nonvolatile memory systems is being productized and has started coming to market. These exciting new technologies promise lower power consumption and higher density for persistent storage. Will these hardware advances revolutionize the data ecosystem as we know it? This compelling panel of data-infrastructure thought leaders discusses the possibilities.
Until recently, batch processing has been the standard model for big data. Today, many have shifted to streaming architectures that offer large benefits in simplicity and robustness, but this isn't your fathers complex event processing. Ted Dunning explores the key design techniques used in modern systems, including percolators, replayable queues, state-point queuing, and microarchitectures.
Costin Leau offers an overview of Elastics current efforts to enhance Elasticsearch's existing integration with Spark, going beyond Spark core and Spark SQL by focusing on text processing and machine learning to allow data processing and tokenizing to be combined with Spark's MLlib algorithms.
Real-time analysis starts with transforming raw data into structured records. Typically this is done with bespoke business logic custom written for each use case. Joey Echeverria presents a configuration-based, reusable library for data transformation that can be embedded in real-time stream-processing systems and demonstrates its real-world use cases with Apache Kafka and Apache Hadoop.
What if we have reached the point where open source can handle massively difficult streaming problems with enterprise-grade durability? Ilya Ganelin presents Capital Ones novel solution for real-time decisioning on Apache Apex. Ilya shows how Apex provides unique capabilities that ensure less than 2 ms latency in an enterprise-grade solution on Hadoop.
Apache Flink is a full-featured streaming framework with high throughput, millisecond latency, strong consistency, support for out-of-order streams, and support for batch as a special case of streaming. Kostas Tzoumas gives an overview of Flink and its streaming-first philosophy, as well as the project roadmap and vision: fully unifying the worlds of batch and streaming analytics.
The Zeta Architecture is an enterprise architecture to move beyond the data lake. The most logical way to scale applications across tiers is to put a messaging platform in between the tiers, which allows a far simpler ability to scale the communications of applications. Jim Scott covers the benefits of this model and offers an example of data-center monitoring.
Alistair Croll leads a full day of case studies, panels, and eye-opening presentations that explain how to use data to make better business decisions faster. Tailored to business strategists, marketers, product managers, and entrepreneurs, this fast-paced day focuses on how to solve today's thorniest business problems with big data. It's the missing MBA for a data-driven, always-on business world.
Yahoo uses Druid to provide visibility into the actions of its billions of users and developed a new type of sketch called a Theta Sketch to enable this analysis. Eric Tschetter discusses how Yahoo leverages Druid and Theta Sketches together to enable user-level understanding of their billions of users.
Developers who want both streaming analytics and ad hoc, OLAP-like analysis have often had to develop complex architectures such as Lambda. Helena Edelson and Evan Chan highlight a much simpler approach using the NoLambda stack (Apache Spark/Scala, Mesos, Akka, Cassandra, Kafka) plus FiloDB, a new entrant to the distributed-database world, which combines streaming and ad hoc analytics.
Metadata services are a critical missing piece of the current open source ecosystem for big data. Joe Hellerstein and Vikram Sreekanti give an overview of their vendor-neutral metadata services layer, Ground, through two reference use cases at UC Berkeley: genomics research driven by Spark and courseware using Jupyter Notebooks.
Not all storage resources are equal. Alluxio has developed Alluxio tiered storage to achieve highly efficient utilization of memory, SSDs, and HDDs that is completely transparent to computation frameworks and user applications. Calvin Jia and Jiri Simsa outline the features and use cases of Alluxio tiered storage.
Years of research in nonvolatile memory systems is being productized and has started coming to market. These exciting new technologies promise lower power consumption and higher density for persistent storage. Will these hardware advances revolutionize the data ecosystem as we know it? This compelling panel of data-infrastructure thought leaders discusses the possibilities.
Until recently, batch processing has been the standard model for big data. Today, many have shifted to streaming architectures that offer large benefits in simplicity and robustness, but this isn't your fathers complex event processing. Ted Dunning explores the key design techniques used in modern systems, including percolators, replayable queues, state-point queuing, and microarchitectures.
Costin Leau offers an overview of Elastics current efforts to enhance Elasticsearch's existing integration with Spark, going beyond Spark core and Spark SQL by focusing on text processing and machine learning to allow data processing and tokenizing to be combined with Spark's MLlib algorithms.
Real-time analysis starts with transforming raw data into structured records. Typically this is done with bespoke business logic custom written for each use case. Joey Echeverria presents a configuration-based, reusable library for data transformation that can be embedded in real-time stream-processing systems and demonstrates its real-world use cases with Apache Kafka and Apache Hadoop.
What if we have reached the point where open source can handle massively difficult streaming problems with enterprise-grade durability? Ilya Ganelin presents Capital Ones novel solution for real-time decisioning on Apache Apex. Ilya shows how Apex provides unique capabilities that ensure less than 2 ms latency in an enterprise-grade solution on Hadoop.
Apache Flink is a full-featured streaming framework with high throughput, millisecond latency, strong consistency, support for out-of-order streams, and support for batch as a special case of streaming. Kostas Tzoumas gives an overview of Flink and its streaming-first philosophy, as well as the project roadmap and vision: fully unifying the worlds of batch and streaming analytics.
The Zeta Architecture is an enterprise architecture to move beyond the data lake. The most logical way to scale applications across tiers is to put a messaging platform in between the tiers, which allows a far simpler ability to scale the communications of applications. Jim Scott covers the benefits of this model and offers an example of data-center monitoring.
So many of the data projects making headlinesfrom a new app for finding public services to a new probabilistic model for predicting weather patterns for subsistence farmersare great accomplishments but dont seem to have end users in mind. Discover how organizations are designing with, not for, people, accounting for what drives them in order to make long-lasting impact.
So many of the data projects making headlinesfrom a new app for finding public services to a new probabilistic model for predicting weather patterns for subsistence farmersare great accomplishments but dont seem to have end users in mind. Discover how organizations are designing with, not for, people, accounting for what drives them in order to make long-lasting impact.
Machines are not objective, and big data is not fair. Michael Williams uses sentiment analysis to show that supervised machine learning has the potential to amplify the voices of the most privileged people in society, violate the spirit and letter of civil rights law, and make your product suck.
2015 saw an increased urgency in the ethics of big data, as the UN began to adopt civil-society partnerships with big data organizations. But what, if anything, are we supposed to do with the data we acquire, interpret, and label big data? Louis Suarez-Potts examines big data ethics to explain best practices for putting to use the information gained by big data methodology.
Jonathan King outlines ethical best practices for big data and explores the difficult questions emerging from missteps that have caused public outcry, as well as the legal, ethical, and regulatory frameworks that are just beginning to take shape around big data.
In the current explosion of the Internet of Things, big data, and mobile, compliance often takes a back seat. But the failure to address legal privacy and consumer-protection considerations has landed many in hot water, resulting in potential legal settlements and business failures. Alysa Hutnik and Kristi Wolff discuss flash points and proactive strategies to avoid becoming a target.
Kathleen Ting, Vikram Srivastava, Darren Lo, and Jordan Hambleton, the instructors of the full-day tutorial Apache Hadoop Operations for Production Systems, field a wide range of detailed questions. Even if you dont have a specific question, join in to hear what others are asking.
Mark Grover, Jonathan Seidman, Ted Malaska, and Gwen Shapira, the authors of <i>Hadoop Application Architectures</i>, participate in an open Q&A session on considerations and recommendations for the architecture and design of applications using Hadoop. Come with questions about your use case and its big data architecture or just listen in on the conversation.
Join the Spark team for an informal Q&A session. Apache Spark architects Reynold Xin, Tathagata Das, and Michael Armbrust will be on hand to field a wide range of detailed questions. Even if you dont have a specific question, join in to hear what others are asking.
Joseph Adler, Ewen Cheslack-Postava, Jun Rao, Jesse Anderson, and Neha Narkhede, the instructors of the Apache Kafka tutorials, field a wide range of detailed questions. Even if you dont have a specific question, join in to hear what others are asking.
John Akred, Scott Kurth, and Colette Glaeser field a wide range of detailed questions about developing a modern enterprise data strategy. Even if you dont have a specific question, join in to hear what others are asking.
So many of the data projects making headlinesfrom a new app for finding public services to a new probabilistic model for predicting weather patterns for subsistence farmersare great accomplishments but dont seem to have end users in mind. Discover how organizations are designing with, not for, people, accounting for what drives them in order to make long-lasting impact.
So many of the data projects making headlinesfrom a new app for finding public services to a new probabilistic model for predicting weather patterns for subsistence farmersare great accomplishments but dont seem to have end users in mind. Discover how organizations are designing with, not for, people, accounting for what drives them in order to make long-lasting impact.
Machines are not objective, and big data is not fair. Michael Williams uses sentiment analysis to show that supervised machine learning has the potential to amplify the voices of the most privileged people in society, violate the spirit and letter of civil rights law, and make your product suck.
2015 saw an increased urgency in the ethics of big data, as the UN began to adopt civil-society partnerships with big data organizations. But what, if anything, are we supposed to do with the data we acquire, interpret, and label big data? Louis Suarez-Potts examines big data ethics to explain best practices for putting to use the information gained by big data methodology.
Jonathan King outlines ethical best practices for big data and explores the difficult questions emerging from missteps that have caused public outcry, as well as the legal, ethical, and regulatory frameworks that are just beginning to take shape around big data.
In the current explosion of the Internet of Things, big data, and mobile, compliance often takes a back seat. But the failure to address legal privacy and consumer-protection considerations has landed many in hot water, resulting in potential legal settlements and business failures. Alysa Hutnik and Kristi Wolff discuss flash points and proactive strategies to avoid becoming a target.
Kathleen Ting, Vikram Srivastava, Darren Lo, and Jordan Hambleton, the instructors of the full-day tutorial Apache Hadoop Operations for Production Systems, field a wide range of detailed questions. Even if you dont have a specific question, join in to hear what others are asking.
Mark Grover, Jonathan Seidman, Ted Malaska, and Gwen Shapira, the authors of <i>Hadoop Application Architectures</i>, participate in an open Q&A session on considerations and recommendations for the architecture and design of applications using Hadoop. Come with questions about your use case and its big data architecture or just listen in on the conversation.
Join the Spark team for an informal Q&A session. Apache Spark architects Reynold Xin, Tathagata Das, and Michael Armbrust will be on hand to field a wide range of detailed questions. Even if you dont have a specific question, join in to hear what others are asking.
Joseph Adler, Ewen Cheslack-Postava, Jun Rao, Jesse Anderson, and Neha Narkhede, the instructors of the Apache Kafka tutorials, field a wide range of detailed questions. Even if you dont have a specific question, join in to hear what others are asking.
John Akred, Scott Kurth, and Colette Glaeser field a wide range of detailed questions about developing a modern enterprise data strategy. Even if you dont have a specific question, join in to hear what others are asking.
Whether you want to extend your on-prem data lake with workflows that leverage the benefits of the clouds elastic scale, or you have sensitive data that you need to anonymize and aggregate on-prem before sending to the cloud, you need a hybrid data-integration solution for Hadoop. Hiren Shah and Anand Subbaraj show how to build hybrid data flows with Microsoft HDInsight and Azure Data Factory.
Baidu runs Alluxio in production with hundreds of nodes managing petabytes of data. Bin Fan and Haojun Wang demonstrate how Alluxio improves big data analytics (ad hoc query)Baidu experienced a 30x performance improvementand explain how Baidu leverages Alluxio in its machine-learning architecture and how it uses Alluxio to manage heterogeneous storage resources.
Thomas Phelan and Joel Baxter investigate the advantages and disadvantages of running specific Hadoop workloads in different infrastructure environments. Thomas and Joel then provide a set of rules to help users evaluate big data runtime environments and deployment options to determine which is best suited for a given application.
Phillip Radley explores how to use an accumulation of marginal gains approach to achieve success with an Apache Hadoop-based enterprise data hub (EDH), drawing on a set of design patterns built up over five years establishing BTs EDH.
John Omernik walks attendees through Operation Ababil's 2013 DDoS attacks to understand how banks were able to implement controls to protect their networks. Using subject-matter experts, Hadoop, and low-friction access to data, members of the US banking industry were able to come up with new models to protect their networks from distributed denial of service attacks.
In a panel discussion, Cloudera's Steve Totman talks about the practicalities and realities of big data-based customer 360 with big data experts Lori Bieda, Nick Curcuru, and Robert Bagley. Attend if you have challenges implementing big data-based customer 360 or just want to learn from the panel's real-world experiences.
Many third-party apps are built on top of the Hadoop platform for data ingest, ETL, analytics, and predictive modeling. These services/apps need a data-governance layer for security and compliance, but it is often burdensome for each individual app to build its own. Chang She describes the challenges in building an extensible metadata layer that serves common governance needs for Hadoop.
Telcos are graduating from exploring Hadoops technical capabilities to implementing full-blown, multiworkload data hubs at the heart of their operations. The worlds leading telcos are delivering compelling results for strategic use cases that leverage big data solutions. Amy O'Connor explores three key case studies that showcase these successes.
You have your Hadoop cluster, and you are ready to fill it up with data. But wait! Which format should you use to store your data? Should you store it in plain text, SequenceFile, Avro, or Parquet? (And should you compress it?) Silvia Oliveros and Stephen O'Sullivan cover the hows, whys, and whens of choosing one format over another and take a closer look at some of the tradeoffs each offers.
Often without realizing it, companies spend significant resources engineering new databases. The need to combine traditional relational datasets with new operational and historical data leads to sharded RDBMS or hybridized RDBMS and NoSQL systems, typically leaving few of the constituent database guarantees intact. Spencer Kimball introduces CockroachDB, an open source, scale-out SQL database.
While organizations understand the importance of customer satisfaction, quantifying its impact on future engagement is a surprisingly hard analytical problem (most rely on Net Promoter Scores). Krishnan Venkata and Jose Abelenda explain how Hotwire used big data to put a dollar figure on promoter/detractor behavior to help the organization objectively prioritize customer-engagement initiatives.
Whether you want to extend your on-prem data lake with workflows that leverage the benefits of the clouds elastic scale, or you have sensitive data that you need to anonymize and aggregate on-prem before sending to the cloud, you need a hybrid data-integration solution for Hadoop. Hiren Shah and Anand Subbaraj show how to build hybrid data flows with Microsoft HDInsight and Azure Data Factory.
Baidu runs Alluxio in production with hundreds of nodes managing petabytes of data. Bin Fan and Haojun Wang demonstrate how Alluxio improves big data analytics (ad hoc query)Baidu experienced a 30x performance improvementand explain how Baidu leverages Alluxio in its machine-learning architecture and how it uses Alluxio to manage heterogeneous storage resources.
Thomas Phelan and Joel Baxter investigate the advantages and disadvantages of running specific Hadoop workloads in different infrastructure environments. Thomas and Joel then provide a set of rules to help users evaluate big data runtime environments and deployment options to determine which is best suited for a given application.
Phillip Radley explores how to use an accumulation of marginal gains approach to achieve success with an Apache Hadoop-based enterprise data hub (EDH), drawing on a set of design patterns built up over five years establishing BTs EDH.
John Omernik walks attendees through Operation Ababil's 2013 DDoS attacks to understand how banks were able to implement controls to protect their networks. Using subject-matter experts, Hadoop, and low-friction access to data, members of the US banking industry were able to come up with new models to protect their networks from distributed denial of service attacks.
In a panel discussion, Cloudera's Steve Totman talks about the practicalities and realities of big data-based customer 360 with big data experts Lori Bieda, Nick Curcuru, and Robert Bagley. Attend if you have challenges implementing big data-based customer 360 or just want to learn from the panel's real-world experiences.
Many third-party apps are built on top of the Hadoop platform for data ingest, ETL, analytics, and predictive modeling. These services/apps need a data-governance layer for security and compliance, but it is often burdensome for each individual app to build its own. Chang She describes the challenges in building an extensible metadata layer that serves common governance needs for Hadoop.
Telcos are graduating from exploring Hadoops technical capabilities to implementing full-blown, multiworkload data hubs at the heart of their operations. The worlds leading telcos are delivering compelling results for strategic use cases that leverage big data solutions. Amy O'Connor explores three key case studies that showcase these successes.
You have your Hadoop cluster, and you are ready to fill it up with data. But wait! Which format should you use to store your data? Should you store it in plain text, SequenceFile, Avro, or Parquet? (And should you compress it?) Silvia Oliveros and Stephen O'Sullivan cover the hows, whys, and whens of choosing one format over another and take a closer look at some of the tradeoffs each offers.
Often without realizing it, companies spend significant resources engineering new databases. The need to combine traditional relational datasets with new operational and historical data leads to sharded RDBMS or hybridized RDBMS and NoSQL systems, typically leaving few of the constituent database guarantees intact. Spencer Kimball introduces CockroachDB, an open source, scale-out SQL database.
While organizations understand the importance of customer satisfaction, quantifying its impact on future engagement is a surprisingly hard analytical problem (most rely on Net Promoter Scores). Krishnan Venkata and Jose Abelenda explain how Hotwire used big data to put a dollar figure on promoter/detractor behavior to help the organization objectively prioritize customer-engagement initiatives.
Ben Lorica hosts a conversation with Doug Cutting and Mike Cafarella, the cofounders of Apache Hadoop.
Jennifer Wu and James Malone offer an insider look at how Google has integrated Hadoop components like HDFS, Impala, and Apache Spark with Google Cloud Platform technologies like Google Compute Engine (GCE), Bigtable, BigQuery, and Cloud Storage. Jennifer and James also explore the importance of Googles growing collaboration with open source communities.
Todd Lipcon explores the tradeoffs between real-time transactional access and fast analytic performance from the perspective of storage-engine internals. Todd also outlines Kudu, the new addition to the open source Hadoop ecosystem that complements HDFS and HBase to provide a new option for achieving fast scans and fast random access from a single API.
Hadoops traditional batch technologies are quickly being supplanted by in-memory columnar execution to drive faster data-to-value. Wes McKinney and Jacques Nadeau provide an overview of in-memory columnar execution, survey key related technologies, including Kudu, Ibis, Impala, and Drill, and cover a sample use case using Ibis in conjunction with Apache Drill to deliver real-time conclusions.
SQL is normally a very static language that assumes a fixed and well-known schema. Apache Drill breaks these assumptions by restructuring the execution of queries so optimizations and type resolution can be done just in time. This has profound consequences for how applicable SQL is in the big data world. Ted Dunning walks attendees through Drill and explores its implications for big data.
Most already know HBase, but many don't know that it can be coupled with other tools from the ecosystem to increase efficiency. Jean-Marc Spaggiari and Kevin O'Dell walk attendees through some real-life HBase use cases and demonstrate how they have been efficiently implemented.
Abin Shahab walks attendees through Altiscale's Docker deployment strategy, describes the design decisions behind it, and discusses the issues encountered and fixed along the way.
Building a real-time monitoring service that handles millions of custom events per second while satisfying complex rules, varied throughput requirements, and numerous dimensions simultaneously is a complex endeavor. Sumeet Singh and Mridul Jain explain how Yahoo approached these challenges with Apache Storm Trident, Kafka, HBase, and OpenTSDB and discuss the lessons learned along the way.
Heron, Twitter's streaming system, has been in production nearly two years and is widely used by several teams for diverse use cases. Karthik Ramasamy discusses Twitter's operating experiences and shares the challenges of running Heron at scale as well as the approaches that Twitter took to solve them.
Oil and gas organizations are at the forefront of big data, adopting technologies such as Hadoop and Spark to develop next-generation fusion systems. Brian Clark and Marco Ippolito introduce a case study from CGG, a builder of common data models to drive analytics of sensor data and associated metadata from fast-changing big data streams, to show how to derive richer value from big data assets.
Traditional data-warehousing techniques are sometimes limited by the scalability of the implementation tools themselves. Arun Thangamani explains how the advanced architectural approaches by tools like Apache Phoenix and HBase allow new, highly scalable live-analytics solutions using the same traditional techniques and showcases a successful implementation at CDK.
Ben Lorica hosts a conversation with Doug Cutting and Mike Cafarella, the cofounders of Apache Hadoop.
Jennifer Wu and James Malone offer an insider look at how Google has integrated Hadoop components like HDFS, Impala, and Apache Spark with Google Cloud Platform technologies like Google Compute Engine (GCE), Bigtable, BigQuery, and Cloud Storage. Jennifer and James also explore the importance of Googles growing collaboration with open source communities.
Todd Lipcon explores the tradeoffs between real-time transactional access and fast analytic performance from the perspective of storage-engine internals. Todd also outlines Kudu, the new addition to the open source Hadoop ecosystem that complements HDFS and HBase to provide a new option for achieving fast scans and fast random access from a single API.
Hadoops traditional batch technologies are quickly being supplanted by in-memory columnar execution to drive faster data-to-value. Wes McKinney and Jacques Nadeau provide an overview of in-memory columnar execution, survey key related technologies, including Kudu, Ibis, Impala, and Drill, and cover a sample use case using Ibis in conjunction with Apache Drill to deliver real-time conclusions.
SQL is normally a very static language that assumes a fixed and well-known schema. Apache Drill breaks these assumptions by restructuring the execution of queries so optimizations and type resolution can be done just in time. This has profound consequences for how applicable SQL is in the big data world. Ted Dunning walks attendees through Drill and explores its implications for big data.
Most already know HBase, but many don't know that it can be coupled with other tools from the ecosystem to increase efficiency. Jean-Marc Spaggiari and Kevin O'Dell walk attendees through some real-life HBase use cases and demonstrate how they have been efficiently implemented.
Abin Shahab walks attendees through Altiscale's Docker deployment strategy, describes the design decisions behind it, and discusses the issues encountered and fixed along the way.
Building a real-time monitoring service that handles millions of custom events per second while satisfying complex rules, varied throughput requirements, and numerous dimensions simultaneously is a complex endeavor. Sumeet Singh and Mridul Jain explain how Yahoo approached these challenges with Apache Storm Trident, Kafka, HBase, and OpenTSDB and discuss the lessons learned along the way.
Heron, Twitter's streaming system, has been in production nearly two years and is widely used by several teams for diverse use cases. Karthik Ramasamy discusses Twitter's operating experiences and shares the challenges of running Heron at scale as well as the approaches that Twitter took to solve them.
Oil and gas organizations are at the forefront of big data, adopting technologies such as Hadoop and Spark to develop next-generation fusion systems. Brian Clark and Marco Ippolito introduce a case study from CGG, a builder of common data models to drive analytics of sensor data and associated metadata from fast-changing big data streams, to show how to derive richer value from big data assets.
Traditional data-warehousing techniques are sometimes limited by the scalability of the implementation tools themselves. Arun Thangamani explains how the advanced architectural approaches by tools like Apache Phoenix and HBase allow new, highly scalable live-analytics solutions using the same traditional techniques and showcases a successful implementation at CDK.
Data 101 introduces you to core principles of data architecture, teaches you how to build and manage successful data teams, and inspires you to do more with your data through real-world applications. Setting the foundation for deeper dives on the following days of Strata + Hadoop World, Data 101 reinforces data fundamentals and helps you focus on how data can solve your business problems.
Mubashir Kazia, Ben Spivey, Sravya Tirukkovalur, and Michael Yoder guide participants through the process of securing a Hadoop cluster. Participants will start with a Hadoop cluster with no security and then add security features related to authentication, authorization, encryption of data at rest, encryption of data in transit, and complete data governance.
Yvonne Quacken and Allen Hoem explore the business and technical challenges that Siemens faced capturing continuous data from millions of sensors across different areas and explain how Teradata Listener helped Siemens simplify this data-capture process with a single, central service to ingest multiple real-time data streams simultaneously in a reliable fashion.
Machine learning is a hot topic. Recommenders, sentiment analysis, churn and click-through prediction, image recognition, and fraud detection are at the core of intelligent applications. However, developing these models is laborious. Carlos Guestrin shares a new approach to leverage massive amounts of data and applied machine learning at scale to create intelligent applications.
Amit Walia, chief product officer of Informatica, hosts a discussion with industry experts on how big data management can enable organizations to deliver faster, more flexible, and more repeatable big data projects while ensuring security and governance. Learn how organizations are using big data management to be more successful with their big data initiatives.
Did you know Apache Spark is helping transform industries, companies, and your everyday life? David Taieb and Mythili Venkatakrishnan demonstrate two use cases of how Apache Spark is being used to harness valuable insights from complex data across cloud and hybrid environments.
High-velocity, high-volume, and high-variety data streams challenge analytics organizations because the ability to get critical insights often decays rapidly. Pat McGarry explains how organizations that embrace heterogeneous computing techniques can overcome hurdles to real-time insights, thereby gaining significant competitive advantages.
Kazunori Sato and Amy Unruh explore how you can use TensorFlow to drive large-scale distributed machine learning against your analytic data sitting in Google BigQuery, with data preprocessing driven by Dataflow (now Apache Beam). Kazunori and Amy dive into practical examples of how these technologies can work together to enable a powerful workflow for distributed machine learning.
Join the SAP team for a demonstration of how OLAP on Hadoop and real-time query federation help unify enterprise and big data, using SAP's new big data solution, SAP HANA Vora. Amit Satoor and Balalji Krishna explore real-world use cases where instant insights from a combination of operational and Hadoop data impact core business operations
An interactive panel, hosted by Dell's Armando Acosta, explores how business units have taken advantage of Hadoop's strengths to quickly identify and implement solutions that deal with massive amounts of data to deliver valuable results across the business.
In the race to pair streaming systems with stateful systems, the winners will be stateful systems that process streams natively. These systems remove the burden on application developers to be distributed systems experts and enable new applications to be both powerful and robust. John Hugg describes whats possible when integrated systems apply a transactional approach to event processing.
Containers have taken the world by storm by radically transforming the way applications are built and deployed. But many fail to appreciate how powerful containers can be for performance-sensitive data applications. Partha Seetala explains how containers can help you "virtualize" your mission-critical enterprise applications, simplify application life cycles, and increase data-center efficiency.
Data 101 introduces you to core principles of data architecture, teaches you how to build and manage successful data teams, and inspires you to do more with your data through real-world applications. Setting the foundation for deeper dives on the following days of Strata + Hadoop World, Data 101 reinforces data fundamentals and helps you focus on how data can solve your business problems.
Mubashir Kazia, Ben Spivey, Sravya Tirukkovalur, and Michael Yoder guide participants through the process of securing a Hadoop cluster. Participants will start with a Hadoop cluster with no security and then add security features related to authentication, authorization, encryption of data at rest, encryption of data in transit, and complete data governance.
Yvonne Quacken and Allen Hoem explore the business and technical challenges that Siemens faced capturing continuous data from millions of sensors across different areas and explain how Teradata Listener helped Siemens simplify this data-capture process with a single, central service to ingest multiple real-time data streams simultaneously in a reliable fashion.
Machine learning is a hot topic. Recommenders, sentiment analysis, churn and click-through prediction, image recognition, and fraud detection are at the core of intelligent applications. However, developing these models is laborious. Carlos Guestrin shares a new approach to leverage massive amounts of data and applied machine learning at scale to create intelligent applications.
Amit Walia, chief product officer of Informatica, hosts a discussion with industry experts on how big data management can enable organizations to deliver faster, more flexible, and more repeatable big data projects while ensuring security and governance. Learn how organizations are using big data management to be more successful with their big data initiatives.
Did you know Apache Spark is helping transform industries, companies, and your everyday life? David Taieb and Mythili Venkatakrishnan demonstrate two use cases of how Apache Spark is being used to harness valuable insights from complex data across cloud and hybrid environments.
High-velocity, high-volume, and high-variety data streams challenge analytics organizations because the ability to get critical insights often decays rapidly. Pat McGarry explains how organizations that embrace heterogeneous computing techniques can overcome hurdles to real-time insights, thereby gaining significant competitive advantages.
Kazunori Sato and Amy Unruh explore how you can use TensorFlow to drive large-scale distributed machine learning against your analytic data sitting in Google BigQuery, with data preprocessing driven by Dataflow (now Apache Beam). Kazunori and Amy dive into practical examples of how these technologies can work together to enable a powerful workflow for distributed machine learning.
Join the SAP team for a demonstration of how OLAP on Hadoop and real-time query federation help unify enterprise and big data, using SAP's new big data solution, SAP HANA Vora. Amit Satoor and Balalji Krishna explore real-world use cases where instant insights from a combination of operational and Hadoop data impact core business operations
An interactive panel, hosted by Dell's Armando Acosta, explores how business units have taken advantage of Hadoop's strengths to quickly identify and implement solutions that deal with massive amounts of data to deliver valuable results across the business.
In the race to pair streaming systems with stateful systems, the winners will be stateful systems that process streams natively. These systems remove the burden on application developers to be distributed systems experts and enable new applications to be both powerful and robust. John Hugg describes whats possible when integrated systems apply a transactional approach to event processing.
Containers have taken the world by storm by radically transforming the way applications are built and deployed. But many fail to appreciate how powerful containers can be for performance-sensitive data applications. Partha Seetala explains how containers can help you "virtualize" your mission-critical enterprise applications, simplify application life cycles, and increase data-center efficiency.
Ewen Cheslack-Postava, Joseph Adler, Jesse Anderson, and Ian Wrigley show how to use Apache Kafka to collect, manage, and process stream data for big data projects and general purpose enterprise data-integration needs alike. Once your data is captured in real time and available as real-time subscriptions, you can start to compute new datasets in real-time from these original feeds.
Patrick McFadin gives a comprehensive overview of the powerful Team Apache: Apache Kafka, Spark, and Cassandra. Patrick demonstrates data models, covers deployment considerations, and explains code for different requirements.
Hadoop can bring great value to businesses but also big headaches. Some solutions that provide SQL access to Hadoop data mean changing your business processes to overcome limitations in the technologies. Emma McGrattan explains how users can unlock tremendous business value through SQL-driven Hadoop solutions. Emma outlines what should be on your checklist and the pitfalls to avoid.
Learn how TD Bank is creating the bank of the future through IT 3.0. Central to this is business agility, fueled by secure, self-service access to enterprise and market data. Mok Choe and Paul Barth detail the fundamentals for success in this transformation, which started with rapid consolidation of hundreds of data sources onto a Hadoop enterprise data provisioning platform.
Inmar handles 3.7 billion transactions annually. Kevin Goode explains Inmar's transformation, starting in 2012, from a business-services company to a data-driven enterprise using Hadoop.
Solr has been adopted by all major Hadoop platform vendors as the de facto standard for big data search. Timothy Potter introduces an open source project that exposes Solr as a SparkSQL datasource. Timothy offers common use cases, access to open source code, and performance metrics to help you develop your own large-scale search and discovery solution.
Joseph Goldberg discusses the attributes required of a batch management platform that can accelerate development by enabling programmers to generate workflows as code, support continuous deployment with rich APIs and lightweight workflow-scheduling infrastructure, and optimize production with comprehensive enterprise operational capabilities like SLA management and full log and output management.
Jagane Sundar discusses the unique challenges of hybrid big data deployments and outlines strategies to address them.
Join Hortonworks to discuss transformational use cases from Hortonworks customers that manage data in motion and data at rest. Hortonworks's Wei Wang and Scott Gnau explore the modern data applications being built and deployed in 2016 that are driving new frontiers in information technology.
When building your data stack, the architecture could be your biggest challenge. Yet it could also be the best predictor for success. With so many elements to consider and no proven playbook, where do you begin to assemble best practices for a scalable data architecture? Ben Sharma offers lessons learned from the field to get you started.
Wei Zheng, Mohan Sadashiva, and Mark Donsky explain how data-wrangling tools not only enable users to work with a variety of new or complex sources of data in Hadoop but also ensure that the data lineage and metadata created through the process are appropriately catalogued and made available to others in the organization.
Sudipto Dasgupta and Ganesan Pandurangan offer a case study of a large multinational imaging and electronics company that migrated accounts receivable reports to the Hadoop-based open source Infosys Information Platform, which implemented dynamic age bucketing capabilities and reduced the number of end-user views from over 400 to 50.
Ewen Cheslack-Postava, Joseph Adler, Jesse Anderson, and Ian Wrigley show how to use Apache Kafka to collect, manage, and process stream data for big data projects and general purpose enterprise data-integration needs alike. Once your data is captured in real time and available as real-time subscriptions, you can start to compute new datasets in real-time from these original feeds.
Patrick McFadin gives a comprehensive overview of the powerful Team Apache: Apache Kafka, Spark, and Cassandra. Patrick demonstrates data models, covers deployment considerations, and explains code for different requirements.
Hadoop can bring great value to businesses but also big headaches. Some solutions that provide SQL access to Hadoop data mean changing your business processes to overcome limitations in the technologies. Emma McGrattan explains how users can unlock tremendous business value through SQL-driven Hadoop solutions. Emma outlines what should be on your checklist and the pitfalls to avoid.
Learn how TD Bank is creating the bank of the future through IT 3.0. Central to this is business agility, fueled by secure, self-service access to enterprise and market data. Mok Choe and Paul Barth detail the fundamentals for success in this transformation, which started with rapid consolidation of hundreds of data sources onto a Hadoop enterprise data provisioning platform.
Inmar handles 3.7 billion transactions annually. Kevin Goode explains Inmar's transformation, starting in 2012, from a business-services company to a data-driven enterprise using Hadoop.
Solr has been adopted by all major Hadoop platform vendors as the de facto standard for big data search. Timothy Potter introduces an open source project that exposes Solr as a SparkSQL datasource. Timothy offers common use cases, access to open source code, and performance metrics to help you develop your own large-scale search and discovery solution.
Joseph Goldberg discusses the attributes required of a batch management platform that can accelerate development by enabling programmers to generate workflows as code, support continuous deployment with rich APIs and lightweight workflow-scheduling infrastructure, and optimize production with comprehensive enterprise operational capabilities like SLA management and full log and output management.
Jagane Sundar discusses the unique challenges of hybrid big data deployments and outlines strategies to address them.
Join Hortonworks to discuss transformational use cases from Hortonworks customers that manage data in motion and data at rest. Hortonworks's Wei Wang and Scott Gnau explore the modern data applications being built and deployed in 2016 that are driving new frontiers in information technology.
When building your data stack, the architecture could be your biggest challenge. Yet it could also be the best predictor for success. With so many elements to consider and no proven playbook, where do you begin to assemble best practices for a scalable data architecture? Ben Sharma offers lessons learned from the field to get you started.
Wei Zheng, Mohan Sadashiva, and Mark Donsky explain how data-wrangling tools not only enable users to work with a variety of new or complex sources of data in Hadoop but also ensure that the data lineage and metadata created through the process are appropriately catalogued and made available to others in the organization.
Sudipto Dasgupta and Ganesan Pandurangan offer a case study of a large multinational imaging and electronics company that migrated accounts receivable reports to the Hadoop-based open source Infosys Information Platform, which implemented dynamic age bucketing capabilities and reduced the number of end-user views from over 400 to 50.
Hadoop is famously scalable, as is cloud computing. R, the thriving and extensible open source data science software...not so much. Mario Inchiosa and Roni Burd outline how to seamlessly combine Hadoop, cloud computing, and R to create a scalable data science platform that lets you explore, transform, model, and score data at any scale from the comfort of your favorite R environment.
The Internet of Things (IoT) continues to provide value and hold promise for both the consumer and enterprise alike. To succeed, an IoT project must concern itself with how to ingest data, build actionable models, and react in real time. Chris Rawles describes approaches to addressing these concerns through a deep dive into an interactive demo centered around classification of human activities.
Bob Hansen outlines the latest innovations from HPE for SQL on Hadoop.
Sandy Steier and Dennis Gleeson explain how the promise of easy data sharing and collaborative analysison petabyte-scale datacan fundamentally change business culture in the same way that the Internet has changed our consumer culture.
Software-defined networking (SDN) and network functions virtualization (NFV) hold tremendous potential to enable efficiency and flexibility in service delivery, but SDN/NFV environments are also highly complex and multilayered. Matt Olson explains why effective support for SDN/NFV services requires leveraging the tremendous amount of service and data streaming from the platform.
Join Bob Rogers, Intels chief data scientist for big data solutions, and special guests to see how Intels open source Trusted Analytics Platform has accelerated and simplified the development of powerful analytics that are changing the game.
Jeff Pohlmann explores the skills, challenges, and solutions necessary to turn big data into big results. Learn more effective ways to increase productivity and decrease costs, aid in the allocation of key personnel and resources, better determine the true sentiment of customers, determine the impact of changing processes on production, and help solve a host of other needs.
Applying big data to an internal business use case is challenging and requires expertise and focus. Even harder is scaling it out across a global enterprise. Don Perigo explains how GE Power Services has been able to deliver results in an uncertain world by leveraging big data and scaling its platform across a global employee base that spans over 25 countries.
In a conversation moderated by Nenshad Bardoliwalla, analytic leaders Conrad Mulcahy, Travis Ringger, and Dave Wells share real-world data-preparation challenges and discuss new technologies, including Spark-powered machine learning, latent semantic indexing, statistical pattern recognition, and text analytics techniques, that accelerate the ability to transform data into usable information.
A major challenge in todays world of big data is getting data into the data lake in a simple, automated way. Coding scripts for disparate sources is time consuming and difficult to manage. Developers need a process that supports disparate sources by detecting and passing metadata automatically. Chuck Yarbrough and Mark Burnette explain how to simplify and automate your data ingestion process.
Hadoop is famously scalable, as is cloud computing. R, the thriving and extensible open source data science software...not so much. Mario Inchiosa and Roni Burd outline how to seamlessly combine Hadoop, cloud computing, and R to create a scalable data science platform that lets you explore, transform, model, and score data at any scale from the comfort of your favorite R environment.
The Internet of Things (IoT) continues to provide value and hold promise for both the consumer and enterprise alike. To succeed, an IoT project must concern itself with how to ingest data, build actionable models, and react in real time. Chris Rawles describes approaches to addressing these concerns through a deep dive into an interactive demo centered around classification of human activities.
Bob Hansen outlines the latest innovations from HPE for SQL on Hadoop.
Sandy Steier and Dennis Gleeson explain how the promise of easy data sharing and collaborative analysison petabyte-scale datacan fundamentally change business culture in the same way that the Internet has changed our consumer culture.
Software-defined networking (SDN) and network functions virtualization (NFV) hold tremendous potential to enable efficiency and flexibility in service delivery, but SDN/NFV environments are also highly complex and multilayered. Matt Olson explains why effective support for SDN/NFV services requires leveraging the tremendous amount of service and data streaming from the platform.
Join Bob Rogers, Intels chief data scientist for big data solutions, and special guests to see how Intels open source Trusted Analytics Platform has accelerated and simplified the development of powerful analytics that are changing the game.
Jeff Pohlmann explores the skills, challenges, and solutions necessary to turn big data into big results. Learn more effective ways to increase productivity and decrease costs, aid in the allocation of key personnel and resources, better determine the true sentiment of customers, determine the impact of changing processes on production, and help solve a host of other needs.
Applying big data to an internal business use case is challenging and requires expertise and focus. Even harder is scaling it out across a global enterprise. Don Perigo explains how GE Power Services has been able to deliver results in an uncertain world by leveraging big data and scaling its platform across a global employee base that spans over 25 countries.
In a conversation moderated by Nenshad Bardoliwalla, analytic leaders Conrad Mulcahy, Travis Ringger, and Dave Wells share real-world data-preparation challenges and discuss new technologies, including Spark-powered machine learning, latent semantic indexing, statistical pattern recognition, and text analytics techniques, that accelerate the ability to transform data into usable information.
A major challenge in todays world of big data is getting data into the data lake in a simple, automated way. Coding scripts for disparate sources is time consuming and difficult to manage. Developers need a process that supports disparate sources by detecting and passing metadata automatically. Chuck Yarbrough and Mark Burnette explain how to simplify and automate your data ingestion process.
This hands-on tutorial provides a quick start to building intelligent business applications using machine learning. Learn about machine-learning basics, feature engineering, anomaly detection, recommender systems, and deep learning as you are guided through all the steps of prototyping and production: data cleaning, feature engineering, model building and evaluation, and deployment.
Data scientists have career-making opportunities to use more diverse datasets to deliver bigger business returns. Nidhi Aggarwal demonstrates how Tamr, a machine-driven, human-guided approach to finding, integrating, and preparing data, enables new levels of insight into corporate spend over previous analytics toolsin one case identifying new savings opportunities worth more than $100M.
Although its been around for decades, machine learning is currently thriving, and organizations are looking to benefit from it. Patrick Hall and Paul Kent offer 10 crucial tips to know before venturing into the mixa personal survival guide from the creators of a solution that was there in the beginning and continues to drive the industry today.
Analyzing real-time streams of data is becoming increasingly important to remain competitive. Siva Raghupathy and Manjeet Chayel guide attendees through some of the proven architectures for processing streaming data using a combination of cloud and open source tools such as Apache Spark. Watch a live demo and learn how you can easily scale your applications with Amazon Web Services.
Effective and efficient model selection and tuning is crucial for building machine-learning systems, but large-scale machine-learning problems require us to rethink the model-selection and tuning process. Peter Prettenhofer and Owen Zhang outline the tradeoffs we need to make and demonstrate how to efficiently search and tune complex machine-learning pipelines in MLlib.
To win in the on-demand economy, businesses must embrace real-time analytics. Eric Frenkiel demos an enterprise approach to data solutions for predictive analytics. Eric is joined by JR Cahill, who outlines Kellogg's approach to advanced analytics with MemSQL, including moving from overnight to intraday analytics and integrating directly with business intelligence tools like Tableau.
Many companies have created extremely powerful Hadoop use cases with highly valuable outcomes. The diverse adoption and application of Hadoop is producing an extremely robust ecosystem. However, teams often create silos around their Hadoop, forgetting some of the hard-learned lessons IT has gained over the years. Keith Manthey discusses one such often overlooked featuregovernance.
Martin Yip and Justin Murray explore the benefits of virtualization of Hadoop on vSphere and delve into three different examples of real-world deploymentsat small, medium, and large scalesto demonstrate how enterprises are currently deploying Hadoop differently on virtual machines.
Celtra provides aplatform for customers like Porsche and Fox to create, track, and analyze digital display advertising. Celtra's platform processes billions of ad events daily to give analysts fast and easy access to reports and ad hoc analytics. Grega Kepret outlines Celtras data-pipeline challenges and explains how it solved them by combining Snowflake's cloud data warehouse with Spark.
The Defense Advanced Research Projects Agency (DARPA) is synonymous with transformational change, developing the seeming impossible into the practical. Matthew van Adelsberg demonstrates how collaborative teams of SMEs, data scientists, and engineers have been organized to achieve DARPA hard results for nearly a decade and offers insights into how companies can do the same.
In order to remain competitive, you need to be able to respond to changing conditions in the moment. New stream-based technologies allow you to build applications that incorporate low-latency processing so you can stream data immediately or whenever youre ready. Steve Wooledge explores how new streaming technologies make this approach work and how they can be applied in many industries.
This hands-on tutorial provides a quick start to building intelligent business applications using machine learning. Learn about machine-learning basics, feature engineering, anomaly detection, recommender systems, and deep learning as you are guided through all the steps of prototyping and production: data cleaning, feature engineering, model building and evaluation, and deployment.
Data scientists have career-making opportunities to use more diverse datasets to deliver bigger business returns. Nidhi Aggarwal demonstrates how Tamr, a machine-driven, human-guided approach to finding, integrating, and preparing data, enables new levels of insight into corporate spend over previous analytics toolsin one case identifying new savings opportunities worth more than $100M.
Although its been around for decades, machine learning is currently thriving, and organizations are looking to benefit from it. Patrick Hall and Paul Kent offer 10 crucial tips to know before venturing into the mixa personal survival guide from the creators of a solution that was there in the beginning and continues to drive the industry today.
Analyzing real-time streams of data is becoming increasingly important to remain competitive. Siva Raghupathy and Manjeet Chayel guide attendees through some of the proven architectures for processing streaming data using a combination of cloud and open source tools such as Apache Spark. Watch a live demo and learn how you can easily scale your applications with Amazon Web Services.
Effective and efficient model selection and tuning is crucial for building machine-learning systems, but large-scale machine-learning problems require us to rethink the model-selection and tuning process. Peter Prettenhofer and Owen Zhang outline the tradeoffs we need to make and demonstrate how to efficiently search and tune complex machine-learning pipelines in MLlib.
To win in the on-demand economy, businesses must embrace real-time analytics. Eric Frenkiel demos an enterprise approach to data solutions for predictive analytics. Eric is joined by JR Cahill, who outlines Kellogg's approach to advanced analytics with MemSQL, including moving from overnight to intraday analytics and integrating directly with business intelligence tools like Tableau.
Many companies have created extremely powerful Hadoop use cases with highly valuable outcomes. The diverse adoption and application of Hadoop is producing an extremely robust ecosystem. However, teams often create silos around their Hadoop, forgetting some of the hard-learned lessons IT has gained over the years. Keith Manthey discusses one such often overlooked featuregovernance.
Martin Yip and Justin Murray explore the benefits of virtualization of Hadoop on vSphere and delve into three different examples of real-world deploymentsat small, medium, and large scalesto demonstrate how enterprises are currently deploying Hadoop differently on virtual machines.
Celtra provides aplatform for customers like Porsche and Fox to create, track, and analyze digital display advertising. Celtra's platform processes billions of ad events daily to give analysts fast and easy access to reports and ad hoc analytics. Grega Kepret outlines Celtras data-pipeline challenges and explains how it solved them by combining Snowflake's cloud data warehouse with Spark.
The Defense Advanced Research Projects Agency (DARPA) is synonymous with transformational change, developing the seeming impossible into the practical. Matthew van Adelsberg demonstrates how collaborative teams of SMEs, data scientists, and engineers have been organized to achieve DARPA hard results for nearly a decade and offers insights into how companies can do the same.
In order to remain competitive, you need to be able to respond to changing conditions in the moment. New stream-based technologies allow you to build applications that incorporate low-latency processing so you can stream data immediately or whenever youre ready. Steve Wooledge explores how new streaming technologies make this approach work and how they can be applied in many industries.
Cloudera Search combines the open standard for search, Apache Solr, with the proven scalability of Apache Hadoop. Glynn Durham guides participants through a 2-day training in using these tools to ingest, transform, index, and query data at scale and also demonstrates how to build interactive dashboards for analytics and integrate the search engine with external applications.
Cloudera Search combines the open standard for search, Apache Solr, with the proven scalability of Apache Hadoop. Glynn Durham guides participants through a 2-day training in using these tools to ingest, transform, index, and query data at scale and also demonstrates how to build interactive dashboards for analytics and integrate the search engine with external applications.
Cloudera Search combines the open standard for search, Apache Solr, with the proven scalability of Apache Hadoop. Glynn Durham guides participants through a 2-day training in using these tools to ingest, transform, index, and query data at scale and also demonstrates how to build interactive dashboards for analytics and integrate the search engine with external applications.
Cloudera Search combines the open standard for search, Apache Solr, with the proven scalability of Apache Hadoop. Glynn Durham guides participants through a 2-day training in using these tools to ingest, transform, index, and query data at scale and also demonstrates how to build interactive dashboards for analytics and integrate the search engine with external applications.
OReilly Media and DataStax have partnered to create a 2-day developer certification course for Apache Cassandra. Get certified as a Cassandra developer at Strata + Hadoop World in San Jose and be recognized for your NoSQL expertise.
OReilly Media and DataStax have partnered to create a 2-day developer certification course for Apache Cassandra. Get certified as a Cassandra developer at Strata + Hadoop World in San Jose and be recognized for your NoSQL expertise.
OReilly Media and DataStax have partnered to create a 2-day developer certification course for Apache Cassandra. Get certified as a Cassandra developer at Strata + Hadoop World in San Jose and be recognized for your NoSQL expertise.
OReilly Media and DataStax have partnered to create a 2-day developer certification course for Apache Cassandra. Get certified as a Cassandra developer at Strata + Hadoop World in San Jose and be recognized for your NoSQL expertise.
The real power and value proposition of Apache Spark is in building a unified use case that combines ETL, batch analytics, real-time stream analysis, machine learning, graph processing, and visualizations. In this 2-day course, Brian Clapper employs hands-on exercises using various Wikipedia datasets to illustrate the variety of ideal programming paradigms Spark makes possible.
The real power and value proposition of Apache Spark is in building a unified use case that combines ETL, batch analytics, real-time stream analysis, machine learning, graph processing, and visualizations. In this 2-day course, Brian Clapper employs hands-on exercises using various Wikipedia datasets to illustrate the variety of ideal programming paradigms Spark makes possible.
The real power and value proposition of Apache Spark is in building a unified use case that combines ETL, batch analytics, real-time stream analysis, machine learning, graph processing, and visualizations. In this 2-day course, Brian Clapper employs hands-on exercises using various Wikipedia datasets to illustrate the variety of ideal programming paradigms Spark makes possible.
The real power and value proposition of Apache Spark is in building a unified use case that combines ETL, batch analytics, real-time stream analysis, machine learning, graph processing, and visualizations. In this 2-day course, Brian Clapper employs hands-on exercises using various Wikipedia datasets to illustrate the variety of ideal programming paradigms Spark makes possible.
Strata + Hadoop World program chairs Roger Magoulas, Doug Cutting, and Alistair Croll welcome you to the first day of keynotes.
Strata + Hadoop World program chairs Roger Magoulas, Doug Cutting, and Alistair Croll welcome you to the second day of keynotes.
2016 marks the 10th anniversary of Apache Hadoop. This birthday provides us an opportunity to celebrate, as well as the chance to reflect on how we got here and where we are going.
The next evolution in the on-demand economy is in predictive analytics fueled by live streams of datain effect knowing what customers want before they do. Eric Frenkiel explains how a real-time trinity of technologiesKafka, Spark, and MemSQLis enabling Uber and others to power their own revolutions with predictive apps and analytics.
Megan Price demonstrates how machine-learning methods help us determine what we know, and what we don't, about the ongoing conflict in Syria. Megan then explains why these methods can be crucial to better understand patterns of violence, enabling better policy decisions, resource allocation, and ultimately, accountability and justice.
Big data is not limited to reporting and analysis; increasingly, companies are differentiating themselves by acting on data in real time. But what does "real time" really mean? Jack Norris discusses the challenges of coordinating data flows, analysis, and integration at scale to truly impact business as it happens.
Pivotals Ian Andrews explores why delivering information in context is the key to competitive differentiation in the digital economy.
Alyosha Efros discusses using computer vision to understand big visual data.
The cybersecurity landscape is quickly changing, and Apache Hadoop is becoming the analytics and data management platform of choice for cybersecurity practitioners. Tom Reilly and Alan Ross explain why organizations are turning toward the open source ecosystem to break down traditional cybersecurity analytics and data constraints in order to detect a new breed of sophisticated attacks.
Joseph Sirosh offers a fascinating look into how brains connected with sensors to the cloud and machine learning could revolutionize a field of medicine.
Bob Rogers, Intel's chief data scientist for big data solutions, demonstrates the power of the question in analytics. Learn how different types of data, from cubes of structured data to live video streams from mobile systems, combine with analytical technology to inform the questions that can be answered.
Michael Franklin offers an overview of the Berkeley Data Analytics Stack, outlines the current directions it's taking, and settles once and for all how BDAS should be pronounced.
As the volume and variety of data continue to grow, organizations have the opportunity to transform their industries and professions, but companies are grappling with how to deliver innovation. Adam Kocoloski shares his experience around this market shift and challenges attendees to join his mission of contributing to the community and investing in the power of open source and the cloud.
Julia Galef explores why Bayesian thinking is so different from what we do by default and outlines the most important principles of thinking like a Bayesian.
As a technical founder at Siri, Sentient, and Viv Labs, Adam Cheyer has helped design and develop a number of intelligent systems solving real-world problems for hundreds of millions of users. Drawing on specific examples, Adam reveals some of the techniques he uses to maximize the impact of the AI technologies he employs.
US Department of Commerce Deputy Secretary of Commerce Bruce Andrews explores using commerce data to fuel innovation.
Paula Poundstone isnt just any comedian. After years of justly criticizing and questioning the purpose of the many studies used for questions on NPRs #1 show, Wait Wait...Dont Tell Me, on which she's a popular panelist, shes here to explore what we can learn about asking the right questions from a unique critique of published behavioral research.
We hear about AI almost every day now. Opinions seem split between impending doom side and "superintelligence will save the human race." Jana Eggers offers the real deal on AI, explaining what's hype and what isn't and what we can do about it.
Strata + Hadoop World program chairs Roger Magoulas, Doug Cutting, and Alistair Croll welcome you to the first day of keynotes.
Strata + Hadoop World program chairs Roger Magoulas, Doug Cutting, and Alistair Croll welcome you to the second day of keynotes.
2016 marks the 10th anniversary of Apache Hadoop. This birthday provides us an opportunity to celebrate, as well as the chance to reflect on how we got here and where we are going.
The next evolution in the on-demand economy is in predictive analytics fueled by live streams of datain effect knowing what customers want before they do. Eric Frenkiel explains how a real-time trinity of technologiesKafka, Spark, and MemSQLis enabling Uber and others to power their own revolutions with predictive apps and analytics.
Megan Price demonstrates how machine-learning methods help us determine what we know, and what we don't, about the ongoing conflict in Syria. Megan then explains why these methods can be crucial to better understand patterns of violence, enabling better policy decisions, resource allocation, and ultimately, accountability and justice.
Big data is not limited to reporting and analysis; increasingly, companies are differentiating themselves by acting on data in real time. But what does "real time" really mean? Jack Norris discusses the challenges of coordinating data flows, analysis, and integration at scale to truly impact business as it happens.
Pivotals Ian Andrews explores why delivering information in context is the key to competitive differentiation in the digital economy.
Alyosha Efros discusses using computer vision to understand big visual data.
The cybersecurity landscape is quickly changing, and Apache Hadoop is becoming the analytics and data management platform of choice for cybersecurity practitioners. Tom Reilly and Alan Ross explain why organizations are turning toward the open source ecosystem to break down traditional cybersecurity analytics and data constraints in order to detect a new breed of sophisticated attacks.
Joseph Sirosh offers a fascinating look into how brains connected with sensors to the cloud and machine learning could revolutionize a field of medicine.
Bob Rogers, Intel's chief data scientist for big data solutions, demonstrates the power of the question in analytics. Learn how different types of data, from cubes of structured data to live video streams from mobile systems, combine with analytical technology to inform the questions that can be answered.
Michael Franklin offers an overview of the Berkeley Data Analytics Stack, outlines the current directions it's taking, and settles once and for all how BDAS should be pronounced.
As the volume and variety of data continue to grow, organizations have the opportunity to transform their industries and professions, but companies are grappling with how to deliver innovation. Adam Kocoloski shares his experience around this market shift and challenges attendees to join his mission of contributing to the community and investing in the power of open source and the cloud.
Julia Galef explores why Bayesian thinking is so different from what we do by default and outlines the most important principles of thinking like a Bayesian.
As a technical founder at Siri, Sentient, and Viv Labs, Adam Cheyer has helped design and develop a number of intelligent systems solving real-world problems for hundreds of millions of users. Drawing on specific examples, Adam reveals some of the techniques he uses to maximize the impact of the AI technologies he employs.
US Department of Commerce Deputy Secretary of Commerce Bruce Andrews explores using commerce data to fuel innovation.
Paula Poundstone isnt just any comedian. After years of justly criticizing and questioning the purpose of the many studies used for questions on NPRs #1 show, Wait Wait...Dont Tell Me, on which she's a popular panelist, shes here to explore what we can learn about asking the right questions from a unique critique of published behavioral research.
We hear about AI almost every day now. Opinions seem split between impending doom side and "superintelligence will save the human race." Jana Eggers offers the real deal on AI, explaining what's hype and what isn't and what we can do about it.
Visual analysis is an iterative process for working with data that exploits the power of the human visual system.  The formal core of visual analysis is the mapping of data to appropriate visual representations. Learn what years of research have taught us about designing visualizations people can learn from and understand.
With the rise of big data more and more people need effective visualizations. Needs may range from simple charts to massive interactive network graphs. A range of tools exist, but still many find none that meet all their requirements: Cross-browser usage, server-side rendering, iOS support, full control of look and feel, and your options are suddenly very slim. We share our lessons and approach.
See how applying traditional data analysis tools, as well as more esoteric ones like computer vision, to multiple disparate data sets and data types can create a more complete and nuanced narrative of one of San Franciscos most vibrant streets.
Data visualization is just one tool that designers use to communicate data-driven recommendations. In this session I present a case study on the use of user-centered design practices to craft meaningful and actionable data presentations for business users. Data visualization and UX work best when they work together.
Custom data exploration tools can provide efficient and exciting interfaces for audiences not well served by out-of-the-box business intelligence solutions.  Frameworks not only beautify data but also surface novel observations from the set.  In this session, we survey the creative coding frameworks that lend themselves to visualization and offer some insight into their strengths and weaknesses.
In this talk we report on the value of tools that support a human-driven approach to revealing innovation opportunities hidden withing big datasets. Based on our experience in data science projects involving multiple stakeholders we found that sketching with data and rapidly sharing interactive information visualizations is a key practice to transform information into useful services and products.
The use of video to communicate data is on the rise, but what is the most effective way to do this? Highlighting our current work with the BBC in this field we will look at best practice from storytelling principles to choosing the right visual treatment.
Long a staple of broadcast sports, augmented reality (AR) effects (like the virtual "1st and 10" line) are increasingly being driven by digital records of sports events (DREs), collected and distributed live, such as NASCAR's race car tracking system and MLB's PitchFX.  The next generation of DRE-derived data will expand the use of AR to more effectively show key "invisible" elements of the game.
With the explosion of mobile devices, there is a plethora of geo-tagged data available for mining and visualization. To make compelling visualizations, it is often necessary to build tools that allow users to easily explore, mine, map, and market this data. This talk will focus on how to use several open-source frameworks to build such visualizations.
Beautiful, useful and scalable techniques for analysing and displaying spatial information are key to unlocking important trends in geospatial and geotemporal data. Recent developments in HTML 5 enable rendering of complex visualisations within the browser, facilitating fast, dynamic user interfaces built around web maps. This session will examine emerging technologies that will shape the geoweb.
The ultimate utility of Big Data is transforming it into Big Insights.  Charts, graphs, and tables of aggregated data are useful but still require interpretation by the end user. With advances in linguistic algorithms and data processing it is now possible to derive meaningful insights from data and present them in digestible narrative content.
A story or report on a subject by its very nature summarizes the underlying data. But readers may have questions specific to a time, date or place. Visualizing the data and providing effective, targeted ways to drill deeper is key to giving the reader more than just the story.
Visual analysis is an iterative process for working with data that exploits the power of the human visual system.  The formal core of visual analysis is the mapping of data to appropriate visual representations. Learn what years of research have taught us about designing visualizations people can learn from and understand.
With the rise of big data more and more people need effective visualizations. Needs may range from simple charts to massive interactive network graphs. A range of tools exist, but still many find none that meet all their requirements: Cross-browser usage, server-side rendering, iOS support, full control of look and feel, and your options are suddenly very slim. We share our lessons and approach.
See how applying traditional data analysis tools, as well as more esoteric ones like computer vision, to multiple disparate data sets and data types can create a more complete and nuanced narrative of one of San Franciscos most vibrant streets.
Data visualization is just one tool that designers use to communicate data-driven recommendations. In this session I present a case study on the use of user-centered design practices to craft meaningful and actionable data presentations for business users. Data visualization and UX work best when they work together.
Custom data exploration tools can provide efficient and exciting interfaces for audiences not well served by out-of-the-box business intelligence solutions.  Frameworks not only beautify data but also surface novel observations from the set.  In this session, we survey the creative coding frameworks that lend themselves to visualization and offer some insight into their strengths and weaknesses.
In this talk we report on the value of tools that support a human-driven approach to revealing innovation opportunities hidden withing big datasets. Based on our experience in data science projects involving multiple stakeholders we found that sketching with data and rapidly sharing interactive information visualizations is a key practice to transform information into useful services and products.
The use of video to communicate data is on the rise, but what is the most effective way to do this? Highlighting our current work with the BBC in this field we will look at best practice from storytelling principles to choosing the right visual treatment.
Long a staple of broadcast sports, augmented reality (AR) effects (like the virtual "1st and 10" line) are increasingly being driven by digital records of sports events (DREs), collected and distributed live, such as NASCAR's race car tracking system and MLB's PitchFX.  The next generation of DRE-derived data will expand the use of AR to more effectively show key "invisible" elements of the game.
With the explosion of mobile devices, there is a plethora of geo-tagged data available for mining and visualization. To make compelling visualizations, it is often necessary to build tools that allow users to easily explore, mine, map, and market this data. This talk will focus on how to use several open-source frameworks to build such visualizations.
Beautiful, useful and scalable techniques for analysing and displaying spatial information are key to unlocking important trends in geospatial and geotemporal data. Recent developments in HTML 5 enable rendering of complex visualisations within the browser, facilitating fast, dynamic user interfaces built around web maps. This session will examine emerging technologies that will shape the geoweb.
The ultimate utility of Big Data is transforming it into Big Insights.  Charts, graphs, and tables of aggregated data are useful but still require interpretation by the end user. With advances in linguistic algorithms and data processing it is now possible to derive meaningful insights from data and present them in digestible narrative content.
A story or report on a subject by its very nature summarizes the underlying data. But readers may have questions specific to a time, date or place. Visualizing the data and providing effective, targeted ways to drill deeper is key to giving the reader more than just the story.
This tutorial provides a solid foundation for those seeking to understand large scale data processing with MapReduce and Hadoop, plus its associated ecosystem. This session is intended for those who are new to Hadoop and are seeking to understand where Hadoop is appropriate and how it fits with existing systems. No programming experience is required.
Wouldn't it be great if there were just use two algorithms which could handle most of your predictive modeling needs? It turns out that actually this is the case. Noted machine learning instructor Dr Mike Bowles and champion data miner Jeremy Howard will teach you everything you need to know to apply them successfully.
In this session, Hortonworks CEO Eric Baldeschwieler will look at the current state of Apache Hadoop, how the ecosystem is evolving by working together to close the existing technological  and knowledge gaps, and present a roadmap for the future of the project.
This session will draw on numerous customer examples to reveal powerful tips, tricks, and in-depth use cases to show how Hadoop can easily integrate, scale, and analyze important data.
In this talk, we'll build a complete, scalable collaborative filtering ("people who X also Y") system that is almost identical to what prominent Internet properties use today. We'll talk about model improvements, performance enhancements, and practical considerations. This is a practical talk accessible to all.
As more companies adopt Hadoop to perform data intensive tasks for large data sets, there is a burning need to make Hadoop available to a broader set of developers. This talk covers two approaches Microsoft is exploring for this purpose: 1. JavaScript interfaces to run Hadoop jobs and 2. web interfaces for Hadoop that let developers write and run MapReduce jobs from any platform.
How do you architect big data systems that leverage virtualization and platform as a service? We will walk through a layered approach to building a unified analytics platform using virtualization, provisioning tools and platform as a service.
Learn how Citygrid built a world class platform to aggregate the data powering it's publicly available local places, content and ads APIs using Hadoop, Solr and MongoDB.
This talk will go in details, architecture and challenges of building a recommendation system on a massive social graph. The talk will describe how we applied learning on large datasets using Apache Hadoop and how we scaled millions of reads and writes. We will also showcase Eventbrite's data platform architecture.
This session discusses financial services use cases and challenges in using Hadoop analytics including long-term storage and analytics of transactions, identifying cross and up sell opportunities by analyzing web log files and customer profiles, value-at-risk analytics, and understanding the SLA issues and identifying problems in a thousands-of-nodes, big-services oriented architecture.
Hundreds of hours of video recordings culled from multiple cameras. Most of these recordings hold little value as the scene does not change for extended periods of time. For organizations that must keep the original in tact, analyzing these recordings can be very difficult. Using Map/Reduce we can harness parallel processing to identify and tag useful periods of time for faster analysis.
Map/Reduce has created tremendous interest in parallel programming and big data analytics, but it isn't always the right tool for the job. Many new projects have emerged in this space over the last year including two cluster schedulers (YARN and Mesos) and numerous parallel computing environments. We'll provide an introduction to these new technologies, including some you might not have heard of.
NetApp collects 250 TB per year of unstructured data from devices that phone home. They need to be able to do ad hoc analysis and build predictive models for device support and cross-sales. We discuss our experiences building a Big Data system with NetApp using Hadoop and HBase to improve customer service, drive sales and develop better products.
The science and commercial worlds share requirements for a high performance informatics platform to support collection, curation, collaboration, exploration, and analysis of massive datasets. SciDB is an open source analytical database that provides seamlessly integrated massively scalable analytics.  We present performance and scalability for non-embarrassingly parallel operations.
This tutorial provides a solid foundation for those seeking to understand large scale data processing with MapReduce and Hadoop, plus its associated ecosystem. This session is intended for those who are new to Hadoop and are seeking to understand where Hadoop is appropriate and how it fits with existing systems. No programming experience is required.
Wouldn't it be great if there were just use two algorithms which could handle most of your predictive modeling needs? It turns out that actually this is the case. Noted machine learning instructor Dr Mike Bowles and champion data miner Jeremy Howard will teach you everything you need to know to apply them successfully.
In this session, Hortonworks CEO Eric Baldeschwieler will look at the current state of Apache Hadoop, how the ecosystem is evolving by working together to close the existing technological  and knowledge gaps, and present a roadmap for the future of the project.
This session will draw on numerous customer examples to reveal powerful tips, tricks, and in-depth use cases to show how Hadoop can easily integrate, scale, and analyze important data.
In this talk, we'll build a complete, scalable collaborative filtering ("people who X also Y") system that is almost identical to what prominent Internet properties use today. We'll talk about model improvements, performance enhancements, and practical considerations. This is a practical talk accessible to all.
As more companies adopt Hadoop to perform data intensive tasks for large data sets, there is a burning need to make Hadoop available to a broader set of developers. This talk covers two approaches Microsoft is exploring for this purpose: 1. JavaScript interfaces to run Hadoop jobs and 2. web interfaces for Hadoop that let developers write and run MapReduce jobs from any platform.
How do you architect big data systems that leverage virtualization and platform as a service? We will walk through a layered approach to building a unified analytics platform using virtualization, provisioning tools and platform as a service.
Learn how Citygrid built a world class platform to aggregate the data powering it's publicly available local places, content and ads APIs using Hadoop, Solr and MongoDB.
This talk will go in details, architecture and challenges of building a recommendation system on a massive social graph. The talk will describe how we applied learning on large datasets using Apache Hadoop and how we scaled millions of reads and writes. We will also showcase Eventbrite's data platform architecture.
This session discusses financial services use cases and challenges in using Hadoop analytics including long-term storage and analytics of transactions, identifying cross and up sell opportunities by analyzing web log files and customer profiles, value-at-risk analytics, and understanding the SLA issues and identifying problems in a thousands-of-nodes, big-services oriented architecture.
Hundreds of hours of video recordings culled from multiple cameras. Most of these recordings hold little value as the scene does not change for extended periods of time. For organizations that must keep the original in tact, analyzing these recordings can be very difficult. Using Map/Reduce we can harness parallel processing to identify and tag useful periods of time for faster analysis.
Map/Reduce has created tremendous interest in parallel programming and big data analytics, but it isn't always the right tool for the job. Many new projects have emerged in this space over the last year including two cluster schedulers (YARN and Mesos) and numerous parallel computing environments. We'll provide an introduction to these new technologies, including some you might not have heard of.
NetApp collects 250 TB per year of unstructured data from devices that phone home. They need to be able to do ad hoc analysis and build predictive models for device support and cross-sales. We discuss our experiences building a Big Data system with NetApp using Hadoop and HBase to improve customer service, drive sales and develop better products.
The science and commercial worlds share requirements for a high performance informatics platform to support collection, curation, collaboration, exploration, and analysis of massive datasets. SciDB is an open source analytical database that provides seamlessly integrated massively scalable analytics.  We present performance and scalability for non-embarrassingly parallel operations.
R and Hadoop, the two hottest stars on the Analytics stage, were meant to be together. The open source RHadoop project was established to make it happen. We'll go over what RHadoop does for you, how to use it, and why you should add it to your toolset.
 At Cloudera, we've found that monitoring Apache Hadoop is itself a big data problem. Here I'll present work we've been doing on turning the vast amounts of monitoring data a Hadoop cluster generates into meaningful signals to help us wrestle with the biggest challenges of maintaining large distributed systems: failure of machines, processes and people, and root-cause analysis after-the-fact.
Hadoop is not an island.  To deliver a complete Big Data solution, a data pipeline needs to be developed that incorporates and orchestrates many diverse technologies. Using an example of real-time weblog processing, in this session we will demonstrate how the open source Spring Batch and Spring Integration projects can be used to build manageable and robust pipeline solutions around Hadoop.
Cloudera Data Scientist Josh Wills will share insights and how to tricks about Crunch, a Java library that aims to make writing, testing and running MapReduce pipelines that run over any type of data easy, efficient and even fun.
Learn how to integrate MongoDB with Hadoop for large-scale distributed data processing.
Using Hadoop based business intelligence analytics, we analyzed Hadoop source code over time.  This talk illustrates text and related analytics with Hadoop on Hadoop to reveal the true hidden secrets of the elephant. This entertaining session highlights the value of data correlation across multiple datasets and the visualization of those correlations to reveal hidden data relationships.
NoSQL, Big Data, massive scale, real-time, in the cloud, do I need it, do I want it, how the heck can I even know if its right for me? Choosing any database solution is a critical and tricky decision. Navigating the murky waters of NoSQL can be even tougher.
Storm is an open-source realtime computation system relied upon by Twitter for much of its analytics. Storm does for realtime computation what Hadoop did for batch computation. It has a huge range of applications and combines ease of use with a robust foundation.
Flurry provides an analytics and advertising platform for smartphone applications. Every month we track over 20 billion sessions across over 330 million devices. This talk will go over the Hadoop and HBase architecture we run and the challenges we face managing a massively growing data set.
Mobile devices offer boundless opportunities for collection and presentation of temporally- and spatially-relevant data. But there are obstacles: intermittent connectivity as well as processing, storage and other constraints. Featuring real-world apps, this session covers device data collection; device-device and device-cloud data synchronization; and data aggregation and analysis in the cloud.
Data storage needs are increasing at an exponential rate. Incumbent storage systems are proprietary, expensive to buy and expensive to maintain. With the advent of the cloud, everyone expects auto scaling. Ceph storage is a massively scalable storage system that aims to fill the distributed storage system void.
Maps of the complex connections that form when people link, like, reply, rate, review, favorite, friend, follow, edit, and mention one another can reveal important trends.  It is possible to create network maps with free and open tools that identify key people and sub-groups in any social media population with just a few key clicks.  Can you make a pie chart? You can now make a network chart.
This workshop is a jumpstart lesson on how to get from a blank page and a pile of data to a useful data visualization. We'll focus on the design process, not specific tools. Bring your sample data and paper or a laptop; leave with new visualization ideas.
Learn now how to use a Hadoop cluster for data analysis using Java MapReduce, Apache Hive and Apache Pig, and get an overview of using the HBase Hadoop database. Some programming experience is strongly recommended for this session.
R and Hadoop, the two hottest stars on the Analytics stage, were meant to be together. The open source RHadoop project was established to make it happen. We'll go over what RHadoop does for you, how to use it, and why you should add it to your toolset.
 At Cloudera, we've found that monitoring Apache Hadoop is itself a big data problem. Here I'll present work we've been doing on turning the vast amounts of monitoring data a Hadoop cluster generates into meaningful signals to help us wrestle with the biggest challenges of maintaining large distributed systems: failure of machines, processes and people, and root-cause analysis after-the-fact.
Hadoop is not an island.  To deliver a complete Big Data solution, a data pipeline needs to be developed that incorporates and orchestrates many diverse technologies. Using an example of real-time weblog processing, in this session we will demonstrate how the open source Spring Batch and Spring Integration projects can be used to build manageable and robust pipeline solutions around Hadoop.
Cloudera Data Scientist Josh Wills will share insights and how to tricks about Crunch, a Java library that aims to make writing, testing and running MapReduce pipelines that run over any type of data easy, efficient and even fun.
Learn how to integrate MongoDB with Hadoop for large-scale distributed data processing.
Using Hadoop based business intelligence analytics, we analyzed Hadoop source code over time.  This talk illustrates text and related analytics with Hadoop on Hadoop to reveal the true hidden secrets of the elephant. This entertaining session highlights the value of data correlation across multiple datasets and the visualization of those correlations to reveal hidden data relationships.
NoSQL, Big Data, massive scale, real-time, in the cloud, do I need it, do I want it, how the heck can I even know if its right for me? Choosing any database solution is a critical and tricky decision. Navigating the murky waters of NoSQL can be even tougher.
Storm is an open-source realtime computation system relied upon by Twitter for much of its analytics. Storm does for realtime computation what Hadoop did for batch computation. It has a huge range of applications and combines ease of use with a robust foundation.
Flurry provides an analytics and advertising platform for smartphone applications. Every month we track over 20 billion sessions across over 330 million devices. This talk will go over the Hadoop and HBase architecture we run and the challenges we face managing a massively growing data set.
Mobile devices offer boundless opportunities for collection and presentation of temporally- and spatially-relevant data. But there are obstacles: intermittent connectivity as well as processing, storage and other constraints. Featuring real-world apps, this session covers device data collection; device-device and device-cloud data synchronization; and data aggregation and analysis in the cloud.
Data storage needs are increasing at an exponential rate. Incumbent storage systems are proprietary, expensive to buy and expensive to maintain. With the advent of the cloud, everyone expects auto scaling. Ceph storage is a massively scalable storage system that aims to fill the distributed storage system void.
Maps of the complex connections that form when people link, like, reply, rate, review, favorite, friend, follow, edit, and mention one another can reveal important trends.  It is possible to create network maps with free and open tools that identify key people and sub-groups in any social media population with just a few key clicks.  Can you make a pie chart? You can now make a network chart.
This workshop is a jumpstart lesson on how to get from a blank page and a pile of data to a useful data visualization. We'll focus on the design process, not specific tools. Bring your sample data and paper or a laptop; leave with new visualization ideas.
Learn now how to use a Hadoop cluster for data analysis using Java MapReduce, Apache Hive and Apache Pig, and get an overview of using the HBase Hadoop database. Some programming experience is strongly recommended for this session.
Want to extract and process Big Data from the web? This tutorial will show you how to use key open source technologies such as Hadoop, Cascading, Bixo, Tika, Mahout and Solr to create scalable, reliable web mining solutions.
Learn first hand from award-winning Guardian journalists how they mix data, journalism and visualization to break and tell compelling stories: all at newsroom speeds.
With billions of social activities passing through the ever-growing realtime social web each day, companies are beginning to harness the power of social data. In this session, participants will learn from real-world case studies in Financial Services, Emergency Response, Brand Analytics and other industries about how businesses are applying social data to their operations to drive value.
Who influences whom? Data science can help answering this question which is of fundamental importance to business, politics, public health and many others.
Pascal Boillat, Fannie Maes Chief Information Officer, will address how changing data standards and implementation strategies is having a profound effect on the financial services industry.
Topics will span the data flow lifecycle from data collection, curation and quality, to aggregation and standardization of a multitude of complex data sources, to the creation of valuable analytics, including recommendations that connect users to the data.
Federal transparency initiatives have spawned millions of rows of data, state and local programs engage developers and wonks with APIs, contests and data galore. Private industry offers attribute-laden device exhaust, forming a geo-footprint of who is going where, when, how and (maybe) for what. Who decides data provenance? Does curated data get treated the same as heterogeneous data?
Facebook's Open Graph, Schema.org, and a recent scramble towards a "Rosetta Stone" for geodata, are all examples of a trend towards linking data across the web. Weaving data into the web simplifies integration. Big Data offers ways to mine huge datasets for insight. Linked Data turns the web into a dataset
So much of the privacy discussion is about data access, fear of future dystopia, and the complexities of law.  There is a vacuum around how societal norms should be mapped to rapidly growing capabilities of big data, leaving data professionals in a "dont ask don't tell" privacy conundrum.  This conversation will discuss specific use-cases and frameworks to guide data pros.
Making sense of the privacy issues around personal data is way too complicated. Pretty Simple Data Privacy builds on the idea that users need three options - Yes, No, Maybe - to control privacy settings on their personal data. We'll explore existing projects and codebases that implement legal and technical tools for all three of the settings.
This talk uses the OODA Loop concept (Observe, Orient, Decide, Act) as a framework to categorize Big Data use cases and data-driven services and the front-ends to those services. Rather than starting with the underlying technology or the data sources, the OODA loop starts with WHY the user needs information. It answers the question of when a black box beats an analytic tool, and vice versa.
In a research environment, under the current operating system, most data and figures collected or generated during your work is lost, intentionally tossed aside or classified as junk, or at worst trapped in silos or locked behind embargo periods. In the digital age, this does not need to be the case - and it's imperative we change that reality.
The common good challenge for Big Data is to deliver actionable information that can be used by nonprofits and civic orgs.  But that challenge isnt new.  Existing data intermediaries for NGOs have a rich history of working in common-good territory.  Lets discuss. What is this history?  What can we take away from it to inform new, perhaps disruptive, approaches to meet this challenge?
Metastasis is the lethal form of cancer. Metastasis arises through cancer cells traveling through the blood of the patient and colonizing in other organs. Finding and characterizing these cells enables the prediction and monitoring of response to cancer treatments.
Want to extract and process Big Data from the web? This tutorial will show you how to use key open source technologies such as Hadoop, Cascading, Bixo, Tika, Mahout and Solr to create scalable, reliable web mining solutions.
Learn first hand from award-winning Guardian journalists how they mix data, journalism and visualization to break and tell compelling stories: all at newsroom speeds.
With billions of social activities passing through the ever-growing realtime social web each day, companies are beginning to harness the power of social data. In this session, participants will learn from real-world case studies in Financial Services, Emergency Response, Brand Analytics and other industries about how businesses are applying social data to their operations to drive value.
Who influences whom? Data science can help answering this question which is of fundamental importance to business, politics, public health and many others.
Pascal Boillat, Fannie Maes Chief Information Officer, will address how changing data standards and implementation strategies is having a profound effect on the financial services industry.
Topics will span the data flow lifecycle from data collection, curation and quality, to aggregation and standardization of a multitude of complex data sources, to the creation of valuable analytics, including recommendations that connect users to the data.
Federal transparency initiatives have spawned millions of rows of data, state and local programs engage developers and wonks with APIs, contests and data galore. Private industry offers attribute-laden device exhaust, forming a geo-footprint of who is going where, when, how and (maybe) for what. Who decides data provenance? Does curated data get treated the same as heterogeneous data?
Facebook's Open Graph, Schema.org, and a recent scramble towards a "Rosetta Stone" for geodata, are all examples of a trend towards linking data across the web. Weaving data into the web simplifies integration. Big Data offers ways to mine huge datasets for insight. Linked Data turns the web into a dataset
So much of the privacy discussion is about data access, fear of future dystopia, and the complexities of law.  There is a vacuum around how societal norms should be mapped to rapidly growing capabilities of big data, leaving data professionals in a "dont ask don't tell" privacy conundrum.  This conversation will discuss specific use-cases and frameworks to guide data pros.
Making sense of the privacy issues around personal data is way too complicated. Pretty Simple Data Privacy builds on the idea that users need three options - Yes, No, Maybe - to control privacy settings on their personal data. We'll explore existing projects and codebases that implement legal and technical tools for all three of the settings.
This talk uses the OODA Loop concept (Observe, Orient, Decide, Act) as a framework to categorize Big Data use cases and data-driven services and the front-ends to those services. Rather than starting with the underlying technology or the data sources, the OODA loop starts with WHY the user needs information. It answers the question of when a black box beats an analytic tool, and vice versa.
In a research environment, under the current operating system, most data and figures collected or generated during your work is lost, intentionally tossed aside or classified as junk, or at worst trapped in silos or locked behind embargo periods. In the digital age, this does not need to be the case - and it's imperative we change that reality.
The common good challenge for Big Data is to deliver actionable information that can be used by nonprofits and civic orgs.  But that challenge isnt new.  Existing data intermediaries for NGOs have a rich history of working in common-good territory.  Lets discuss. What is this history?  What can we take away from it to inform new, perhaps disruptive, approaches to meet this challenge?
Metastasis is the lethal form of cancer. Metastasis arises through cancer cells traveling through the blood of the patient and colonizing in other organs. Finding and characterizing these cells enables the prediction and monitoring of response to cancer treatments.
This tutorial will enable anyone with some programming experience to begin analyzing data with the R programming language
This presentation goes beyond the hype, buzzwords, and rehashed slides and actually presents the attendees with a hands-on, step-by-step tutorial on how to write a Java application on top of Apache Cassandra. It focuses on concepts such as idempotence, tunable consistency, and shared-nothing clusters to help attendees get started with Apache Cassandra quickly while avoiding common pitfalls.
In this panel discussion, DataStax CEO BIlly Bosworth will moderate a discussion that will spotlight real mission critical Big Data use cases from "hands-on" practitioners. With companies like Walmart, Netflix, & Apigee among many others adopting Apache Cassandra and other new database technologies, there's never been a more exciting time to be building data intensive applications.
The race is on to create the next competitive advantage. Attend this customer session for a brief introduction to Greenplums Big Data Analytics platform.
Advances in columnar databases are creating bio-science opportunities that were previously not possible.  Fernanda Foertter and the team at Genus discovered an innovative way to store and access the huge volumes of data being generated modeling genotypes. She and Jim Tommaney discuss the benefits of column storage and how InfiniDBs Map Reduce empowers high performance Big Data analytics.
Running large scale datastores requires us to handle various challenges such as scalability, reliability, performance, and reduced operational overhead. In this talk, we will discuss how Amazon DynamoDB was designed to address these problems.
This session looks at the requirements for a multi-tenant big data cluster: one where different lines of businesses, different projects, and multiple applications can be run with assured SLAs, resulting in higher utilization and ROI for these clusters.
This presentation will cover the next generation of Apache Hadoop, known as hadoop-0.23. Learn how MapReduce has been re-architected by the community to improve reliability, availability and scalability as well as adding support for alternate programming paradigms. Also learn about HDFS Federation, which allows for significant scalability improvements, as well as other important advancements.
In this session, attendees will learn about a new method for solving big data analytics via HPCC Systems, an open-source enterprise proven platform for Big Data.  A case study will be given using patent data to demonstrate how big data can be process, linked, analyzed, searched and delivered to answer various queries.
Machine learning (ML) holds the key to the most advanced uses of big data.  But is ML really possible on big data with state-of-the-art methods, or just simple ones?  Can ML really be done in real time today?  Is MapReduce the right answer?  The cloud?  I will review the current state of ML technology both at the research level and the industry-readiness level, and current best solution options.
The session will talk about costs involved in Big Data projects, covering the apparent and also hidden aspects of these costs. It will also discuss how to build a Big Data solution with lower cost of per TB Data Managed and Analyzed.
This tutorial will enable anyone with some programming experience to begin analyzing data with the R programming language
This presentation goes beyond the hype, buzzwords, and rehashed slides and actually presents the attendees with a hands-on, step-by-step tutorial on how to write a Java application on top of Apache Cassandra. It focuses on concepts such as idempotence, tunable consistency, and shared-nothing clusters to help attendees get started with Apache Cassandra quickly while avoiding common pitfalls.
In this panel discussion, DataStax CEO BIlly Bosworth will moderate a discussion that will spotlight real mission critical Big Data use cases from "hands-on" practitioners. With companies like Walmart, Netflix, & Apigee among many others adopting Apache Cassandra and other new database technologies, there's never been a more exciting time to be building data intensive applications.
The race is on to create the next competitive advantage. Attend this customer session for a brief introduction to Greenplums Big Data Analytics platform.
Advances in columnar databases are creating bio-science opportunities that were previously not possible.  Fernanda Foertter and the team at Genus discovered an innovative way to store and access the huge volumes of data being generated modeling genotypes. She and Jim Tommaney discuss the benefits of column storage and how InfiniDBs Map Reduce empowers high performance Big Data analytics.
Running large scale datastores requires us to handle various challenges such as scalability, reliability, performance, and reduced operational overhead. In this talk, we will discuss how Amazon DynamoDB was designed to address these problems.
This session looks at the requirements for a multi-tenant big data cluster: one where different lines of businesses, different projects, and multiple applications can be run with assured SLAs, resulting in higher utilization and ROI for these clusters.
This presentation will cover the next generation of Apache Hadoop, known as hadoop-0.23. Learn how MapReduce has been re-architected by the community to improve reliability, availability and scalability as well as adding support for alternate programming paradigms. Also learn about HDFS Federation, which allows for significant scalability improvements, as well as other important advancements.
In this session, attendees will learn about a new method for solving big data analytics via HPCC Systems, an open-source enterprise proven platform for Big Data.  A case study will be given using patent data to demonstrate how big data can be process, linked, analyzed, searched and delivered to answer various queries.
Machine learning (ML) holds the key to the most advanced uses of big data.  But is ML really possible on big data with state-of-the-art methods, or just simple ones?  Can ML really be done in real time today?  Is MapReduce the right answer?  The cloud?  I will review the current state of ML technology both at the research level and the industry-readiness level, and current best solution options.
The session will talk about costs involved in Big Data projects, covering the apparent and also hidden aspects of these costs. It will also discuss how to build a Big Data solution with lower cost of per TB Data Managed and Analyzed.
The biggest problem in data science is ... the data itself.
New analysts or engineers are often lost when textbook approaches fail on real world data. Drawing inspiration from problem solving techniques in mathematics and physics, we will walk through examples that illustrate how come up with creative solutions and solve real world problems with data.
How to simplify the data integration process and save a significant amount of development time by automatically generating code for processes (data profiling, data cleansing, and record linkage).  A case study will show a complex, Big Data linking application, where insurance data was converted to HPCC using the SALT tool and reduced 20,000+ lines of source code to a 48-line SALT specification.
Instead of working too hard to define the parameters in an attempt to completely remove the ambiguity, look at what people do, interact with and talk about.  We can watch what people do and decide from there what a coffee shop is and where the boundaries of your neighborhood are.  It might not be the truth, but it can be darn close.
Netflix is known for pushing the envelope of recommendation technologies. The Netflix Prize put a spotlight on recommender system research and a focus on predicting ratings. But, predicting a rating is only part of the recommendation problem. In this talk I will describe how other sources of implicit and contextualized information can be used to create a personalized experience.
Data science applied in engineering driven industries is revolutionizing how highly complex products are developed.  Unprecedented access to computing power combined with advanced data science tools provide the opportunity to not only increase the speed of development but also improve the final design.  Using a practical aerospace example, Joris will illustrate the tools and techniques described.
In today's environments, we're often forced to collect data before we know if it will be useful. This tendency leads toe seas of data, flowing in real-time with very little structure or understanding of what the data means.  Given that, how can you tell when data "is normal?"  Let's find out.
In "The Evolution of Data Products", O'Reilly Media's Mike Loukides notes: "the question of how we take the next step  where data recedes into the background  is surprisingly tough." Jeremy Howard will show why this is tough, and what to do about it. He will show how predictive modelling, simulation, and optimization can be combined to deliver results instead of just delivering data.
In this session we discuss approaches to mining unstructured data that gradually find their way into the real world. Text mining and analytics algorithms strive to identify documents categories, main topics, mentioned names and other entities; they summarize and detect sentiment. We describe case studies that take advantage of such algorithms in the legal, forensics and healthcare sectors.
Big data isn't just about multi-terrabyte data sets hidden inside eventually-concurrent distributed databases in the cloud. It's also about the hidden data you carry with you all the time. This talk will discuss the data that you carry with you all the time; the data on your cell phone and other mobile devices, along with the possibilities for making use of that hidden data.
In this talk, we will analyze various dimensions of microwork that characterize applications, tasks, and crowds. Drawing on our experience at companies that have pioneered the use of microwork (Samasource) and data science (LinkedIn), we will offer practical advice to help you design crowdsourcing workflows to meet your data product needs.
Mendeley is a New York and London-based startup that has crowdsourced the world's largest database of academic literature. Over 1M researchers strong, Mendeley is taking academia to the cloud.
The biggest problem in data science is ... the data itself.
New analysts or engineers are often lost when textbook approaches fail on real world data. Drawing inspiration from problem solving techniques in mathematics and physics, we will walk through examples that illustrate how come up with creative solutions and solve real world problems with data.
How to simplify the data integration process and save a significant amount of development time by automatically generating code for processes (data profiling, data cleansing, and record linkage).  A case study will show a complex, Big Data linking application, where insurance data was converted to HPCC using the SALT tool and reduced 20,000+ lines of source code to a 48-line SALT specification.
Instead of working too hard to define the parameters in an attempt to completely remove the ambiguity, look at what people do, interact with and talk about.  We can watch what people do and decide from there what a coffee shop is and where the boundaries of your neighborhood are.  It might not be the truth, but it can be darn close.
Netflix is known for pushing the envelope of recommendation technologies. The Netflix Prize put a spotlight on recommender system research and a focus on predicting ratings. But, predicting a rating is only part of the recommendation problem. In this talk I will describe how other sources of implicit and contextualized information can be used to create a personalized experience.
Data science applied in engineering driven industries is revolutionizing how highly complex products are developed.  Unprecedented access to computing power combined with advanced data science tools provide the opportunity to not only increase the speed of development but also improve the final design.  Using a practical aerospace example, Joris will illustrate the tools and techniques described.
In today's environments, we're often forced to collect data before we know if it will be useful. This tendency leads toe seas of data, flowing in real-time with very little structure or understanding of what the data means.  Given that, how can you tell when data "is normal?"  Let's find out.
In "The Evolution of Data Products", O'Reilly Media's Mike Loukides notes: "the question of how we take the next step  where data recedes into the background  is surprisingly tough." Jeremy Howard will show why this is tough, and what to do about it. He will show how predictive modelling, simulation, and optimization can be combined to deliver results instead of just delivering data.
In this session we discuss approaches to mining unstructured data that gradually find their way into the real world. Text mining and analytics algorithms strive to identify documents categories, main topics, mentioned names and other entities; they summarize and detect sentiment. We describe case studies that take advantage of such algorithms in the legal, forensics and healthcare sectors.
Big data isn't just about multi-terrabyte data sets hidden inside eventually-concurrent distributed databases in the cloud. It's also about the hidden data you carry with you all the time. This talk will discuss the data that you carry with you all the time; the data on your cell phone and other mobile devices, along with the possibilities for making use of that hidden data.
In this talk, we will analyze various dimensions of microwork that characterize applications, tasks, and crowds. Drawing on our experience at companies that have pioneered the use of microwork (Samasource) and data science (LinkedIn), we will offer practical advice to help you design crowdsourcing workflows to meet your data product needs.
Mendeley is a New York and London-based startup that has crowdsourced the world's largest database of academic literature. Over 1M researchers strong, Mendeley is taking academia to the cloud.
What are the fundamental skills that a CEO needs to become Data Driven? In this session we will discuss the 3 essential skills that will enable CEOs to effectively lead their organizations into the Data Revolution. These organizations will harness the power of data to innovate, grow profits and beat the competition.
Mark Madsen talks about how regular businesses will eventually embrace a data-driven mindset, with some trademark 'Madsen' history background to put it in context. People throw around 'industrial revolution of data' and 'new oil' a lot without really thinking about what things like the scientific method, or steam power, or petrochemicals did as a result.
"Big data" provides the opportunity to combine new, rich data sources in novel ways to discover business insights.  How do you use analytics to exploit this data so that it will yield real business value?  Learn a proven technique that ensures you identify where and how big data analytics can be successfully deployed within your organization.  Case study examples will demonstrate its use.
In this session, business agility expert Michael Hugos will present examples from his work in applying immersive animation techniques and gaming dynamics, and discuss how they can address the challenges of consuming - and responding to - the data deluge, turning information overload into business advantage.
In this session, Marcia Tal will demonstrate how significant business value is being realized through sophisticated understanding of intent and interconnectedness, at scale.
Jon Bruner leads a panel discussion with a few of the days presenters and takes final questions from the audience.
Opening remarks by Program Chair, Alistair Croll, Founder, Bitcurrent
Author and digital marketing evangelist Avinash Kaushik shares his perspective, drawing from experience with some of the world's largest online marketers, and looks at how an analyst mentality is quickly permeating all aspects of business and marketing.
The effect of big data on all business models cannot be denied. This panel of SCM experts looks at how business are using, or should be using, big data to drive supply chain management issues focusing on the broader manufacturing issues that must be addressed as well as practical tips that can be applied in dealing with supply chains that now span the globe.
This presentation lays out some clear, concrete gating conditions for when it makes sense to pull the trigger on big data initiatives, and how they should be procured, depending on the use case, the data assets, and the resources available.
There are many rapidly evolving technologies that provide objective metrics and analytics for most outward facing business interactions. The evolution of similar inward facing tools has not kept pace. In this presentation we discuss which sources of internal organizational data are frequently neglected, approaches for automating data collection, and what valuable insights can result from analysis.
Search user interfaces are slow to change; ideas for new search interfaces rarely take hold.  This talk will forecast how search is likely to change and what will stay the same in the coming years.
What are the fundamental skills that a CEO needs to become Data Driven? In this session we will discuss the 3 essential skills that will enable CEOs to effectively lead their organizations into the Data Revolution. These organizations will harness the power of data to innovate, grow profits and beat the competition.
Mark Madsen talks about how regular businesses will eventually embrace a data-driven mindset, with some trademark 'Madsen' history background to put it in context. People throw around 'industrial revolution of data' and 'new oil' a lot without really thinking about what things like the scientific method, or steam power, or petrochemicals did as a result.
"Big data" provides the opportunity to combine new, rich data sources in novel ways to discover business insights.  How do you use analytics to exploit this data so that it will yield real business value?  Learn a proven technique that ensures you identify where and how big data analytics can be successfully deployed within your organization.  Case study examples will demonstrate its use.
In this session, business agility expert Michael Hugos will present examples from his work in applying immersive animation techniques and gaming dynamics, and discuss how they can address the challenges of consuming - and responding to - the data deluge, turning information overload into business advantage.
In this session, Marcia Tal will demonstrate how significant business value is being realized through sophisticated understanding of intent and interconnectedness, at scale.
Jon Bruner leads a panel discussion with a few of the days presenters and takes final questions from the audience.
Opening remarks by Program Chair, Alistair Croll, Founder, Bitcurrent
Author and digital marketing evangelist Avinash Kaushik shares his perspective, drawing from experience with some of the world's largest online marketers, and looks at how an analyst mentality is quickly permeating all aspects of business and marketing.
The effect of big data on all business models cannot be denied. This panel of SCM experts looks at how business are using, or should be using, big data to drive supply chain management issues focusing on the broader manufacturing issues that must be addressed as well as practical tips that can be applied in dealing with supply chains that now span the globe.
This presentation lays out some clear, concrete gating conditions for when it makes sense to pull the trigger on big data initiatives, and how they should be procured, depending on the use case, the data assets, and the resources available.
There are many rapidly evolving technologies that provide objective metrics and analytics for most outward facing business interactions. The evolution of similar inward facing tools has not kept pace. In this presentation we discuss which sources of internal organizational data are frequently neglected, approaches for automating data collection, and what valuable insights can result from analysis.
Search user interfaces are slow to change; ideas for new search interfaces rarely take hold.  This talk will forecast how search is likely to change and what will stay the same in the coming years.
This hands-on tutorial teaches you how to setup and use Hive, a high-level, data warehouse tool for Hadoop. Hive provides a SQL-like query language, HiveQL, that is easy to learn for people with prior SQL experience, making Hive attractive for data warehousing teams. Hive leverages the power of Hadoop for working with massive data sets without requiring expertise in MapReduce programming.
In this hands-on class, learn how to turn data into effective, interactive visualizations. You do not require a Tableau license to participate, but must bring a Windows laptop or virtual machine.
In this session, Expedia, one of the worlds leading online travel companies, describes how they tapped into their massive machine data to deliver unprecedented insights across key IT and business areas  from ad metrics and risk analysis, to capacity planning, security, and availability analysis.
Microsoft's Big Data solution turns signals into information.  Learn how Microsoft's Hadoop service and rich BI capabilities can drive your business forward.
Today's users won't tolerate slow applications. More often than not, the database is the bottleneck in the application. Learn how VMware vFabric SQLFire can give you the speed and scale you need in a substantially simpler way. SQLFire is a memory-optimized and horizontally-scalable distributed SQL database. Attend this session to learn how SQLFire gives high performance without the complexity.
This session will explore a new class of analytic platforms and technologies such as SQL-MapReduce which bring the science of data to the art of business. By fusing standard business intelligence and analytics with next-generation data processing techniques such as MapReduce, big data analysis is no longer just in the hands of the few data science or MapReduce specialists in an organization!
Data Scientists must deal with many Big Data challenges including volume, velocity and variety of data.  These challenges require a new solution - Automated Understanding - a new evolution in software. In this session Tim Estes will show the power of this new capability on a large and valuable dataset that has never been deeply understood by software before.
Monitoring thousands of servers generates a lot of data. Many organizations trying to harness enormous amounts of data struggle with the same types of challenges as the Rackspace cloud monitoring team. Find out how Rackspace uses NoSQL technology, distributed concepts, and open source software in novel ways to produce a multi-region cloud monitoring API.
Nick Halstead CTO of DataSift will talk about Hadoop, HBase and dealing with storing and processing a billion tweets every 3 days. You will get insights into the architecture, pitfalls and real-world lessons on using Big Data technologies.
Gary Lang, Senior VP Engineering, MarkLogic, will discuss the concept of Big Data Applications and walk through three in-production implementations of Big Data Applications in action.
Theres a big opportunity for big data: human processing.  Max Yankelevich explores the latest innovations combining the scalable quality control of artificial intelligence with the scalable human judgment of crowdsourcing to solve big data problems.  Learn the surprisingly easy methods to leverage the crowd to collect, control, validate and enrich data.
This hands-on tutorial teaches you how to setup and use Hive, a high-level, data warehouse tool for Hadoop. Hive provides a SQL-like query language, HiveQL, that is easy to learn for people with prior SQL experience, making Hive attractive for data warehousing teams. Hive leverages the power of Hadoop for working with massive data sets without requiring expertise in MapReduce programming.
In this hands-on class, learn how to turn data into effective, interactive visualizations. You do not require a Tableau license to participate, but must bring a Windows laptop or virtual machine.
In this session, Expedia, one of the worlds leading online travel companies, describes how they tapped into their massive machine data to deliver unprecedented insights across key IT and business areas  from ad metrics and risk analysis, to capacity planning, security, and availability analysis.
Microsoft's Big Data solution turns signals into information.  Learn how Microsoft's Hadoop service and rich BI capabilities can drive your business forward.
Today's users won't tolerate slow applications. More often than not, the database is the bottleneck in the application. Learn how VMware vFabric SQLFire can give you the speed and scale you need in a substantially simpler way. SQLFire is a memory-optimized and horizontally-scalable distributed SQL database. Attend this session to learn how SQLFire gives high performance without the complexity.
This session will explore a new class of analytic platforms and technologies such as SQL-MapReduce which bring the science of data to the art of business. By fusing standard business intelligence and analytics with next-generation data processing techniques such as MapReduce, big data analysis is no longer just in the hands of the few data science or MapReduce specialists in an organization!
Data Scientists must deal with many Big Data challenges including volume, velocity and variety of data.  These challenges require a new solution - Automated Understanding - a new evolution in software. In this session Tim Estes will show the power of this new capability on a large and valuable dataset that has never been deeply understood by software before.
Monitoring thousands of servers generates a lot of data. Many organizations trying to harness enormous amounts of data struggle with the same types of challenges as the Rackspace cloud monitoring team. Find out how Rackspace uses NoSQL technology, distributed concepts, and open source software in novel ways to produce a multi-region cloud monitoring API.
Nick Halstead CTO of DataSift will talk about Hadoop, HBase and dealing with storing and processing a billion tweets every 3 days. You will get insights into the architecture, pitfalls and real-world lessons on using Big Data technologies.
Gary Lang, Senior VP Engineering, MarkLogic, will discuss the concept of Big Data Applications and walk through three in-production implementations of Big Data Applications in action.
Theres a big opportunity for big data: human processing.  Max Yankelevich explores the latest innovations combining the scalable quality control of artificial intelligence with the scalable human judgment of crowdsourcing to solve big data problems.  Learn the surprisingly easy methods to leverage the crowd to collect, control, validate and enrich data.
Despite the hype, Big Data has yet to live up to its potential. Why?  Because weve spent too much time thinking about the data itself and not enough time considering which business decisions can be improved through the intelligent application of data.  Panjiva CEO Josh Green will discuss an alternative approach: starting with a challenging business problem and then tracking down relevant data.
There is a revolution at hand centering on this groundswell of data and it will change how we execute our businesses through greater efficiencies, new revenue discovery and even enable innovation. It is the revolution of Big Data. Management Strategies for Big Data will explain this new wave of technology and provide a roadmap for businesses to take advantage of this growing trend.
While enterprises see an opportunity to increase revenues and decrease costs by becoming a data-driven organization, it is not easy to decide where and how to begin. This session highlights some principles for success through examining two real-world big data case studies.
Leapfrog enabled their learning toys and set up a system to have millions of toy owners upload their play logs.  This talk covers the business strategy and the technical implementation hurdles from perspective of the former Director of Data Services who implemented it.
Learn about how data is used for a fashion retailer that is on a rapid growth path. At ModCloth we don't believe in dictating fashion trends to our customerwe are inverting the pyramid and democratizing fashion. Buying patterns and user interactions are leveraged to help us understand how we can meet our customers' desires
Moneyball is to marketing science as CSI is to forensic science. The expectations are high and marketers are shouting "where's the insight?" and "ENHANCE!". Data is long and marketing scientists are short. We can only scale through technology. This is the story of how a developer and two marketing scientists became data scientists in crossing that gap.
A high level overview of Microsoft IT's BI strategy and it's various applications, focusing on Self Service BI, Scorecards and Dashboards, Data Visualizations, and Leadership Decision making through robust BI tools.
By charging interchange fees for retailers and account fees for customers banks have taken a combative approach for revenue generation.  However, technologies are emerging that enable financial institutions to leverage big data drawn from the transaction data stream to provide new, pro-consumer revenue streams.
What does it really take to build a data product?  Recall and relevancy are only parts of the challenge.  In fact, an entire new approach is required to build consistently great data products.
One of the most significant challenges faced by individuals and organizations is how to discover and collaborate with data within and across their organizations, which often stays trapped in application and organizational silos.
Due to recent advancements in Big Data, cloud computing, and network maturity it's now possible to work with extremely large weather-related data sets. The Climate Corporation CTO Siraj Khaliq discusses how to apply big data principles to the real-world challenge of protecting people and businesses from the financial impact of weather.
Measuring productivity remains a notoriously difficult problem. We will show how real-time collaboration data are being leveraged to measure, model and forecast organizational productivity and performance in the innovation teams at Boeing and in 3 Formula One teams. On the back of these forecasts, we will show how investment yields were improved by 15% and productivity raised by nearly 20%.
Despite the hype, Big Data has yet to live up to its potential. Why?  Because weve spent too much time thinking about the data itself and not enough time considering which business decisions can be improved through the intelligent application of data.  Panjiva CEO Josh Green will discuss an alternative approach: starting with a challenging business problem and then tracking down relevant data.
There is a revolution at hand centering on this groundswell of data and it will change how we execute our businesses through greater efficiencies, new revenue discovery and even enable innovation. It is the revolution of Big Data. Management Strategies for Big Data will explain this new wave of technology and provide a roadmap for businesses to take advantage of this growing trend.
While enterprises see an opportunity to increase revenues and decrease costs by becoming a data-driven organization, it is not easy to decide where and how to begin. This session highlights some principles for success through examining two real-world big data case studies.
Leapfrog enabled their learning toys and set up a system to have millions of toy owners upload their play logs.  This talk covers the business strategy and the technical implementation hurdles from perspective of the former Director of Data Services who implemented it.
Learn about how data is used for a fashion retailer that is on a rapid growth path. At ModCloth we don't believe in dictating fashion trends to our customerwe are inverting the pyramid and democratizing fashion. Buying patterns and user interactions are leveraged to help us understand how we can meet our customers' desires
Moneyball is to marketing science as CSI is to forensic science. The expectations are high and marketers are shouting "where's the insight?" and "ENHANCE!". Data is long and marketing scientists are short. We can only scale through technology. This is the story of how a developer and two marketing scientists became data scientists in crossing that gap.
A high level overview of Microsoft IT's BI strategy and it's various applications, focusing on Self Service BI, Scorecards and Dashboards, Data Visualizations, and Leadership Decision making through robust BI tools.
By charging interchange fees for retailers and account fees for customers banks have taken a combative approach for revenue generation.  However, technologies are emerging that enable financial institutions to leverage big data drawn from the transaction data stream to provide new, pro-consumer revenue streams.
What does it really take to build a data product?  Recall and relevancy are only parts of the challenge.  In fact, an entire new approach is required to build consistently great data products.
One of the most significant challenges faced by individuals and organizations is how to discover and collaborate with data within and across their organizations, which often stays trapped in application and organizational silos.
Due to recent advancements in Big Data, cloud computing, and network maturity it's now possible to work with extremely large weather-related data sets. The Climate Corporation CTO Siraj Khaliq discusses how to apply big data principles to the real-world challenge of protecting people and businesses from the financial impact of weather.
Measuring productivity remains a notoriously difficult problem. We will show how real-time collaboration data are being leveraged to measure, model and forecast organizational productivity and performance in the innovation teams at Boeing and in 3 Formula One teams. On the back of these forecasts, we will show how investment yields were improved by 15% and productivity raised by nearly 20%.
Contrary to popular belief, SQL and NoSQL are not at odds with each other, they are dualsin fact NoSQL should really be called coSQL.  Recognizing this duality can change the way we think about which technology to use when, and what we need to invest in next.
With the collection of almost every piece of information about your customers comes the ability to start asking your data the right question: Why do they do what they do? And even more: what would they do if I could interact with them. We show for the case of online display advertising, how causal analysis gives interesting new answers about the right (and wrong) ways of spending your money.
Getting training data for a recommender system is easy: if users clicked it, its a positive - if they didnt, its a negative.   Or is it? In this talk, we use examples from production recommender systems to bring training data to the forefront: from overcoming presentation bias to the art of crowdsourcing subjective judgments to creative data exhaust exploitation and feature creation.
Learn various ways to bootstrap a custom corpus for training highly accurate natural language processing models. Real world examples will be presented with Python code samples using NLTK. Each example will show you how, starting from scratch, you can rapidly produce a highly accurate custom corpus for training the kinds of natural language processing models you need.
Twenty-first century big data is being used to train predictive models of emotional sentiment, customer churn, patient health, and other behavioral complexities. Variable importance and feature selection reduces the dimensionality of our models, so an unfeasible and complex problem may become somewhat more predictable.
The tools of social network analysis are based on mathematical network theory. There is very little in these techniques that actually requires that the data represents social activity. We'll show how these techniques can be applied to data from areas such as geo, linguistics and the Wikipedia link graph. We'll visualise and explore the data using Gephi, the "Photoshop for graphs".
Relational databases were based on Set theory  which insists that the order of items does not matter.  For many (most?) data problems, however, order does matter.  By using Array theory, a relational-like database gains a considerable advantage over set-theory based engines.
We examine the effectiveness of a statistical technique known as survival analysis to optimize the cache time-to-live for hotel rates in a hotel rate cache.  We describe how we collect and prepare nearly a billion records per day utilizing MongoDB and Hadoop. Finally, we show how this analysis is improving the operation of our hotel rate cache.
Join leading data scientists in debating hot issues in the profession.
Contrary to popular belief, SQL and NoSQL are not at odds with each other, they are dualsin fact NoSQL should really be called coSQL.  Recognizing this duality can change the way we think about which technology to use when, and what we need to invest in next.
With the collection of almost every piece of information about your customers comes the ability to start asking your data the right question: Why do they do what they do? And even more: what would they do if I could interact with them. We show for the case of online display advertising, how causal analysis gives interesting new answers about the right (and wrong) ways of spending your money.
Getting training data for a recommender system is easy: if users clicked it, its a positive - if they didnt, its a negative.   Or is it? In this talk, we use examples from production recommender systems to bring training data to the forefront: from overcoming presentation bias to the art of crowdsourcing subjective judgments to creative data exhaust exploitation and feature creation.
Learn various ways to bootstrap a custom corpus for training highly accurate natural language processing models. Real world examples will be presented with Python code samples using NLTK. Each example will show you how, starting from scratch, you can rapidly produce a highly accurate custom corpus for training the kinds of natural language processing models you need.
Twenty-first century big data is being used to train predictive models of emotional sentiment, customer churn, patient health, and other behavioral complexities. Variable importance and feature selection reduces the dimensionality of our models, so an unfeasible and complex problem may become somewhat more predictable.
The tools of social network analysis are based on mathematical network theory. There is very little in these techniques that actually requires that the data represents social activity. We'll show how these techniques can be applied to data from areas such as geo, linguistics and the Wikipedia link graph. We'll visualise and explore the data using Gephi, the "Photoshop for graphs".
Relational databases were based on Set theory  which insists that the order of items does not matter.  For many (most?) data problems, however, order does matter.  By using Array theory, a relational-like database gains a considerable advantage over set-theory based engines.
We examine the effectiveness of a statistical technique known as survival analysis to optimize the cache time-to-live for hotel rates in a hotel rate cache.  We describe how we collect and prepare nearly a billion records per day utilizing MongoDB and Hadoop. Finally, we show how this analysis is improving the operation of our hotel rate cache.
Join leading data scientists in debating hot issues in the profession.
The big data world is extremely chaotic based on technology in its infancy. Learn how to tame this chaos, integrate it within your existing data environments (RDBMS, analytic databases, applications), manage the workflow, orchestrate jobs, improve productivity and make using big data technologies accessible to a much wider spectrum of developers, analysts and data scientists.
While extracting entities from massive amounts of text is a major problem, a proven solution exists. This tutorial will demonstrate a natural language parsing technology to extract entities from all kinds of text using massively parallel clusters.
The big data world is extremely chaotic based on technology in its infancy. Learn how to tame this chaos, integrate it within your existing data environments (RDBMS, analytic databases, applications), manage the workflow, orchestrate jobs, improve productivity and make using big data technologies accessible to a much wider spectrum of developers, analysts and data scientists.
While extracting entities from massive amounts of text is a major problem, a proven solution exists. This tutorial will demonstrate a natural language parsing technology to extract entities from all kinds of text using massively parallel clusters.
Opening remarks by the Strata program chairs, Edd Dumbill and Alistair Croll.
Apache Hadoop forms the kernel of an operating system for Big Data. This ecosystem of interdependent projects enables institutions to affordably explore ever vaster quantities of data.  The platform is young, but it is strong and vibrant, built to evolve.
The explosion of data is both a challenge and opportunity for businesses.  In order to thrive in this new world, organizations will need a technical strategy for sifting through all of this data and driving insights.
How big data tools and technologies give us back our individual identity ... because if you didn't know you were unique and special, well, you are. Big data can be applied to solving socio-economic problems that rival the scale and importance of building ad optimization models.
Tools for attacking big data problems originated at consumer internet companies, but the number and variety of big data problems have spread across industries and around the world. I'll present a brief summary of some of the critical social and business problems that we're attacking with the open source Apache Hadoop platform.
Back in the late 80s artificial intelligence was set to take over the world; it didnt happen. In 2012; AI has been stripped down, dressed up and reborn as machine learning. Will it take over the world this time? What makes a Big Data - Machine Learning solution better?
The increasing use of online software and digital devices in the classroom provides a source of high-frequency data streams that can be analyzed to better understand student progress, identify individual needs, and develop personal recommendations.
So you've hoarded the world's data within your enterprise. Now what? Author and digital marketing evangelist Avinash Kaushik shares lessons from the nascent world of Web Analytics on how multiplicity, scale and outsourcing powers a data democracy, and how that in turn drives business action.
Negative results from clinical trials go missing far too often, leading us to overestimate the benefits of treatments. Attempts to remedy this problem haven't worked well. Ben Goldacre, both a doctor and data geek, will talk about how to fix this, and other, problems in medicine.
Opening remarks by the Strata program chairs, Alistair Croll and Edd Dumbill.
Big data isn't just an abstract problem for corporations, financial firms, and tech companies. To your mother, a 'big data' problem might simply be too much email, or a lost file on her computer. We need to democratize access to the tools used for understanding information by taking the hard-work out of drawing insight from excessive quantities of information.
Why unstructured data beats structured.
The expected massive growth of connected device, appliance and sensor markets in the coming years - often called 'The Internet of Things' - will need a more rich concept of 'open data' than is currently common.
Big Data is about extracting value from fast, huge, varied, complex data sets. But simply crunching data is only the first step. As adoption of MapReduce and data analytic technologies increases, forward thinking companies are starting to build applications on their core data assets.
Dr. Richard Merkin, President and CEO of Heritage Provider Network, that was recently named one of Fast Companys 10 most innovative healthcare companies for 2012, will announce the winner of the second progress prize in the $3 million dollar Heritage Health Prize competition.
Google Insights for Search provides an index of search activity for millions of queries.  These queries can sometimes help understand consumer behavior.  Hal describes some of the issues that arise in trying to use this data for short-term economic forecasts and provide examples.
How are businesses using big data to connect with their customers, deliver new products or services faster and create a competitive advantage?  Learn about the changing nature of customer intimacy and how the technologies and techniques around big data analysis provide business advantage in today's social, mobile environment  and why it is imperative to adopt a big data analytics strategy.
Why data can tell us only so much about food, flavor, and our preferences.
Opening remarks by the Strata program chairs, Edd Dumbill and Alistair Croll.
Apache Hadoop forms the kernel of an operating system for Big Data. This ecosystem of interdependent projects enables institutions to affordably explore ever vaster quantities of data.  The platform is young, but it is strong and vibrant, built to evolve.
The explosion of data is both a challenge and opportunity for businesses.  In order to thrive in this new world, organizations will need a technical strategy for sifting through all of this data and driving insights.
How big data tools and technologies give us back our individual identity ... because if you didn't know you were unique and special, well, you are. Big data can be applied to solving socio-economic problems that rival the scale and importance of building ad optimization models.
Tools for attacking big data problems originated at consumer internet companies, but the number and variety of big data problems have spread across industries and around the world. I'll present a brief summary of some of the critical social and business problems that we're attacking with the open source Apache Hadoop platform.
Back in the late 80s artificial intelligence was set to take over the world; it didnt happen. In 2012; AI has been stripped down, dressed up and reborn as machine learning. Will it take over the world this time? What makes a Big Data - Machine Learning solution better?
The increasing use of online software and digital devices in the classroom provides a source of high-frequency data streams that can be analyzed to better understand student progress, identify individual needs, and develop personal recommendations.
So you've hoarded the world's data within your enterprise. Now what? Author and digital marketing evangelist Avinash Kaushik shares lessons from the nascent world of Web Analytics on how multiplicity, scale and outsourcing powers a data democracy, and how that in turn drives business action.
Negative results from clinical trials go missing far too often, leading us to overestimate the benefits of treatments. Attempts to remedy this problem haven't worked well. Ben Goldacre, both a doctor and data geek, will talk about how to fix this, and other, problems in medicine.
Opening remarks by the Strata program chairs, Alistair Croll and Edd Dumbill.
Big data isn't just an abstract problem for corporations, financial firms, and tech companies. To your mother, a 'big data' problem might simply be too much email, or a lost file on her computer. We need to democratize access to the tools used for understanding information by taking the hard-work out of drawing insight from excessive quantities of information.
Why unstructured data beats structured.
The expected massive growth of connected device, appliance and sensor markets in the coming years - often called 'The Internet of Things' - will need a more rich concept of 'open data' than is currently common.
Big Data is about extracting value from fast, huge, varied, complex data sets. But simply crunching data is only the first step. As adoption of MapReduce and data analytic technologies increases, forward thinking companies are starting to build applications on their core data assets.
Dr. Richard Merkin, President and CEO of Heritage Provider Network, that was recently named one of Fast Companys 10 most innovative healthcare companies for 2012, will announce the winner of the second progress prize in the $3 million dollar Heritage Health Prize competition.
Google Insights for Search provides an index of search activity for millions of queries.  These queries can sometimes help understand consumer behavior.  Hal describes some of the issues that arise in trying to use this data for short-term economic forecasts and provide examples.
How are businesses using big data to connect with their customers, deliver new products or services faster and create a competitive advantage?  Learn about the changing nature of customer intimacy and how the technologies and techniques around big data analysis provide business advantage in today's social, mobile environment  and why it is imperative to adopt a big data analytics strategy.
Why data can tell us only so much about food, flavor, and our preferences.
The Kepler spacecraft launched on March 7, 2009, initiating NASA's first search for Earth-size planets orbiting Sun-like stars, with stunning results after being on the job for just over two years. Designing and building the Kepler science pipeline software that processes and analyzes the resulting data to make the discoveries presented a daunting set of challenges.
Analytics projects are often bedeviled  or simply stopped in their tracks  by challenges emanating from organizational culture, misunderstanding of statistical concepts, and discomfort with probabilistic reasoning. This talk will provide a number of case studies and offer practical tips for achieving organizational buy-in.
One of the best ways to use data is for marketing purposes: to help deliver the most relevant products to your customers. This talk will describe LinkedIn's approach to marketing: the philosophy behind our marketing efforts, the way that we use data, and the technical systems that we use to do personalized marketing at scale.
'Crowdsourcing big data' might sound like a randomly generated selection of buzz words, but it turns out to represent a powerful leap forward in the accuracy of predictive analytics. This session will explore the reasons why this is the case, using case studies from the fields of astronomy, sports ratings systems and tourism forecasting.
Big data provides the opportunity to combine new, rich data sources in novel ways to discover business insights.  How do you use analytics to exploit this data so that it will yield real business value?  Learn a proven technique that ensures you identify where and how big data analytics can be successfully deployed within your organization.  Case study examples will demonstrate its use.
How the core principles of Hadoop address the core problems in banking  and will lead to a transformation of an industry in need of one.
A deep dive into how big data plays in emergency management, from boots on the ground to business continuity - now and in the future. User interfaces for sense-making will be discussed.
We will talk about Global Viral Forecasting's 'EpidemicIQ' project, which tracks all the globe's known and potential disease outbreaks. It is the largest humanitarian application of machine-learning and crowdsourcing to date, dynamically adapting to new threats and data sources in real-time.
United Nations Global Pulse and Adaptive Path have been collaborating on a new global crisis impact tool called HunchWorks that allows experts to post hypotheses about emerging crises and crowd source verification. The presentation will focus on lessons learned from a complex project that combines human expertise and big data algorithms using human-centered design and assistive intelligence.
Analytical culture is the substrate necessary for organizations to turn the promise of data driven decisions to reality. Fostering and developing such a culture is hard. The presentation will focus on a set of organizational and solution design principles gleaned from real world experiences that have proven to be very effective to build an analytical culture.
This session examines the challenges and approaches for collecting, organizing and extracting value from machine data  the data generated continuously by all IT systems containing a record of all activity and behavior. Harnessing this data can provide valuable new insights for both IT and business users. This session will be hosted by Splunk's CIO and Salesforce.com.
This session will introduce the history and philosophy of MongoDB. We'll also review a few key use cases for NoSQL and MongoDB in particular.
Factual creates canonical reference sets of 40 million entities from over 2.5 billion fragmentary inputs.  This talk explains the Hadoop-based science of our approach combined with what we believe to be a necessary art -- the application of domain-specific knowledge -- in creating pragmatic data services.
Youve collected a ton of data and your team is busily crunching numbers and coming to conclusions... but are they the right ones? You can only know with the right context and you cant get context working in a silo. We invite you to bring the rest of the world into your data warehouse. Dont worry, itll add more value than it takes and instead of working on the data, you can work on your vision.
Structured search improves the search experience through the identification of entities and their relationships in documents and queries. This panel will explore the current state of structured and semi-structured search, as well as exploring the open problems in an area that promises to revolutionize information seeking.
Experts say that there is no such thing as clean data, yet every day critical decisions are being made based on electronic data.  Elizabeth Charnock, author of E-Habits, will discuss how to make decisions based on digital character, and not on individual bits or bytes.
In this session we'll discuss strategies for building agile big-data clouds that make it much faster and easier for data scientists to discover, provision and analyze data. We'll discuss where and how new technologies (both vendor and OSS) fit into this model.
The science and commercial worlds share requirements for a high performance informatics platform to support collection, curation, collaboration, exploration, and analysis of massive datasets. SciDB is an open source analytical database that provides better analytical performance than relational databases as well as supports key features such as provenance and versioning.
The recent integration of previously isolated telescopes into expanding smart telescope networks, spanning continents and responding to transient events in seconds, sees novel architectures emerging to help filter science from data in real time. Machine learning and collective decision making are used by these geographically distributed networks to optimise output in the face of scarce resources.
At the heart of every system that harnesses big data is a pipeline that comprises collecting large volumes of raw data, extract value from it through analytics or data transformations, then delivering that condensed set of results back out -- potentially to millions of users. This talk examines the challenges of building manageable, robust pipelines.
A new breed of chief privacy officer (CPO) is emerging. An engineer that is comfortable in a product focus group, engineering scrum, or analyzing test results. They have the historical awareness, frontier spirit, regulatory caution, and technical chops to work through the toughest data issues. The promise of the engineer CPO is that products, not only safeguard privacy, but compete on it.
All big data models are wrong but some are useful, as George Box might have said. Models are not the end result of a big data architecture, but exploratory tools in their own right. They are most useful when data scientists try to understand the business, and when our users learn a bit about data. How can the actual process of modeling improve a big data system, and teach the organization?
With over 700 million check-ins, 10 million nodes in the social graph, and billions of cumulative signals, Justin Moore will be explaining how foursquare processes, analyzes, and builds products to help people explore the real world.
While most of the focus in data science is on the rapid analysis of vast volumes of data, the hardest part of most solutions is the data acquisition, movement, transformation, and loading - the "data logistics". This presentation will describe the common challenges and solutions - including the best and worst practices that can be reused from Data Warehousing.
Statistics, math and data analysis would easily make most people's "Top 10 Most Boring Topics" list. But an effective data visualization can bring new insights, raise awareness and tell a great story. We want to share our insights from efforts to enable data visualizations on top of massive amounts of data, explore good - and bad - examples and share some of the tools and techniques we use.
Widespread reaction to the recent phone hacking story prompted the Guardian to  capture and visualize Twitter traffic during key events. Find out how we produced interactive interfaces that enable readers to make sense of over 1.5 million tweets in a few minutes.
Readers and preparers of graphs: Learn to recognize and avoid some common graphical mistakes to understand your data better and make better decisions from data.
This talk will introduce the concept of "responsible data visualization" in the context of two distinct uses: exploration and narrative. Using personal and industry examples to show best and worst practices in each approach, this talk will offer practical suggestions to bringing data visualization into one's data workflow.
Economists utilize a data analysis toolkit and intuition that can be very helpful to Data Scientists. In particular, econometric methods are quite useful in disentangling correlation and causation, a use case not well-handled by standard machine learning and statistical techniques. This session will cover examples of econometric methods in action, as well as other economics-related insights.
How do data infrastructure, insights and products change when your user base grows by orders of magnitude?
Politically charged data visualization emerged over the last election cycle as a provocative and powerful means of persuasive communication.  Weve seen organizational charts used as protest signs and the White House regularly releases infographics.  With these political chart wars as a backdrop, this presentation will show you how to be a smart consumer of data visualizations and infographics.
A jumpstart lesson on how to get from a blank page and a pile of data to a useful data visualization. We'll focus on the design process, not specific tools. The talk will include discussion of figuring out what story to tell, selecting right data, and picking appropriate encodings. We'll briefly discuss tools and visualization styles, and look at several examples.
This session will address specific use cases relevant to customers with big data needs.  We will highlight customers already successfully utilizing this service as well as showcase top scenarios and explain why it makes sense to leverage the cloud for Big Data needs.
Google is a Data business: over the past few years, many of the tools Google created to store, query, analyze, visualize its data, have been exposed to developers as services. This talk will give you an overview of Google services for Data Crunchers.
Data flows with little friction; its gathered and stored in increasing volume. Consumer concerns include guarding data against misuse and managing how its shared. Low friction data channels are valuable but vulnerable information ecologies. This session shows how data fuels life, and why we must balance regulatory and security controls with recognition of how data flow drives economy and culture
A panel discussion that will identify and debate the key ethical, legal, and policy issues in analytics and applications of big data.  The panelists will hash out some of the novel privacy concerns, but they will also consider issues around autonomy, fairness, fragmentation, and transparency, as well as the appropriate and available responses to them.
The Apache Cassandra database has added many new enterprise features this year based on the real-world needs of companies like Twitter, Netflix, Openwave, and others building massively scalable systems. This talk will focus on the shift to real-time data driven applications and what that means and why Cassandra is ideal for todays enterprise data applications.
This session will explore a new class of analytic platforms and technologies such as SQL-MapReduce which bring the science of data to the art of business.
Learn how Thomson Reuters manages and processes a variety of very large and diverse data sources to quickly publish timely, trusted, and relevant information to their clients.
Gaining Adoption Through Data Visualization
Dr. Amr Awadallah, CTO at Cloudera, illustrates how Apache Hadoop is changing the business intelligence data stack, and how the evolving architecture delivers advanced capabilities for solving key business challenges. By enabling the complete value to be derived from both unstructured and structured data, organizations are able to ask and get answers to previously un-addressable big questions.
Telecom network switches, network servers and other equipment generate and store large amounts of data every day. The data is mainly used for billing and network operations, If utilized fully, this data can have an enormous impact on network operations and overall profitability.
Ron Avnur, SVP Engineering, MarkLogic, and Mark Rodgers, Sr. Director of Product Engineering, LexisNexis will reveal how LexisNexis is rebuilding its business platform to handle Big Data in real-time. Avnur and Rodgers will discuss what it means to have Big Data, how the global organization addressed that challenge, and the business benefits resulting from the solution.
In this talk, Dr. Dunning will outline strategies for integrating big data technologies like Hadoop into existing business data systems. The talk will provide vignettes drawn from real-life situations that expose the challenges customers have faced and the solutions that meet these challenges
Steven Hillion, VP of EMC Greenplums Data Analytics Lab lends insight into emerging technologies to take advantage of the big data opportunity and how big data challenges todays BI architectures and approaches to data management.
Panel Discussion on Assembling Data to Fight Breast Cancer
Businesses today are moving beyond the buzz and experimentation with batch processing options of Hadoop and MapReduce, stretching the limits for cutting edge performance & scalability. This session will talk about emerging trends of a new generation of NoHadoop (Not Only Hadoop) architectures for future proof big data scalability and prepare you for life beyond the elephant ride !
BoF topics are entirely up to you. Sign up on site to lead a conversation during lunch on Thursday, September 22.
BoF topics are entirely up to you. Sign up on site to lead a conversation during lunch on Friday, September 23.
From hackathons to API-enabled civic data, learn how New York City government is evolving thanks to deeper engagement with the technology community.
The Kepler Mission began its science observations just over two years ago on May 12, 2009, initiating NASAs first search for Earth-like planets. Initial results and light curves from Kepler are simply breath-taking, including  confirmation of the first unquestionable rocky planet, Kepler-10b, and Kepler-11b, a system of 6 transiting planets orbiting one Sun-like star.
In this presentation, Jer Thorp will discuss his work with names--designing an arrangement algorithm for the 9/11 Memorial in Manhattan. Hell walk through collaborative processes, admit to a series of failures and ultimately show how humans and software can combine to solve extraordinary problems.
This session will show you how you can bring the science of data to the art of business and empower more business users and analysts to operationalize insights and drive results.
Data scientists and technology companies are rapidly recognizing the immense power of data for drawing insights about their impact and operations, yet NGOs and non-profits are increasingly being left behind with mounting data and few resources to make use of it.
The first person to conceive of something is usually not the first. They're the first to re-conceive at a point where the current technology caught up to someone else's idea. We're at a point today where many old ideas are being reinvented. Hear why looking to the past, beyond your core field of interest, is worthwhile.
The BodyTrack Project is building tools, both technological and cultural, to empower more people to embrace an "investigator" role in their own lives
Big Data is more than just volume and velocity. MarkLogic CEO Ken Bado will address why complexity is the key gotcha for organizations trying to outflank their competition by managing Big Data in real time. Learn how winners today are using MarkLogic to manage the complexity of their unstructured information to drive revenue and results.
The flow of data across the social web tells us what people, around the world, are paying attention to at any given moment. Understanding this flow is both a mathematical and a human problem, as we develop and adapt techniques to find stories in the data. Come hear about the expected and the surprises in the bitly data, as well as generalized techniques that apply to any 'realtime' data system.
Opening remarks by the Strata program chairs, Edd Dumbill and Alistair Croll.
Opening remarks by the Strata program chairs, Alistair Croll and Edd Dumbill.
Dr. Richard Merkin, President and CEO of Heritage Provider Network, is pleased to announce the winner of the first $3 million dollar Heritage Health Progress Prize.
This talk will discuss the pitfalls of the man versus machine premise while underscoring the need for man and machine to work together in order to make the most of Big Data. We will also address the need to create a new, visual language to allow humans and machines to realize the full potential of their collaboration.
Quantitative Engineer? Business Intelligence Analyst? Data Scientist? The data deluge has come upon us so quickly that we don't even know what to call ourselves, much less how to make a career of working with data.  This talk examines the critical traits that lead to success by looking back to what may be the first act of data science.
Sometimes the hardest part about making a viz is knowing where to start. Check out the winning vizzes from the Strata/Tableau Data Visualization Contest and get inspired to create your own beautiful visualizations.
Communicating Data Clearly describes how to draw clear, concise, accurate graphs that are easier to understand than many of the graphs one sees today.  The tutorial emphasizes how to avoid common mistakes that produce confusing or even misleading graphs. Graphs for one, two, three, and many variables are covered as well as general principles for creating effective graphs.
This workshop is a jumpstart lesson on how to get from a blank page and a pile of data to a useful data visualization. We'll focus on the design process, not specific tools. Bring your sample data and paper or a laptop; leave with new visualization ideas.
Big Data takes on the planets toughest challenge by analyzing weathers complex behavior. Using hundreds of terabytes of data and trillions of simulation datapoints, The Climate Corporation models weathers impact on crops to create customized insurance for farmers facing the financial impact of extreme weather.
Anne Milgram, Senior Fellow at the NYU Law Center on the Administration of Criminal Law Center.
How a traditional Spanish-language media company, made the strategic decision to build a robust analytics intelligence division to more effectively target the Hispanic market. Attendees will walk away with insights on how this traditional media company implemented a big data and MapReduce operations from the ground up.
Real estate used to be an industry that had large information asymmetry between professionals and consumers. Zillow has leveled the playing field through its living database on over 100 million homes.  Advanced statistical modeling gives consumers even more information and tools, such as the well-known Zestimate. Using data and analytics, Zillow has changed the real estate industry forever.
In this session, Kevin Foster, IBM Big Data Solution Architect, will provide an overview of big data analytic accelerators and how they are being used by organizations to speed up deployments and solve big data problems sooner.
Success in Big Data requires both finding new signals buried in your data sources ("explore"), and using those signals to drive business value ("exploit").  Based on his background in Web Search and Internet Advertising, the speaker will describe these two aspects of Big Data and some of the success factors for each.
What can Big Data analysis tell us about human well-being? About how people cope with unemployment, rising food prices, or about peoples perceptions of HIV and other deadly diseases? A lot.
We often hear of private-sector companies' use of sophisticated analytics in search of profit.  What about the civic sector?  How are local governments using their data to improve city services?  This talk will explore how the Chicago Mayor's Office teamed up with civic-minded data scientists to pursue data mining solutions for some of the citys experimental projects.
This talk will cover what tools and techniques work and dont work well for data scientists working on Hadoop today and how Cloudera Impala increases the productivity of data science and analysis on Hadoop. Cloudera Impala builds upon experiences and leading edge technology from big data systems at Facebook, Google, and Yahoo.
This is a presentation that talks about how cluster design impacts performance. The presentation will cover several different design options and the trade offs in terms of performance and cost. The talk will also cover some of the tuning options based on the underlying hardware considerations.
There has been a lot of excitement lately about streaming approaches to handling Big Data such as Storm, S4, SQLStream, and InfoStreams. But many use cases can be better handled by low latency access with NoSQL databases and search indexing backed by scoring with batch analytics in Hadoop. We compare such integrated Big Data with streaming systems and look to the future.
The quantity of digital information collected and processed every day is growing at an exponential rate. To make sense of this mountain of data we can no longer afford the delays of batch processing systems. In this track we'll introduce Storm, a new, real-time analytic framework, and show how to use it to massively parallelize information analysis, to get instant results from your data.
Communicating Data Clearly describes how to draw clear, concise, accurate graphs that are easier to understand than many of the graphs one sees today.  The tutorial emphasizes how to avoid common mistakes that produce confusing or even misleading graphs. Graphs for one, two, three, and many variables are covered as well as general principles for creating effective graphs.
This workshop is a jumpstart lesson on how to get from a blank page and a pile of data to a useful data visualization. We'll focus on the design process, not specific tools. Bring your sample data and paper or a laptop; leave with new visualization ideas.
Big Data takes on the planets toughest challenge by analyzing weathers complex behavior. Using hundreds of terabytes of data and trillions of simulation datapoints, The Climate Corporation models weathers impact on crops to create customized insurance for farmers facing the financial impact of extreme weather.
Anne Milgram, Senior Fellow at the NYU Law Center on the Administration of Criminal Law Center.
How a traditional Spanish-language media company, made the strategic decision to build a robust analytics intelligence division to more effectively target the Hispanic market. Attendees will walk away with insights on how this traditional media company implemented a big data and MapReduce operations from the ground up.
Real estate used to be an industry that had large information asymmetry between professionals and consumers. Zillow has leveled the playing field through its living database on over 100 million homes.  Advanced statistical modeling gives consumers even more information and tools, such as the well-known Zestimate. Using data and analytics, Zillow has changed the real estate industry forever.
In this session, Kevin Foster, IBM Big Data Solution Architect, will provide an overview of big data analytic accelerators and how they are being used by organizations to speed up deployments and solve big data problems sooner.
Success in Big Data requires both finding new signals buried in your data sources ("explore"), and using those signals to drive business value ("exploit").  Based on his background in Web Search and Internet Advertising, the speaker will describe these two aspects of Big Data and some of the success factors for each.
What can Big Data analysis tell us about human well-being? About how people cope with unemployment, rising food prices, or about peoples perceptions of HIV and other deadly diseases? A lot.
We often hear of private-sector companies' use of sophisticated analytics in search of profit.  What about the civic sector?  How are local governments using their data to improve city services?  This talk will explore how the Chicago Mayor's Office teamed up with civic-minded data scientists to pursue data mining solutions for some of the citys experimental projects.
This talk will cover what tools and techniques work and dont work well for data scientists working on Hadoop today and how Cloudera Impala increases the productivity of data science and analysis on Hadoop. Cloudera Impala builds upon experiences and leading edge technology from big data systems at Facebook, Google, and Yahoo.
This is a presentation that talks about how cluster design impacts performance. The presentation will cover several different design options and the trade offs in terms of performance and cost. The talk will also cover some of the tuning options based on the underlying hardware considerations.
There has been a lot of excitement lately about streaming approaches to handling Big Data such as Storm, S4, SQLStream, and InfoStreams. But many use cases can be better handled by low latency access with NoSQL databases and search indexing backed by scoring with batch analytics in Hadoop. We compare such integrated Big Data with streaming systems and look to the future.
The quantity of digital information collected and processed every day is growing at an exponential rate. To make sense of this mountain of data we can no longer afford the delays of batch processing systems. In this track we'll introduce Storm, a new, real-time analytic framework, and show how to use it to massively parallelize information analysis, to get instant results from your data.
Python is the language of choice when it comes to integrating analytical components.  We will present a series of concepts and walkthroughs that illustrate how easy scientific computing is in Python, from machine learning and time series to spatial relationships and network analysis.
In this tutorial, well provide an introduction to an open source Map/Reduce library for R called RHadoop that makes Map/Reduce programming convenient and easy to understand for statistical modeling users. The session will cover the basics of RHadoop, common techniques and best practices, and some interactive real-world examples.
The Hadoop and data science communities have matured to the point now that common design patterns across domains are beginning to emerge. Now that Hadoop is maturing and momentum is gaining in the user base, the experienced users can start documenting design patterns that can be shared. In this talk, we'll talk about what makes up a MapReduce design pattern and give some examples.
Open-source developers all over the world contribute to millions of projects every day on GitHub: writing and reviewing code, filing bug reports and updating docs. Data from these events provides an amazing window into open source trends: project momentum, language adoption, community demographics, and more.
Proper tooling and good habits that maximize reproducibility are essential to being productive as a data scientist. From management of raw data to model version control, the entire workflow must be carefully controlled from end-to-end to produce quality research that scales with the quantity and complexity of data being analyzed.
In this talk techniques from mathematical financial models will be compared and contrasted with methods coming from machine learning. Specifically, we will discuss the concept of time series data, taking account of seasonality, how to avoid overfitting, continuous updating, and fitting a bayesian prior to your data science model. We will also discuss the question of when to use what tools.
An effective data science team looks a lot like an effective design team: brainstorming creative ideas, making prototypes, receiving feedback, telling stories, and deeply understanding the needs of others.
Amy O'Connor, Sr. Director of Nokia Analytics, together with her daughter and Nokia Intern, Danielle Dean, will share what makes a great data scientist, their different paths to acquiring the diverse skill sets that are needed and finally Amy will discuss how to spot, attract and train emerging data scientists in what is quickly becoming a heated market.
Data manipulation, cleaning, integration, and preparation can be one of the most time consuming parts of the data science process. In this talk I will discuss key points in the design and implementation of data structures and algorithms for structured data manipulation. It is an accumulation of lessons learned and experience building pandas, a widely-used Python data analysis toolkit.
This talk will describe how real-time learning can be used for advanced A/B testing as well as a variety of advertising and document targeting problems.  The crux of these applications is the Bayesian Bandit algorithm.  This algorithm is simple but provides state-of-the-art performance. This talk will be intuitive and practical, but not simple-minded.  All code examples are available on github.
Nearly a billion people actively create and modify nodes and their structured associations in the Facebook object graph. In this talk, Justin Moore describes how a small team within Facebook uses a combination of product, machine learning, and crowdsourcing to maintain and gain insight into this dataset.
Building a reliable data-driven solution to a complex business problem is like designing a pocket watch from scratch. At the heart of successful analytics is the art of decomposing the looming big objective into smaller components, each of which may have its own data feed, modeling technique and runtime constraint. We showcase this process on the example of M6Ds online display advertising.
How do you build and deploy predictive analytics into ongoing business processes so results can be used in real-time to improve operations? This is a common request, in applications ranging from machine-to-machine to oil & gas and utilities. Learn how to leverage all your data assets  including sensor data  to build and operationalize predictive models that improve business operations.
Julia is a high-level, high-performance dynamic language for efficient, large-scale scientific and technical computing, which provides simple, flexible primitives for distributed computing, out of the box. These primitives allow various approaches to distributed computation to be implemented succinctly and easily, with high performance, entirely in Julia.
Python is the language of choice when it comes to integrating analytical components.  We will present a series of concepts and walkthroughs that illustrate how easy scientific computing is in Python, from machine learning and time series to spatial relationships and network analysis.
In this tutorial, well provide an introduction to an open source Map/Reduce library for R called RHadoop that makes Map/Reduce programming convenient and easy to understand for statistical modeling users. The session will cover the basics of RHadoop, common techniques and best practices, and some interactive real-world examples.
The Hadoop and data science communities have matured to the point now that common design patterns across domains are beginning to emerge. Now that Hadoop is maturing and momentum is gaining in the user base, the experienced users can start documenting design patterns that can be shared. In this talk, we'll talk about what makes up a MapReduce design pattern and give some examples.
Open-source developers all over the world contribute to millions of projects every day on GitHub: writing and reviewing code, filing bug reports and updating docs. Data from these events provides an amazing window into open source trends: project momentum, language adoption, community demographics, and more.
Proper tooling and good habits that maximize reproducibility are essential to being productive as a data scientist. From management of raw data to model version control, the entire workflow must be carefully controlled from end-to-end to produce quality research that scales with the quantity and complexity of data being analyzed.
In this talk techniques from mathematical financial models will be compared and contrasted with methods coming from machine learning. Specifically, we will discuss the concept of time series data, taking account of seasonality, how to avoid overfitting, continuous updating, and fitting a bayesian prior to your data science model. We will also discuss the question of when to use what tools.
An effective data science team looks a lot like an effective design team: brainstorming creative ideas, making prototypes, receiving feedback, telling stories, and deeply understanding the needs of others.
Amy O'Connor, Sr. Director of Nokia Analytics, together with her daughter and Nokia Intern, Danielle Dean, will share what makes a great data scientist, their different paths to acquiring the diverse skill sets that are needed and finally Amy will discuss how to spot, attract and train emerging data scientists in what is quickly becoming a heated market.
Data manipulation, cleaning, integration, and preparation can be one of the most time consuming parts of the data science process. In this talk I will discuss key points in the design and implementation of data structures and algorithms for structured data manipulation. It is an accumulation of lessons learned and experience building pandas, a widely-used Python data analysis toolkit.
This talk will describe how real-time learning can be used for advanced A/B testing as well as a variety of advertising and document targeting problems.  The crux of these applications is the Bayesian Bandit algorithm.  This algorithm is simple but provides state-of-the-art performance. This talk will be intuitive and practical, but not simple-minded.  All code examples are available on github.
Nearly a billion people actively create and modify nodes and their structured associations in the Facebook object graph. In this talk, Justin Moore describes how a small team within Facebook uses a combination of product, machine learning, and crowdsourcing to maintain and gain insight into this dataset.
Building a reliable data-driven solution to a complex business problem is like designing a pocket watch from scratch. At the heart of successful analytics is the art of decomposing the looming big objective into smaller components, each of which may have its own data feed, modeling technique and runtime constraint. We showcase this process on the example of M6Ds online display advertising.
How do you build and deploy predictive analytics into ongoing business processes so results can be used in real-time to improve operations? This is a common request, in applications ranging from machine-to-machine to oil & gas and utilities. Learn how to leverage all your data assets  including sensor data  to build and operationalize predictive models that improve business operations.
Julia is a high-level, high-performance dynamic language for efficient, large-scale scientific and technical computing, which provides simple, flexible primitives for distributed computing, out of the box. These primitives allow various approaches to distributed computation to be implemented succinctly and easily, with high performance, entirely in Julia.
Apache Hadoop is enabling companies across many different industries that need to process and analyze large data sets.  In this tutorial you will learn why and how people are using Hadoop and related technologies like Hive, Pig and HBase.
This tutorial will help participants understand why distributed search is important and teach them how to use the landscape of tools available. Based on our hands-on experience at NetApp, we will lead a tutorial session that will teach participants how to setup and use search technologies such as Apache Solr and Lucene to enable real-time Big Data analytics with Hadoop, HBase, and other NoSQL.
In this session we will discuss how subjectivity can be encoded in data, and how this data can be used to help users experience a city more gracefully.  We'll create maps and visualizations that re-enforce the ways users engage with cities and augment these experiences using social and crowd-sourced data sources, analytics and both artistic and literal visualization to convey this information.
Data has been locked in a mindset of rows and columns. Our brains are trapped by database schemas. To get out of that predisposition and communicate visually requires new thinking. This session covers techniques for reframing our thoughts about data, how to describe data, forming a narrative, and coming up with visual solutions.
hGraph is a compelling, standardized visual representation of a patient's health status for clinicians and patients. Designed to increase awareness of the individual's factors that can affect one's health and lead to improved outcomes, hGraph aggregates all of an individual's health metrics in one location, in a single picture.
MLB captures 10Tb of game data every year. While valuable data, lessons were quickly learned that effective use of this data required different visual front-ends for fans, players, coaches and scouts. The ability to adapt and address different audiences helped the success of this project and can help other big data projects.
Attendees with learn practical examples how to build a collaborative environment that accelerates the value of big data, with the goal of making data part of every conversation.
"Data visualization" means different things to different people. Some say that to be effective, visualizations need to be clear, concise and accurate.  Others say that to be effective, visualizations need to be eye-catching, engaging, and innovative. Naomi Robbins will moderate a panel composed of Jon Peltier and Nigel Homes.
How do you build technology to empower designers to create data visualizations? This talk is about thought principles and technologies exploring a few ways in which we can do so.
This session presents a simple analytical and generative toolkit for interface design. It provides designers with an effective starting point for creating satisfying and relevant user experiences for Big Data and discovery interfaces. The toolkit helps designers understand and describe users' activities and needs, and then define and design the interactions and interfaces necessary.
Apache Hadoop is enabling companies across many different industries that need to process and analyze large data sets.  In this tutorial you will learn why and how people are using Hadoop and related technologies like Hive, Pig and HBase.
This tutorial will help participants understand why distributed search is important and teach them how to use the landscape of tools available. Based on our hands-on experience at NetApp, we will lead a tutorial session that will teach participants how to setup and use search technologies such as Apache Solr and Lucene to enable real-time Big Data analytics with Hadoop, HBase, and other NoSQL.
In this session we will discuss how subjectivity can be encoded in data, and how this data can be used to help users experience a city more gracefully.  We'll create maps and visualizations that re-enforce the ways users engage with cities and augment these experiences using social and crowd-sourced data sources, analytics and both artistic and literal visualization to convey this information.
Data has been locked in a mindset of rows and columns. Our brains are trapped by database schemas. To get out of that predisposition and communicate visually requires new thinking. This session covers techniques for reframing our thoughts about data, how to describe data, forming a narrative, and coming up with visual solutions.
hGraph is a compelling, standardized visual representation of a patient's health status for clinicians and patients. Designed to increase awareness of the individual's factors that can affect one's health and lead to improved outcomes, hGraph aggregates all of an individual's health metrics in one location, in a single picture.
MLB captures 10Tb of game data every year. While valuable data, lessons were quickly learned that effective use of this data required different visual front-ends for fans, players, coaches and scouts. The ability to adapt and address different audiences helped the success of this project and can help other big data projects.
Attendees with learn practical examples how to build a collaborative environment that accelerates the value of big data, with the goal of making data part of every conversation.
"Data visualization" means different things to different people. Some say that to be effective, visualizations need to be clear, concise and accurate.  Others say that to be effective, visualizations need to be eye-catching, engaging, and innovative. Naomi Robbins will moderate a panel composed of Jon Peltier and Nigel Homes.
How do you build technology to empower designers to create data visualizations? This talk is about thought principles and technologies exploring a few ways in which we can do so.
This session presents a simple analytical and generative toolkit for interface design. It provides designers with an effective starting point for creating satisfying and relevant user experiences for Big Data and discovery interfaces. The toolkit helps designers understand and describe users' activities and needs, and then define and design the interactions and interfaces necessary.
This tutorial will explore the tools and techniques you need to ensure that your MapReduce applications are both correct and efficient.  You'll learn how to do unit testing, integration testing and performance testing for your Hadoop jobs, as well as how to intepret diagnostic information to isolate and solve problems in your code.
This hands-on tutorial teaches you how to setup and use Hive, a high-level, data warehouse tool for Hadoop. Hive provides a SQL-like query language, HiveQL, that is easy to learn for people with prior SQL experience, making Hive attractive for data warehousing teams. Hive leverages the power of Hadoop for working with massive data sets without requiring expertise in MapReduce programming.
How does Opower deliver insights to millions of households with big (and getting bigger) data? I discuss how to effectively use Hadoop, integrate it with R and Python, and harness an engaged workforce to solve data science and efficiency problems.
DCGS-Army Standard Cloud Multimedia (DSC-M) is focused on the Full Motion Video aspects of our Cloudera Hadoop-based implementation for the U.S. Army.
Imagine the social graph where personal relationships are replaced by commercial relationships based on real financial data. Imagine the possibilities for small businesses to grow, connect, transact and prosper.
ODS is Facebook's internal large-scale monitoring system. HBase turns out be to a good fit for its workload and solves some manageability and scalability challenges with the previous MySQL based setup. We would like to share a series of valuable experiences learnt from building this large scale realtime system based on HBase.
A look at using Hadoop, HBase and other technologies to bring together and process health data from many sources in real time.  This includes techniques for dealing with data that's incomplete or out-of-order when it arrives, merging bulk and real-time data sets, and creating search indexes and data models to enable better health care.
Your DNA, written out as a string of G, A, T, and C, is about three and half gigabytes long.  That string is about 99.9% identical to an arbitrary Reference Genome.  Practically all of those differences are harmless, but a a tiny fraction can cause disease, contribute to disease, or just change how your body reacts to drugs.  We're using Hadoop to find the variants that actually matter.
Building analytical models is a process of trial and error. Often it makes sense to sample down a data set so that numerous methods and new variables can be tried quickly. Consider moving to the entire data set with Hadoop only after the lessons gleaned from the failures have been incorporated into a few candidate models.
Many companies use Hadoop for traditional data warehousing applications including data analysis, business reporting, and data storage. But you can use Hadoop to do much more. In this talk, we'll describe how LinkedIn uses Hadoop to create new content, develop recommendations, and send messages to users.
This tutorial will explore the tools and techniques you need to ensure that your MapReduce applications are both correct and efficient.  You'll learn how to do unit testing, integration testing and performance testing for your Hadoop jobs, as well as how to intepret diagnostic information to isolate and solve problems in your code.
This hands-on tutorial teaches you how to setup and use Hive, a high-level, data warehouse tool for Hadoop. Hive provides a SQL-like query language, HiveQL, that is easy to learn for people with prior SQL experience, making Hive attractive for data warehousing teams. Hive leverages the power of Hadoop for working with massive data sets without requiring expertise in MapReduce programming.
How does Opower deliver insights to millions of households with big (and getting bigger) data? I discuss how to effectively use Hadoop, integrate it with R and Python, and harness an engaged workforce to solve data science and efficiency problems.
DCGS-Army Standard Cloud Multimedia (DSC-M) is focused on the Full Motion Video aspects of our Cloudera Hadoop-based implementation for the U.S. Army.
Imagine the social graph where personal relationships are replaced by commercial relationships based on real financial data. Imagine the possibilities for small businesses to grow, connect, transact and prosper.
ODS is Facebook's internal large-scale monitoring system. HBase turns out be to a good fit for its workload and solves some manageability and scalability challenges with the previous MySQL based setup. We would like to share a series of valuable experiences learnt from building this large scale realtime system based on HBase.
A look at using Hadoop, HBase and other technologies to bring together and process health data from many sources in real time.  This includes techniques for dealing with data that's incomplete or out-of-order when it arrives, merging bulk and real-time data sets, and creating search indexes and data models to enable better health care.
Your DNA, written out as a string of G, A, T, and C, is about three and half gigabytes long.  That string is about 99.9% identical to an arbitrary Reference Genome.  Practically all of those differences are harmless, but a a tiny fraction can cause disease, contribute to disease, or just change how your body reacts to drugs.  We're using Hadoop to find the variants that actually matter.
Building analytical models is a process of trial and error. Often it makes sense to sample down a data set so that numerous methods and new variables can be tried quickly. Consider moving to the entire data set with Hadoop only after the lessons gleaned from the failures have been incorporated into a few candidate models.
Many companies use Hadoop for traditional data warehousing applications including data analysis, business reporting, and data storage. But you can use Hadoop to do much more. In this talk, we'll describe how LinkedIn uses Hadoop to create new content, develop recommendations, and send messages to users.
New techniques like Hadoop are leading the way to provide a scalable and cost effective solution.  This session reviews the technical requirements for a low latency multi-tenant 'big-data' cluster - one where different lines of business and multiple applications can be run with assured SLAs, resulting in higher ROI for these clusters.
Join us for a live demonstration of how you can leverage a data science platform, an open-source model, internal and external data, analytics tools, and visualization using Hadoop. See how unprecedented access to data scientists can deliver entirely new levels of insight to push the boundaries of whats possible. Find out what you can do NOW to move your data science efforts forward.
Business users' attitude to data is changing rapidly  remember when building an EDW was all consuming? Now Big Data is edging the EDW to the side or likely into obscurity. Is this good or bad? How do you bring the values and software investment surrounding the EDW to the wild west of Big Data?
Hadoop continues to climb the IT hype cycle.  Along the way, plenty of truth, myth and folklore has been created around Hadoop's business capabilities and technical infrastructure requirements. Come hear NetApps real-world discoveries about Hadoop and find out what myths need retiring, as well as which truths need uncovering.
This session will delve into the MapReduce computation paradigm, introduced by Google and widely adopted via the open-source Hadoop platform, combined with commodity hardware to execute computation at the storage node where data exists.
Details to come...
Kicking off with an Ignite-style presentation on the growing importance of our topic, this panel will feature multiple perspectives on what K-12 education can learn from Big Data efforts underway in other industries
This presentation will provide a detailed understanding of the latest techniques in entity resolution and simplified training of machine learning models and the direct impact on the quality of a comprehensive predictive analytics solution.  Specific use cases in the financial services and intelligence communities will be featured.
This presentation provides an overview of how to comprehensively address big data, including emerging strategies for information management, analytics, and high performance computing.
Big Data is attracting strong interest from technologists and business users alike.  Yet few organizations can actually reap the benefits of Big Data today because the barriers to entry are still too high.  Existing tools are complex and require deep expertise in Hadoop and Data Analysis that are both in short supply.
Over the next decade, organizations will need to absorb, analyze, and act upon 50 times more data than they do today. To do this, they will need a scalable infrastructure that can support data-driven discovery and decision-making in real-time.
Google pioneered the use of the MapReduce framework and inspired the creation of Hadoop through their 2004 white paper. To understand the future of Hadoop and the future of Big Data, its important to understand how Google processes and analyzes Big Data internally.
This tutorial will provide novice users with an overview of a range of common tools use for data cleaning and analysis - including Microsoft Excel, Google Refine, Python and R - along with their relative strengths and weaknesses. Attendees will not only learn useful new skills, and they will know what kind of expertise they need to seek out for help with more complex tasks.
Apache Flume (incubating) is a scalable, reliable, fault-tolerant, distributed system designed to collect and transfer massive amounts of event data from disparate systems into some storage tier such as Hadoop HDFS. In this tutorial we show how to easily build a large-scale data collection and transfer system in a scalable way using Flume NG, the next generation of Flume.
New techniques like Hadoop are leading the way to provide a scalable and cost effective solution.  This session reviews the technical requirements for a low latency multi-tenant 'big-data' cluster - one where different lines of business and multiple applications can be run with assured SLAs, resulting in higher ROI for these clusters.
Join us for a live demonstration of how you can leverage a data science platform, an open-source model, internal and external data, analytics tools, and visualization using Hadoop. See how unprecedented access to data scientists can deliver entirely new levels of insight to push the boundaries of whats possible. Find out what you can do NOW to move your data science efforts forward.
Business users' attitude to data is changing rapidly  remember when building an EDW was all consuming? Now Big Data is edging the EDW to the side or likely into obscurity. Is this good or bad? How do you bring the values and software investment surrounding the EDW to the wild west of Big Data?
Hadoop continues to climb the IT hype cycle.  Along the way, plenty of truth, myth and folklore has been created around Hadoop's business capabilities and technical infrastructure requirements. Come hear NetApps real-world discoveries about Hadoop and find out what myths need retiring, as well as which truths need uncovering.
This session will delve into the MapReduce computation paradigm, introduced by Google and widely adopted via the open-source Hadoop platform, combined with commodity hardware to execute computation at the storage node where data exists.
Details to come...
Kicking off with an Ignite-style presentation on the growing importance of our topic, this panel will feature multiple perspectives on what K-12 education can learn from Big Data efforts underway in other industries
This presentation will provide a detailed understanding of the latest techniques in entity resolution and simplified training of machine learning models and the direct impact on the quality of a comprehensive predictive analytics solution.  Specific use cases in the financial services and intelligence communities will be featured.
This presentation provides an overview of how to comprehensively address big data, including emerging strategies for information management, analytics, and high performance computing.
Big Data is attracting strong interest from technologists and business users alike.  Yet few organizations can actually reap the benefits of Big Data today because the barriers to entry are still too high.  Existing tools are complex and require deep expertise in Hadoop and Data Analysis that are both in short supply.
Over the next decade, organizations will need to absorb, analyze, and act upon 50 times more data than they do today. To do this, they will need a scalable infrastructure that can support data-driven discovery and decision-making in real-time.
Google pioneered the use of the MapReduce framework and inspired the creation of Hadoop through their 2004 white paper. To understand the future of Hadoop and the future of Big Data, its important to understand how Google processes and analyzes Big Data internally.
This tutorial will provide novice users with an overview of a range of common tools use for data cleaning and analysis - including Microsoft Excel, Google Refine, Python and R - along with their relative strengths and weaknesses. Attendees will not only learn useful new skills, and they will know what kind of expertise they need to seek out for help with more complex tasks.
Apache Flume (incubating) is a scalable, reliable, fault-tolerant, distributed system designed to collect and transfer massive amounts of event data from disparate systems into some storage tier such as Hadoop HDFS. In this tutorial we show how to easily build a large-scale data collection and transfer system in a scalable way using Flume NG, the next generation of Flume.
HBase is one of the more popular open source NoSQL databases that have cropped up over the last few years. Building applications that use HBase effectively is challenging. This tutorial is geared towards teaching the basics of building applications using HBase and covers concepts that a developer should know while using HBase as a backend store for their application.
A successful big data analytic project is not just about selecting the right algorithm for building a predictive model, but also about how to deploy the model efficiently into operational systems, how to evaluate the effectiveness of the model, and how to continuously improve it.  In this tutorial we cover best practices for each of these phases in the life cycle of a predictive model.
With traditional ETL (extract-transform-load) you need to decide how you want to transform and store the data before it arrives. Hadoop allows a much more agile pipeline  store the raw data, add a little metadata, and iteratively pull from it at whatever level of detail is needed right now by the application. We'll explore this approach and show you how you can start using it today
While many of the necessary building blocks for data processing exist within the Hadoop ecosystem, it can be a challenge to assemble them as a production ETL platform. This presentation covers one approach to data ingest, organization, format selection, process orchestration, and external system integration, based on collective experience acquired across many production Hadoop deployments.
In this session well discuss our experience extending Hadoop development to new platforms and languages, and key aspects of using non-JVM languages in the Hadoop environment.
Apache Hadoop MapReduce has undergone a complete re-haul to emerge as Apache Hadoop YARN, a generic compute fabric to support MapReduce and other application paradigms. This really changes the game to recast Hadoop as a much more powerful data-processing system making Hadoop very different from itself 12 months ago. Now, ever wonder what it might look like in 12 months or 24 months or longer?
Hadoop 2.0 offers significant HDFS improvements: new append-pipeline, federation, wire compatibility, NameNode HA, performance improvements, etc. We describe the new features and their benefits   and our plans for HDFS over the next year which includes Snapshots, Disaster recovery, RAID, performance improvements etc. We conclude with some of the misconceptions and myths about HDFS.
The initial implementation of a highly-available HDFS NameNode successfully removed all single points of failure from HDFS. This talk discusses further improvements to this work, including automatic failure detection and failover initiation, as well as removing the dependency on an HA NFS filer.
Performing investigative analysis on data stored in HBase is challenging. Most tools operate on files stored in HDFS, and interact poorly with HBase's data model. This talk will describe characteristics of data in HBase and exploratory analysis patterns. We will describe best practices for modeling this data efficiently and survey tools and techniques appropriate for data science teams.
As Apache HBase matures, the community has augmented it with new features that are considered hard requirements for many enterprises. We will discuss how the upcoming HBase 0.96 release addresses many of these shortcomings by introducing new features that will help the administrator minimize downtime, monitor performance, control access to the system, and geo-replicate data across data centers.
HBase is one of the more popular open source NoSQL databases that have cropped up over the last few years. Building applications that use HBase effectively is challenging. This tutorial is geared towards teaching the basics of building applications using HBase and covers concepts that a developer should know while using HBase as a backend store for their application.
A successful big data analytic project is not just about selecting the right algorithm for building a predictive model, but also about how to deploy the model efficiently into operational systems, how to evaluate the effectiveness of the model, and how to continuously improve it.  In this tutorial we cover best practices for each of these phases in the life cycle of a predictive model.
With traditional ETL (extract-transform-load) you need to decide how you want to transform and store the data before it arrives. Hadoop allows a much more agile pipeline  store the raw data, add a little metadata, and iteratively pull from it at whatever level of detail is needed right now by the application. We'll explore this approach and show you how you can start using it today
While many of the necessary building blocks for data processing exist within the Hadoop ecosystem, it can be a challenge to assemble them as a production ETL platform. This presentation covers one approach to data ingest, organization, format selection, process orchestration, and external system integration, based on collective experience acquired across many production Hadoop deployments.
In this session well discuss our experience extending Hadoop development to new platforms and languages, and key aspects of using non-JVM languages in the Hadoop environment.
Apache Hadoop MapReduce has undergone a complete re-haul to emerge as Apache Hadoop YARN, a generic compute fabric to support MapReduce and other application paradigms. This really changes the game to recast Hadoop as a much more powerful data-processing system making Hadoop very different from itself 12 months ago. Now, ever wonder what it might look like in 12 months or 24 months or longer?
Hadoop 2.0 offers significant HDFS improvements: new append-pipeline, federation, wire compatibility, NameNode HA, performance improvements, etc. We describe the new features and their benefits   and our plans for HDFS over the next year which includes Snapshots, Disaster recovery, RAID, performance improvements etc. We conclude with some of the misconceptions and myths about HDFS.
The initial implementation of a highly-available HDFS NameNode successfully removed all single points of failure from HDFS. This talk discusses further improvements to this work, including automatic failure detection and failover initiation, as well as removing the dependency on an HA NFS filer.
Performing investigative analysis on data stored in HBase is challenging. Most tools operate on files stored in HDFS, and interact poorly with HBase's data model. This talk will describe characteristics of data in HBase and exploratory analysis patterns. We will describe best practices for modeling this data efficiently and survey tools and techniques appropriate for data science teams.
As Apache HBase matures, the community has augmented it with new features that are considered hard requirements for many enterprises. We will discuss how the upcoming HBase 0.96 release addresses many of these shortcomings by introducing new features that will help the administrator minimize downtime, monitor performance, control access to the system, and geo-replicate data across data centers.
Googles Dremel is a scalable, interactive ad-hoc query system capable of running SQL-like queries over trillion-row tables in seconds. BigQuery is the externalization of this technology as a REST API and web app. This session will discuss the capabilities of Dremel and dive into the design challenges necessary to make this technology accessible and performant for developers and business users.
OMGPOPs Draw Something broke all records when it went viral, skyrocketing to more than 50 million downloads and billions of drawings within a few weeks of launch  with no downtime. This session highlights the application architecture and data management technology that enabled this growth, and provides a real-time data management model for developers of any interactive web application.
Hadoop is considered THE technology for addressing Big Data. While it shines as a processing platform, it does not respond anywhere close to "human time". In developing our solution, we needed the ability to query across billions of rows in seconds. Hear how and why we developed Druid, our distributed, in-memory OLAP data store after investigating various commercial and open source alternatives.
The big data movement has highlighted the value of historical information, and storage is readily available, so why are you still using an update-in-place database? In this talk we'll deconstruct the traditional monolithic database with an eye towards leveraging the scaling properties of distributed architectures, while meeting the business needs for complete historical information.
Trecul is a dataflow system that powers Akamai's Online Adversting business, processing billions of events hourly.  Trecul is built on top of HDFS & Hadoop Pipes to achieve fantastic runtime performance. We'll talk about it's use of LLVM-based JIT compilation so everything runs as native C++ code, no Java and no runtime interpreter. Akamai has open-sourced Trecul and it is available on Github.
The exponential growth of graph-based data analysis is fueling the need for machine learning. Recently, frameworks have emerged to perform these computations at large scale. But, feeding data to these frameworks is a challenge in itself. This talk introduces the GraphBuilder library for Hadoop, which makes the job easier for programmers. Several case studies showacse the utility of library.
To unlock the value of Big Data, analytics must be applied. Some enterprises hire platoons of data analysts but many others can't afford to pring on such skilled and expensive resources. How do those businesses uncover opportunity and insight within Big Data assets? They use analytic tools that offload some data discovery to business professionals or deploy intelligent analytic appications.
Our Data Science tech stack has shifted from best-of-breed, "classic" business intelligence technologies to a hybrid environment, fully leveraging Hadoop and other Big Data solutions. Our philosophy has also evolved, now distilled in thinking and practice into "data science as a service". Why did we do it? What does it look like? What are the benefits? Come find out.
Googles Dremel is a scalable, interactive ad-hoc query system capable of running SQL-like queries over trillion-row tables in seconds. BigQuery is the externalization of this technology as a REST API and web app. This session will discuss the capabilities of Dremel and dive into the design challenges necessary to make this technology accessible and performant for developers and business users.
OMGPOPs Draw Something broke all records when it went viral, skyrocketing to more than 50 million downloads and billions of drawings within a few weeks of launch  with no downtime. This session highlights the application architecture and data management technology that enabled this growth, and provides a real-time data management model for developers of any interactive web application.
Hadoop is considered THE technology for addressing Big Data. While it shines as a processing platform, it does not respond anywhere close to "human time". In developing our solution, we needed the ability to query across billions of rows in seconds. Hear how and why we developed Druid, our distributed, in-memory OLAP data store after investigating various commercial and open source alternatives.
The big data movement has highlighted the value of historical information, and storage is readily available, so why are you still using an update-in-place database? In this talk we'll deconstruct the traditional monolithic database with an eye towards leveraging the scaling properties of distributed architectures, while meeting the business needs for complete historical information.
Trecul is a dataflow system that powers Akamai's Online Adversting business, processing billions of events hourly.  Trecul is built on top of HDFS & Hadoop Pipes to achieve fantastic runtime performance. We'll talk about it's use of LLVM-based JIT compilation so everything runs as native C++ code, no Java and no runtime interpreter. Akamai has open-sourced Trecul and it is available on Github.
The exponential growth of graph-based data analysis is fueling the need for machine learning. Recently, frameworks have emerged to perform these computations at large scale. But, feeding data to these frameworks is a challenge in itself. This talk introduces the GraphBuilder library for Hadoop, which makes the job easier for programmers. Several case studies showacse the utility of library.
To unlock the value of Big Data, analytics must be applied. Some enterprises hire platoons of data analysts but many others can't afford to pring on such skilled and expensive resources. How do those businesses uncover opportunity and insight within Big Data assets? They use analytic tools that offload some data discovery to business professionals or deploy intelligent analytic appications.
Our Data Science tech stack has shifted from best-of-breed, "classic" business intelligence technologies to a hybrid environment, fully leveraging Hadoop and other Big Data solutions. Our philosophy has also evolved, now distilled in thinking and practice into "data science as a service". Why did we do it? What does it look like? What are the benefits? Come find out.
Evaluating an experiment amidst the shifting landscape of continuous deployment is a difficult task as traditional methods of monitoring operational metrics dont provide enough information to make product-level decisions. This talk will focus on the framework that we have built to solve this problem - from data logging to the final analysis that drive decision making and everything in between.
Managing Hadoop clusters to meet business needs can be challenging. Learn how Monsanto has effectively tamed the elephant using Cloudera Manager.
Apache Pig makes Apache Hadoop easier to use thanks to its high-level data flow language, Pig Latin. In this talk, we will discuss common data analysis tasks, the choices one can make while writing a query and impact of each on performance. The core principles behind the optimization recommendations shared during this presentation are applicable to all MapReduce applications.
By applying machine learning algorithms to large aggregations of spatiotemporal data we can better understand how people interact with cities and build novel tools to help people navigate the real-world.
Evaluating an experiment amidst the shifting landscape of continuous deployment is a difficult task as traditional methods of monitoring operational metrics dont provide enough information to make product-level decisions. This talk will focus on the framework that we have built to solve this problem - from data logging to the final analysis that drive decision making and everything in between.
Managing Hadoop clusters to meet business needs can be challenging. Learn how Monsanto has effectively tamed the elephant using Cloudera Manager.
Apache Pig makes Apache Hadoop easier to use thanks to its high-level data flow language, Pig Latin. In this talk, we will discuss common data analysis tasks, the choices one can make while writing a query and impact of each on performance. The core principles behind the optimization recommendations shared during this presentation are applicable to all MapReduce applications.
By applying machine learning algorithms to large aggregations of spatiotemporal data we can better understand how people interact with cities and build novel tools to help people navigate the real-world.
Since the first human scrawled an image on a cave wall, the brain has been processing petabytes of data. Today, we're passing through an historical threshold where big data is leaching out of our braincases into the disembodied cloud. For the first time in human existence, we can "think" outside of our brains. What does this mean for privacy, morality, ethics, and the law?
Being a data-driven organization is core to developing and growing a successful Internet company today. This session will delve into the data ownership implications and considerations product teams need to take into account as they build products and services aimed at growing their user base and scaling their companies business.
Jonathan Alexander, VP Engineering at Vocalocity and the author of Codermetrics (OReilly 2011) and Moneyball for Software Engineering (OReilly Radar 2011/2012) presents new ideas on how to gather data and use analytics to create more effective software development teams.
Since the first human scrawled an image on a cave wall, the brain has been processing petabytes of data. Today, we're passing through an historical threshold where big data is leaching out of our braincases into the disembodied cloud. For the first time in human existence, we can "think" outside of our brains. What does this mean for privacy, morality, ethics, and the law?
Being a data-driven organization is core to developing and growing a successful Internet company today. This session will delve into the data ownership implications and considerations product teams need to take into account as they build products and services aimed at growing their user base and scaling their companies business.
Jonathan Alexander, VP Engineering at Vocalocity and the author of Codermetrics (OReilly 2011) and Moneyball for Software Engineering (OReilly Radar 2011/2012) presents new ideas on how to gather data and use analytics to create more effective software development teams.
In this talk we will explore how businesses are marrying human judgment with large scale processing, improving the accuracy of Big Data analytics without sacrificing efficiency or scalability. Real-world examples will be discussed in which Hadoop and crowdsourcing are combined through the Amazon Web Services technologies Elastic MapReduce and Mechanical Turk.
In this session, we will introduce Knitting Boar, an open-source Java library for performing distributed online learning on a Hadoop cluster under YARN. We will give an overview of how Woven Wabbit works and examine the lessons learned from YARN application construction.
Start on low heat with a base of Hadoop; map, then reduce. Flavor, to taste, with Scala's concise, functional syntax and collections library. Simmer with some Pig bones: a tuple model and high-level join and aggregation operators. Mix in Cascading to hold everything together and boil until it's very, very hot, and you get Scalding, a new API for MapReduce out of Twitter.
Explore the network capabilities and architecture necessary to build multi-petabyte clusters. Compare and contrast different networking architectures for Big Data. Use real-world case studies from many of the largest HDFS deployments. Explain how topology aware file systems interact with the network substrate. Discuss differences in architecture based on workload profile and data set size
In this talk we will explore how businesses are marrying human judgment with large scale processing, improving the accuracy of Big Data analytics without sacrificing efficiency or scalability. Real-world examples will be discussed in which Hadoop and crowdsourcing are combined through the Amazon Web Services technologies Elastic MapReduce and Mechanical Turk.
In this session, we will introduce Knitting Boar, an open-source Java library for performing distributed online learning on a Hadoop cluster under YARN. We will give an overview of how Woven Wabbit works and examine the lessons learned from YARN application construction.
Start on low heat with a base of Hadoop; map, then reduce. Flavor, to taste, with Scala's concise, functional syntax and collections library. Simmer with some Pig bones: a tuple model and high-level join and aggregation operators. Mix in Cascading to hold everything together and boil until it's very, very hot, and you get Scalding, a new API for MapReduce out of Twitter.
Explore the network capabilities and architecture necessary to build multi-petabyte clusters. Compare and contrast different networking architectures for Big Data. Use real-world case studies from many of the largest HDFS deployments. Explain how topology aware file systems interact with the network substrate. Discuss differences in architecture based on workload profile and data set size
An increasing number of organizations are embracing data to drive intelligent decisions. For many industries, this is a monumental shift in method and culture. Data communication strategies come in many flavors, from static metric reports to immersive data experiences. In this session I present a user-centered framework for designing or evaluating data delivery methods.
You want to publish your data for clients, developers or the general public to use and enjoy. But which file formats to use? Which standards? How to provide an API? Should you visualize the data? And if so, how? DataMarket has been on the receiving end of data from many of the World's key data providers and is now helping leading information companies publishing theirs. Here we share our findings.
As data scientists, we encounter large networks all the time. Recommendations, social ties, transactions, and other types of data are naturally represented as networks. To understand these networks, metrics help, but visualization is crucial. This talk will focus on tools, techniques, and frameworks to visualize networks cleanly, avoiding or at least minimizing hairballs.
Advances in browser and mobile technologies have made the visualization and interaction of data on web a viable alternative to traditional tools used to visually explore data. Panelists will discuss the current state of web data visualization, as well as novel approaches made possible by recent advances.
An increasing number of organizations are embracing data to drive intelligent decisions. For many industries, this is a monumental shift in method and culture. Data communication strategies come in many flavors, from static metric reports to immersive data experiences. In this session I present a user-centered framework for designing or evaluating data delivery methods.
You want to publish your data for clients, developers or the general public to use and enjoy. But which file formats to use? Which standards? How to provide an API? Should you visualize the data? And if so, how? DataMarket has been on the receiving end of data from many of the World's key data providers and is now helping leading information companies publishing theirs. Here we share our findings.
As data scientists, we encounter large networks all the time. Recommendations, social ties, transactions, and other types of data are naturally represented as networks. To understand these networks, metrics help, but visualization is crucial. This talk will focus on tools, techniques, and frameworks to visualize networks cleanly, avoiding or at least minimizing hairballs.
Advances in browser and mobile technologies have made the visualization and interaction of data on web a viable alternative to traditional tools used to visually explore data. Panelists will discuss the current state of web data visualization, as well as novel approaches made possible by recent advances.
Maximize the value of data stored in Hadoop via operational and ad-hoc reporting, highly interactive analysis, advanced visualizations and dashboards
PayPal utilizes Hadoop as a cost-effective data platform to handle growing data volumes. Hadoop along with other traditional data platforms serves different business needs at PayPal for customer sentiment analysis, fraud detection, market segmentation, etc. PayPal will share some early experiences with Informatica on Hadoop to move & integrate data on Hadoop & between different data platforms
Richard Just, Big Data Program Manager at Capital One Labs, will share his experience using Hadoop and Platfora software to analyze several aspects of their business, including the adoption of their mobile application. The final solution produced an interactive, self-service web-based BI access to the data.
This session explores the benefits and implications of virtualizing Hadoop and highlights several VMware initiatives aimed at bridging Hadoop and virtualization.
The convergence of Analytics and the Cloud creates an interesting opportunity to solve many Big Data challenges that were previously untenable.  Alteryx has historically served retailers and consumer brands on optimizing merchandising and store operations decisions with its Strategic Analytics product.
Big data is everywhere, and it is increasingly complex and growing quickly, rendering manual and legacy approaches obsolete.  Organizations can only realize the business value of big data with a meaning based platform technology that automatically understands all data, structured and unstructured, in real time.  Join this session to learn more about Big Data and the technologies around it.
Nokias Big Data analytics service is a strategic multi-tenant, multi-petabyte platform that executes 10,000 jobs each day.  It is made up of technologies that provide location content processing, ETL, ad-hoc SQL, dashboards and advanced analytics, including Calpont InfiniDB for SQL, Scribe, REST, Hadoop, and R. This talk discusses the platform, motivations behind design choices, and challenges.
It's not easy doing predictive analytics on Hadoop, with few tools that make it easier or more scalable than writing code from scratch.  Join us to discuss a new paradigm that addresses the need for a scalable, powerful solution  one that is purpose-built for Big Data yet is easy to use  illustrated by a demonstration of predictive analytics run on the largest public Hadoop cluster in the world.
How do you keep up with the velocity and variety of data streaming in from the operational systems that power your business? What about getting analytics on your data even before you persist and replicate it?
Opposites attract and thats the case with Hadoop and Enterprise Data Warehouses. Both have a role to play in your Big Data projects. This session explores the various approaches to marrying Hadoop to your EDW, and why youll want to do that in the first place.
In this joint session, experts from Cisco and Cloudera reveal the fundamental design considerations of Hadoop in the Enterprise Data Center. Drawing from lessons learned in the real world, they'll share best practices from deployments of Cloudera's Hadoop distribution alongside Cisco's networking components.
Monitoring thousands of servers generates a lot of data.  Many organizations trying to harness the power of big data struggle with the same types of challenges as Rackspace's Cloud Monitoring team.
Maximize the value of data stored in Hadoop via operational and ad-hoc reporting, highly interactive analysis, advanced visualizations and dashboards
PayPal utilizes Hadoop as a cost-effective data platform to handle growing data volumes. Hadoop along with other traditional data platforms serves different business needs at PayPal for customer sentiment analysis, fraud detection, market segmentation, etc. PayPal will share some early experiences with Informatica on Hadoop to move & integrate data on Hadoop & between different data platforms
Richard Just, Big Data Program Manager at Capital One Labs, will share his experience using Hadoop and Platfora software to analyze several aspects of their business, including the adoption of their mobile application. The final solution produced an interactive, self-service web-based BI access to the data.
This session explores the benefits and implications of virtualizing Hadoop and highlights several VMware initiatives aimed at bridging Hadoop and virtualization.
The convergence of Analytics and the Cloud creates an interesting opportunity to solve many Big Data challenges that were previously untenable.  Alteryx has historically served retailers and consumer brands on optimizing merchandising and store operations decisions with its Strategic Analytics product.
Big data is everywhere, and it is increasingly complex and growing quickly, rendering manual and legacy approaches obsolete.  Organizations can only realize the business value of big data with a meaning based platform technology that automatically understands all data, structured and unstructured, in real time.  Join this session to learn more about Big Data and the technologies around it.
Nokias Big Data analytics service is a strategic multi-tenant, multi-petabyte platform that executes 10,000 jobs each day.  It is made up of technologies that provide location content processing, ETL, ad-hoc SQL, dashboards and advanced analytics, including Calpont InfiniDB for SQL, Scribe, REST, Hadoop, and R. This talk discusses the platform, motivations behind design choices, and challenges.
It's not easy doing predictive analytics on Hadoop, with few tools that make it easier or more scalable than writing code from scratch.  Join us to discuss a new paradigm that addresses the need for a scalable, powerful solution  one that is purpose-built for Big Data yet is easy to use  illustrated by a demonstration of predictive analytics run on the largest public Hadoop cluster in the world.
How do you keep up with the velocity and variety of data streaming in from the operational systems that power your business? What about getting analytics on your data even before you persist and replicate it?
Opposites attract and thats the case with Hadoop and Enterprise Data Warehouses. Both have a role to play in your Big Data projects. This session explores the various approaches to marrying Hadoop to your EDW, and why youll want to do that in the first place.
In this joint session, experts from Cisco and Cloudera reveal the fundamental design considerations of Hadoop in the Enterprise Data Center. Drawing from lessons learned in the real world, they'll share best practices from deployments of Cloudera's Hadoop distribution alongside Cisco's networking components.
Monitoring thousands of servers generates a lot of data.  Many organizations trying to harness the power of big data struggle with the same types of challenges as Rackspace's Cloud Monitoring team.
Opening remarks by the Strata program chairs, Edd Dumbill and Alistair Croll.
Society confronts enormous challenges today: How will we feed nine billion people? How can we diagnose and treat diseases better, and more cheaply? How will we produce more energy, more cleanly, than ever before? Big questions like these demand new approaches, and "Big Data" is a crucial of the toolkit we will use over the coming years to answer them.
Hadoop is scalable, inexpensive and can store near-infinite amounts of data. But driving it requires exotic skills and hours of batch processing to answer straightforward questions. Learn how everything is about to change.
New York City is a complex, thriving organism. Hear how data science has played a surprising and effective role in helping the city government provide services to over 8 million people, from preventing public safety catastrophes to improving New Yorkers' quality of life.
Data science is a team sport. Collaboration inside and outside your organization is the ultimate Big Data technique. Success depends on having a collaboration platform and solving the number one problem of the Big Data era: the supply and demand for data scientists. Learn how you can take action today to accelerate the success of your data science efforts.
While moving away from single powerful servers, distributed databases still tend to be monolithic solutions. But e.g. key-value storage is rapidly becoming a commodity service, on which richer databases might be built. What are the implications?
Data integration for Big Data projects can consume up to 80% of the development effort and yet too many developers reinvent the wheel by hand-coding custom connectors, data parsers, and data integration transformations. A metadata-driven, codeless IDE with pre-built transformations and data quality rules have proven to be up to 10X more productive than hand coding and easier to maintain.
In recent years, "Big Data" has matured from a vague description of massive corporate data to a household term that refers to not just volume but the diversity of data and velocity of change. Today, there's a wealth of data trapped in corporate data repositories, new platforms like Hadoop, a new generation of data marketplaces and volumes generated hourly on the Web.
The onset of the Big Data phenomenon has created a unique opportunity, but the challenge ahead of us is to move beyond Big Data infrastructure to morally and practically useful applications. This requires new technologies that close the "Understanding Gap"  and, by doing so, can make great strides to prevent evil, reduce suffering, and create more actualized human potential.
Opening remarks by the Strata program chairs, Edd Dumbill and Alistair Croll.
Over the past two decades, Rick Smolan, creator of the best selling "Day in the Life" books, has produced a series of ambitious global projects in collaboration with hundreds of the worlds leading photographers, writers, and graphic designers. This year Smolan invited more than 100 journalists around the globe to explore the world of Big Data.
Were excited to launch the Strata Data Innovation Awards to recognize disruptive, innovative technologies in big data and data science, highlight data science as an increasing importance for companies, and showcase the highlights of the growing data community.
This session will provide insights into how the combination of scale, efficiency, and analytic flexibility creates the power to expand the applications for Hadoop to transform companies as well as entire industries.
Hadoop started as an offline, batch-processing system.  It made it practical to store and process much larger datasets than before.  Subsequently, more interactive, online systems emerged, integrating with Hadoop.
In this rapid-fire keynote, well introduce how virtually every new technology trend is inextricably linked  or should be to attain maximum leverage. Well discuss how you can use technologies such as cloud and mobility to spread the value of analytics pervasively across your virtual organization, and how that positively impacts your employees, customers and partners.
A fireside chat with Cathy O'Neil about why universities can't make data scientists. Lots of companies want to hire data scientists, and there aren't enough to go around. Some universities are adding data science graduate departments, but they're facing an uphill battle, thanks to a lack of good data for academics, political infighting, and scalability issues.
You need more than a database 'hammer' for today's Big Data projects. Organizations need a 'data platform' providing integrated tools to capture, store, process and present data. Without it companies can achieve - volume, velocity, or variety - but not all three. Join us to learn the extreme capabilities needed to distill new business signals from big data.
The story of Big Data technology has centered on engines, algorithms, and statistical methods for data analysis.  Less has been said-and too little has been done-regarding technology to improve the lives of data analysts.
Samantha Ravich, former National Security Advisor to Vice President Richard Cheney, will discuss the challenges that face strategic decision makers from the wealth of data now provided by advances in technology.
Opening remarks by the Strata program chairs, Edd Dumbill and Alistair Croll.
Society confronts enormous challenges today: How will we feed nine billion people? How can we diagnose and treat diseases better, and more cheaply? How will we produce more energy, more cleanly, than ever before? Big questions like these demand new approaches, and "Big Data" is a crucial of the toolkit we will use over the coming years to answer them.
Hadoop is scalable, inexpensive and can store near-infinite amounts of data. But driving it requires exotic skills and hours of batch processing to answer straightforward questions. Learn how everything is about to change.
New York City is a complex, thriving organism. Hear how data science has played a surprising and effective role in helping the city government provide services to over 8 million people, from preventing public safety catastrophes to improving New Yorkers' quality of life.
Data science is a team sport. Collaboration inside and outside your organization is the ultimate Big Data technique. Success depends on having a collaboration platform and solving the number one problem of the Big Data era: the supply and demand for data scientists. Learn how you can take action today to accelerate the success of your data science efforts.
While moving away from single powerful servers, distributed databases still tend to be monolithic solutions. But e.g. key-value storage is rapidly becoming a commodity service, on which richer databases might be built. What are the implications?
Data integration for Big Data projects can consume up to 80% of the development effort and yet too many developers reinvent the wheel by hand-coding custom connectors, data parsers, and data integration transformations. A metadata-driven, codeless IDE with pre-built transformations and data quality rules have proven to be up to 10X more productive than hand coding and easier to maintain.
In recent years, "Big Data" has matured from a vague description of massive corporate data to a household term that refers to not just volume but the diversity of data and velocity of change. Today, there's a wealth of data trapped in corporate data repositories, new platforms like Hadoop, a new generation of data marketplaces and volumes generated hourly on the Web.
The onset of the Big Data phenomenon has created a unique opportunity, but the challenge ahead of us is to move beyond Big Data infrastructure to morally and practically useful applications. This requires new technologies that close the "Understanding Gap"  and, by doing so, can make great strides to prevent evil, reduce suffering, and create more actualized human potential.
Opening remarks by the Strata program chairs, Edd Dumbill and Alistair Croll.
Over the past two decades, Rick Smolan, creator of the best selling "Day in the Life" books, has produced a series of ambitious global projects in collaboration with hundreds of the worlds leading photographers, writers, and graphic designers. This year Smolan invited more than 100 journalists around the globe to explore the world of Big Data.
Were excited to launch the Strata Data Innovation Awards to recognize disruptive, innovative technologies in big data and data science, highlight data science as an increasing importance for companies, and showcase the highlights of the growing data community.
This session will provide insights into how the combination of scale, efficiency, and analytic flexibility creates the power to expand the applications for Hadoop to transform companies as well as entire industries.
Hadoop started as an offline, batch-processing system.  It made it practical to store and process much larger datasets than before.  Subsequently, more interactive, online systems emerged, integrating with Hadoop.
In this rapid-fire keynote, well introduce how virtually every new technology trend is inextricably linked  or should be to attain maximum leverage. Well discuss how you can use technologies such as cloud and mobility to spread the value of analytics pervasively across your virtual organization, and how that positively impacts your employees, customers and partners.
A fireside chat with Cathy O'Neil about why universities can't make data scientists. Lots of companies want to hire data scientists, and there aren't enough to go around. Some universities are adding data science graduate departments, but they're facing an uphill battle, thanks to a lack of good data for academics, political infighting, and scalability issues.
You need more than a database 'hammer' for today's Big Data projects. Organizations need a 'data platform' providing integrated tools to capture, store, process and present data. Without it companies can achieve - volume, velocity, or variety - but not all three. Join us to learn the extreme capabilities needed to distill new business signals from big data.
The story of Big Data technology has centered on engines, algorithms, and statistical methods for data analysis.  Less has been said-and too little has been done-regarding technology to improve the lives of data analysts.
Samantha Ravich, former National Security Advisor to Vice President Richard Cheney, will discuss the challenges that face strategic decision makers from the wealth of data now provided by advances in technology.
Peng Du and Randy Wei offer an overview of Ubers data science workbench, which provides a central platform for data scientists to perform interactive data analysis through notebooks, share and collaborate on scripts, and publish results to dashboards and is seamlessly integrated with other Uber services, providing convenient features such as task scheduling, model publishing, and job monitoring.
In the past, typical real-time data processing was reserved for answering operational questions and very basic analytical questions, but with better processing frameworks and more-capable hardware, the streaming context can now enable personalization applications. Christopher Colburn and Monal Daxini explore the challenges faced when building a streaming application at scale at Netflix.
LinkedIn has one of the largest Kafka installations in the world, ingesting more than a trillion messages per day. Apache Samza-based stream processing applications process this deluge of data. Kartik Paramasivam discusses key improvements and architectural patterns that LinkedIn has adopted in its data systems in order to process millions of requests per second while keeping costs in control.
Sridhar Alla and Shekhar Agrawal explain how Comcast built the largest Kudu cluster in the world (scaling to PBs of storage) and explore the new kinds of analytics being performed there, including real-time processing of 1 trillion events and joining multiple reference datasets on demand.
Netflix Keystone processes over a trillion events per day with at-least-once processing semantics in the cloud. Monal Daxini explores what it means to offer stream processing as a service (SPaaS), how Netflix implemented a scalable, fault-tolerant multitenant SPaaS internal offering, and how it evolved the system in flight with no downtime.
Tony Xing offers an overview of Microsoft's common anomaly detection platform, an API service built internally to provide product teams the flexibility to plug in any anomaly detection algorithms to fit their own signal types.
The Netflix data platform is constantly evolving, but fundamentally it's an all-cloud platform at a massive scale (40+ PB and over 700 billion new events per day) focused on empowering developers. Kurt Brown dives into the current technology landscape at Netflix and offers some thoughts on what the future holds.
Uber relies on making data-driven decisions at every level, and most of these decisions can benefit from faster data processing. Vinoth Chandar and Prasanna Rajaperumal introduce Hoodie, a newly open sourced system at Uber that adds new incremental processing primitives to existing Hadoop technologies to provide near-real-time data at 10x reduced cost.
There are many good reasons to run more than one Kafka cluster...and a few bad reasons too. Great architectures are driven by use cases, and multicluster deployments are no exception. Gwen Shapira offers an overview of several use cases, including real-time analytics and payment processing, that may require multicluster solutions to help you better choose the right architecture for your needs.
Not all data science problems are big data problems. Lots of small and medium product companies want to start their journey to become data driven. Nischal HP and Raghotham Sripadraj share their experience building data science platforms for various enterprises, with an emphasis on making the right architecture choices and using distributed and fault tolerant tools.
Data warehouses are critical in driving business decisionswith SQL dominantly used to build ETL pipelines. While the technology has shifted from using RDBMS-centric data warehouses to data pipelines based on Hadoop and MPP databases, engineering and quality processes have not kept pace. Avinash Padmanabhan highlights the changes that Intuit's team made to improve processes and data quality.
Big data famously enables anyone to contribute to the enterprise data store. Integrating previously siloed data can uncover powerful insights for the business. But without data governance, inefficiencies and incorrect business decisions may result. Barbara Eckman explains how Comcast is using Apache Avro for enterprise data governance, the challenges faced, and methods to address these challenges.
Dirk Jungnickel explains how Dubai-based telco leader du leverages big data to create smart cities and enable location-based data monetization, covering business objectives and outcomes and addressing technical and analytical challenges.
Launched in late 2015, Astellas's enterprise data lake project is taking the company on a data governance journey. Kishore Papineni offers an overview of the project, providing insights into some of the business pain points and key drivers, how it has led to organizational change, and the best practices associated with Astellas's new data governance process.
Building a cross-functional data science team at a large, multinational manufacturing company presents a number of cultural, organizational, technical, and operational challenges. Carlo Torniai explains how Pirelli grew an organization that was able to deliver key insights in less than a year and shares advice for both new and established data science teams.
When processing 24% of total global credit card transactions, data, risk, and security are top priorities for American Express. Bryan Harrison highlights the modern process, people, and architecture approach that has enabled Amex to scale BI on Hadoop and given instant access to real-time, granular data, as well as broad historical views to model, so Amex can stay ahead of fraud in the future.
Chris Murphy explains how a major insurance company adopted Hadoop to leverage its data to underpin the new customer-centric ethos of the organization and how this has enabled a new approach to helping customers truly understand their financial portfolios, build a roadmap to meet their financial goals, identify opportunities, and help them secure their financial future.
New users are the most delicate for any service. Nailing their first experience with your product is essential to growing your user base. Maura Lynch offers an overview of Pinterest's hybrid-curation approach to creating compelling content streams for users when there is very little signal as to their preferences.
Being able to monitor the emotional state of police officers over a period of time would enable the officers supervisors to intervene if a given officer is subject to repeated emotional stress. Nixon Patel presents deep learning and AI models that capture and analyze the emotional state of law enforcement officers over a period of time.
ING is a data-driven enterprise that is heavily investing in big data, analytics, and streaming processing. Bas Geerdink offers an overview of ING's streaming analytics solution for providing actionable insights to customers, built with a combination of open source technologies, including Kafka, Flink, and Cassandra.
With more than 10,000 active applications, 4.23 million daily ad conversions, and over a billion total app downloads, Tapjoy knows mobile advertising. To ensure that users have the best application experiences, Tapjoy has architected a real-time advertising engine. Marcus Walser shares the critical considerations for building such a streaming architecture.
Modern political campaigns at the local, state, and national level cannot be won without working with voter data. This has given birth to a wave of technology that has both influenced and insinuated itself into the fabric of modern politics. Peeking behind the curtain, Jim Harrold explores how this system utilizes data to help campaign strategists win elections.
We dont need to try hard to convince anyone today that human beings are emotional first, and rational second. Decisions that we make every minute - from what flavour of Starbucks to buy to who to vote for - are made deep inside our emotional minds influenced by many conscious reasons and sub-conscious primes.
Organizations need a model to measure how effectively they are using data and analytics. Once they know where they are and where they need to go, they then need a framework to determine the economic value of their data. William Schmarzo explores techniques for getting business users to think like a data scientist so they can assist in identifying data that makes the best performance predictors.
The move to streaming architectures from batch processing is a revolution in how companies use data. But what is the state of the union for stream processing, and what gaps remain in the technology we have? How will this technology impact the architectures and applications of the future? Jay Kreps explores the future of Apache Kafka and the stream processing ecosystem.
Roger Barga offers an overview of Kinesis, Amazons data streaming platform, which includes Kinesis Firehose, Kinesis Analytics, and Kinesis Streams, and explains how customers have architected their applications using Kinesis services for low-latency and extreme scale.
Metamarkets operates several Kafka clusters in its real-time streaming event ingestion pathway. Over the last three years, they have grown beyond the original parameters, with hundreds of terabytes flowing through every day, petabytes of retention, and gigabytes of historical data streaming to and from storage. Michael Edwards shares experiences and lessons learned operating Kafka at this scale.
Kevin Mao explores the value of and challenges associated with collecting raw security event data from disparate corners of enterprise infrastructure and transforming them into high-quality intelligence that can be used to forecast, detect, and mitigate cybersecurity threats.
Apache DistributedLog (incubating) is a low-latency, high-throughput replicated log service. Sijie Guo shares how Twitter has used DistributedLog as the real-time data foundation in production for years, supporting services like distributed databases, pub-sub messaging, and real-time stream computing and delivering more than 1.5 trillion (17 PB) events per day.
Join Tyler Akidau for a whirlwind tour of the conceptual building blocks of massive-scale data processing systems over the last decade, as Tyler compares and contrasts systems at Google with popular open source systems in use today.
Watermarks are a system for measuring progress and completeness in out-of-order streaming systems and are utilized to emit correct results in a timely manner. Given the trend toward out-of-order processing in existing streaming systems, watermarks are an increasingly important tool when designing streaming pipelines. Slava Chernyak explains watermarks and explores real-world applications.
Jamie Grier outlines the latest important features in Apache Flink and walks you through building a working demo to show these features off. Topics include queryable state, dynamic scaling, streaming SQL, very large state support, and whatever is the latest and greatest in March 2017.
As Accenture scaled to millions of predictive models, it needed automation to manage models at scale, ensure accuracy, prevent false alarms, and preserve trust as models are created, tested, and deployed into production. Teresa Tung, Jrgen Weichenberger, and Ishmeet Grewal share their approach to implementing DevOps for models and employing a self-healing approach to model lifecycle management.
Visa is transforming the way it manages data: database appliances are giving way to Hadoop and HBase, and proprietary ETL is being replaced by Spark. Nandu Jayakumar discusses the adoption of big data practices at this conservative financial enterprise and contrasts it against the adoption of the same ideas at his previous employer, a web/ad-tech company.
Dustin Cote and Ryan Pridgeon share their experience troubleshooting Apache Kafka in production environments and discuss how to avoid pitfalls like message loss or performance degradation in your environment.
Deep learning is white-hot at the moment, but why does it matter? Developers are usually the first to understand why some technologies cause more excitement than others. Edd Wilder-James relates this insider knowledge, providing a tour through the hottest emerging data technologies of 2017 to explain why theyre exciting in terms of both new capabilities and the new economies they bring.
Life doesnt happen in batches. Being able to work with data from continuous events as data streams is a better fit to the way life happens, but doing so presents some challenges. Ellen Friedman examines the advantages and issues involved in working with streaming data, takes a look at emerging technologies for streaming, and describes best practices for this style of work.
The cloud is becoming pervasive, but it isnt always full of rainbows. Defining a strategy that works for your company or for your use cases is critical to ensuring success. Jim Scott explores different use cases that may be best run in the cloud versus on-premises, points out opportunities to optimize cost and operational benefits, and explains how to get the data moved between locations.
Data science is not only about machine learning. To be a successful data person, you also need a significant understanding of statistics. Gabriela de Queiroz walks you through the top five statistical concepts you need to know to work with data.
Melanie Warrick explores the definition of artificial intelligence and seeks to clarify what AI will mean for our world. Melanie summarizes AIs most important effects to date and demystifies the changes well see in the immediate future, separating myth from realistic expectation.
Seemingly harmless choices in visualization design and content selection can distort your data and lead to false conclusions. Aneesh Karve presents a quantitative framework for identifying and overcoming distortions by applying recent research in algebraic visualization.
Want to ramp up your knowledge of Amazon's big data web services and launch your first big data application on the cloud? Ben Snively, Radhika Ravirala, Ryan Nienhuis, and Dario Rivera walk you through building a big data application using open source technologies, such as Apache Hadoop, Spark, and Zeppelin, and AWS managed services, such as Amazon EMR, Amazon Kinesis, and more.
Dave Kale, Susan Eraly, and Josh Patterson explain how to build, train, and deploy neural networks using Deeplearning4j. Topics include the fundamentals of deep learning, ND4J and DL4J, and scalable training using GPUs and Apache Spark. You'll gain hands-on experience with several models, including convolutional and recurrent neural nets.
Unbounded, out-of-order, global-scale data is now the norm. Even for the same computation, each use case entails its own balance between completeness, latency, and cost. Kenneth Knowles shows how Apache Beam gives you control over this balance in a unified programming model that is portable to any Beam runner, including Apache Spark, Apache Flink, and Google Cloud Dataflow.
Food production and preparation have always been labor and capital intensive, but with the internet of things, low-cost sensors, cloud-computing ubiquity, and big data analysis, farmers and chefs are being replaced with connected, big data robotsnot just in the field but also in your kitchen. Tim Gasper explores the tech stack, data science techniques, and use cases driving this revolution.
Joseph Blue and Carol Mcdonald walk you through a reference application that processes ECG data encoding using HL7 with a modern anomaly detector, demonstrating how combining visualization and alerting enables healthcare professionals to improve outcomes and reduce costs and sharing lessons learned from their experience dealing with real data in real medical situations.
Kishore Reddipalli explores how to stream data at a large scale from the edge to the cloud to the client, detect anomalies, analyze machine data in stream and rest in an industrial world, and optimize the industrial operations by providing real-time insights and recommendations using big data technologies.
IoT applications often need more-complex queries than those supported by traditional time series databases. Michael Freedman outlines a new distributed time series database for such workloads, supporting efficient queries, including complex predicates across many metrics, while scaling out to support IoT ingest rates.
Twitter processes billions of events per day the instant the data is generated using Heron, an open source streaming engine tailored for large-scale environments. Bill Graham, Avrilia Floratau, and Ashvin Agrawal explore the techniques Heron uses to elastically scale resources in order to handle highly varying loads without sacrificing real-time performance or user experience.
Anomaly detection plays a key role in the context of analysis of real-time streams. This is exemplified by, say, detection incidents in real life from tweet storms. Arun Kejariwal and Karthik Ramasamy walk you through how anomaly detection is supported in real-time data streams in Heronthe streaming system built in-house at Twitter (and open sourced) for real-time computation.
In pursuit of speed, big data is evolving toward columnar execution. The solid foundation laid by Arrow and Parquet for a shared columnar representation across the ecosystem promises a great future. Julien Le Dem and Jacques Nadeau discuss the future of columnar and the hardware trends it takes advantage of, such as RDMA, SSDs, and nonvolatile memory.
Shivnath Babu offers an introduction to using deep learning to solve complex problems in IT operations analytics. Shivnath focuses on how deep learning can derive operations insights automatically for the complex big data application stack composed of systems such as Hadoop, Spark, Cassandra, Elasticsearch, and Impala, using examples of open source tools for deep learning.
In 2016, digital advertising overtook TV in spend, requiring companies to cut through the noise to reach their audience. Manny Puentes explains how Rebel AI decides which ads to serve across devices and how it delivers multidimension reporting in milliseconds.
How can we empower individuals with special needs to reach their potential? Julie Lockner offers an overview of a project to develop collaboration applications that use wearable device data to improve the ability to develop the best possible care and education plans. Join in to learn how real-time IoT data analytics are making this possible.
Apache Spark is written in Scala. Hence, many if not most data engineers adopting Spark are also adopting Scala, while most data scientists continue to use Python and R. Dean Wampler offers an overview of the core features of Scala you need to use Spark effectively, using hands-on exercises with the Spark APIs.
Using an interactive demo format with accompanying online materials and data, data scientist Juliet Hougland offers a practical overview of the basics of using Python data tools with a Hadoop cluster.
Ram Shankar Siva Kumar and Andrew Wicker explain how to operationalize security analytics for production in the cloud, covering a framework for assessing the impact of compliance on model design, six strategies and their trade-offs to generate labeled attack data for model evaluation, key metrics for measuring security analytics efficacy, and tips to scale anomaly detection systems in the cloud.
Cesar Berho and Alan Ross offer an overview of open source project Apache Spot (incubating), which delivers next-generation cybersecurity analytics architecture through unsupervised learning using machine-learning techniques at cloud scale for anomaly detection.
Apache Kafka is used by over 35% of Fortune 500 companies to store and process some of their most sensitive datasets. Ajit Gaddam and Jiphun Satapathy provide a security reference architecture to secure your Kafka cluster while leveraging it to support your organization's cybersecurity requirements.
Security will always be very important in the world of big data, but the choices today mostly start with Kerberos. Does that mean setting up security is always going to be painful? What if your company standardizes on other security alternatives? What if you want to have the freedom to decide what security type to support? Yuliya Feldman discusses your options.
Recently, research on applying and designing ML algorithms and systems for security has grown quickly as information and communications have become more ubiquitous and more data has become available. Parvez Ahammad explores generalized system designs, underlying assumptions, and use cases for applying ML in security.
How many of your users are really fraudsters waiting to strike? These sleeper cells exist in all online communities. Using data from more than 400M users and 500B events from online services across the world, Yinglian Xie explores sleeper cells, explains sophisticated attack techniques being used to evade detection, and shows how Spark's in-memory big data security analytics can help.
In a panel moderated by Steve Totman, Mike Olson, Laura Eisenhardt, Craig Hibbeler, and David Goodman discuss real-world projects using big data as a force for good to address problems ranging from Zika to child trafficking. If youre interested in how big data can benefit humankind, join in to learn how to get involved.
Shirshanka Das and Yael Garten share best practices learned using Kafka and Hadoop as the foundation of a petabyte-scale data warehouse at LinkedIn, offering concrete suggestions to help you process data seamlessly. Along the way, Shirshanka and Yael discuss their experience running governance to empower data teams.
Open government datafree public data than anyone can use and republishis a major resource for entrepreneurs and innovators. The Center for Open Data Enterprise has partnered with the White House, government, and businesses to show how this resource can create economic value. Joel Gurin and Katherine Garcia share case studies of how open data is being used and a vision for its future.
Warren Reed explains how he and his team at the US Treasurys Office of Financial Research leverage data visualization techniques to build interactive data products for risk measurement and monitoring.
When it comes to visibility into account takeover, spam, and fake accounts, the cloud is making things hazy. Cloud-hosted attacks skirt IP blacklists and make fraudulent users seem like they are located somewhere they are not. Drawing on data from 500 billion events and 400 million user accounts, Ting-Fang Yen examines cloud-based attack trends across verticals and regions.
Join in to learn how to do scalable, end-to-end data science in R on single machines as well as on Spark clusters. You'll be assigned an individual Spark cluster with all contents preloaded and software installed and use it to gain experience building, operationalizing, and consuming machine-learning models using distributed functions in R.
Sparklyr provides an R interface to Spark. With sparklyr, you can manipulate Spark datasets to bring them into R for analysis and visualization and use sparklyr to orchestrate distributed machine learning in Spark from R with the Spark MLlib and H2O SparkingWater libraries. John Mount demonstrates how to use sparklyr to analyze big data in Spark.
Reynold Xin looks back at the history of data systems, from filesystems, databases, and big data systems (e.g., MapReduce) to "small data" systems (e.g., R and Python), covering the pros and cons of each, the abstractions they provide, and the engines underneath. Reynold then shares lessons learned from this evolution, explains how Spark is developed, and offers a peek into the future of Spark.
Apache Spark 2.0 introduced the core APIs for Structured Streaming, a new streaming processing engine on Spark SQL. Since then, the Spark team has focused its efforts on making the engine ready for production use. Michael Armbrust and Tathagata Das outline the major features of Structured Streaming, recipes for using them in production, and plans for new features in future releases.
Sparklyr makes it easy and practical to analyze big data with Ryou can filter and aggregate Spark DataFrames to bring data into R for analysis and visualization and use R to orchestrate distributed machine learning in Spark using Spark ML and H2O SparkingWater. Edgar Ruiz walks you through these features and demonstrates how to use sparklyr to create R functions that access the full Spark API.
Spark powers various services in Bing, but the Bing team had to customize and extend Spark to cover its use cases and scale the implementation of Spark-based data pipelines to handle internet-scale data volume. Kaarthik Sivashanmugam explores these use cases, covering the architecture of Spark-based data platforms, challenges faced, and the customization done to Spark to address the challenges.
Both Spark workloads and use of the public cloud have been rapidly gaining adoption in mainstream enterprises. Anand Iyer and Philip Langdale discuss new developments in Spark and provide an in-depth discussion on the intersection between the latest Spark and cloud technologies.
Just like any six-year-old, Apache Spark does not always do its job and can be hard to understand. Michael Armbrust and Eric Liang look at the top causes of job failures customers encountered in production and examine ways to mitigate such problems by modifying Spark. They also share a methodology for improving resilience: a combination of monitoring and debugging techniques for users.
Alluxio (formerly Tachyon) is an open source memory-speed virtual distributed storage system. The project has experienced a tremendous improvement in performance and scalability and was extended with key new features. Haoyuan Li and Gene Pang explore Alluxio's goal of making its product accessible to an even wider set of users through a focus on security, new language bindings, and APIs.
Much of Apache Sparks power comes from lazy evaluation along with intelligent pipelining, which can make debugging more challenging than on traditional distributed systems. Holden Karau and Joey Echeverria explore how to debug Apache Spark applications, the different options for logging in Sparks variety of supported languages, and some common errors and how to detect them.
Alexander Ulanov and Manish Marwah explain how they implemented a scalable version of loopy belief propagation (BP) for Apache Spark, applying BP to large web-crawl data to infer the probability of websites to be malicious. Applications of BP include fraud detection, malware detection, computer vision, and customer retention.
Bryan Cheng and Karen Hsu describe how they built machine-learning and graph traversal systems on Apache Spark to help government organizations and private businesses stay informed in the brave new world of blockchain technology. Bryan and Karen also share lessons learned combining these two bleeding-edge technologies and explain how these techniques can be applied to private and federated chains.
Structured Streaming is new in Apache Spark 2.0, and work is being done to integrate the machine-learning interfaces with this new streaming system. Holden Karau and Seth Hendrickson demonstrate how to do streaming machine learning using Structured Streaming and walk you through creating your own streaming model.
Amy Unruh and Yufeng Guo walk you through training and deploying a machine-learning system using TensorFlow, a popular open source library. You'll learn how to build machine-learning systems from simple classifiers to complex image-based models and how to deploy models in production using TensorFlow Serving.
Using Entity 360 as an example, Jonathan Seidman, Ted Malaska, Mark Grover, and Gwen Shapira explain how to architect a modern, real-time big data platform leveraging recent advancements in the open source software world, using components like Kafka, Impala, Kudu, Spark Streaming, and Spark SQL with Hadoop to enable new forms of data processing and analytics.
Todd Lipcon offers a very brief refresher on the goals and feature set of the Kudu storage engine, covering the development that has taken place over the last year, including new features such as improved support for time series workloads, performance improvements, Spark integration, and highly available replicated masters.
Sean Suchter and Shekhar Gupta describe the use of very fine-grained performance data from many Hadoop clusters to build a model predicting excessive swapping events.
Paige Liu explores the options and trade-offs to consider when building a Cloudera cluster on Microsoft Azure Cloud and explains how to deploy and scale a Cloudera cluster on Azure and how to connect a Cloudera cluster with other Azure services to build enterprise-grade end-to-end big data solutions.
Dwai Lahiri explains how to leverage private cloud infrastructure to successfully build Hadoop clusters and outlines dos, don'ts, and gotchas for running Hadoop on private clouds.
Recently, the volume of data collected from farmers' fields via sensors, rovers, drones, in-cabin technologies, and other sources has forced Monsanto to rethink its geospatial processing capabilities. Naghman Waheed and Martin Mendez-Costabel explain how Monsanto built a scalable geospatial platform using cloud and open source technologies.
Todd Lipcon and Marcel Kornacker offer an introduction to using Impala + Kudu to power your real-time data-centric applications for use cases like time series analysis (fraud detection, stream market data), machine data analytics, and online reporting.
Apache Kylin, which started as a big data OLAP engine, is reaching its v2.0. Yang Li explains how, armed with snowflake schema support, a full SQL interface, and the ability to consume real-time streaming data, Apache Kylin is closing the gap to becoming a real-time data warehouse.
Chao Zhong offers an overview of a new predictive model for customer lifetime value (LTV) in a cloud-computing business. This model is also the first known application of the Fader RFM approach to a cloud businessa Bayesian approach that predicts a customer's LTV with a symmetric absolute percentage error (SAPE) of only 3% on an out-of-time testing dataset.
David Yan offers an overview of Apache Apex, a stream processing engine used in production by several large companies for real-time data analytics. With Apex, you can build applications that scalably and reliably process their data with high throughput and low latency.
Alluxio bridges Spark applications with various storage systems and further accelerates data-intensive applications. Gene Pang and Jiri Simsa introduce Alluxio, explain how Alluxio can help Spark be more effective, show benchmark results with Spark RDDs and DataFrames, and describe production deployments with both Alluxio and Spark working together.
Docker makes it easy to bundle an application with its dependencies and provide full isolation, and YARN now supports Docker as an execution engine for submitted applications. Daniel Templeton explains how YARN's Docker support works, why you'd want to use it, and when you shouldn't.
GoDaddy ingests and analyzes 100,000 EPS of logs, metrics, and events each day. Felix Gorodishter shares GoDaddy's big data journey and explains how the company makes sense of 10+-TB-per-day growth for operational insights of its cloud leveraging Kafka, Hadoop, Spark, Pig, Hive, Cassandra, and Elasticsearch.
Zillow pioneered providing access to unprecedented information about the housing market. Long gone are the days when you needed an agent to get comparables and prior sale and listing data. And with more data, data science has enabled more use cases. Jasjeet Thind explains how Zillow uses Spark and machine learning to transform real estate.
Gwen Shapira and Bob Lehmann share their experience and patterns building a cross-data-center streaming data platform for Monsanto. Learn how to facilitate your move to the cloud while "keeping the lights on" for legacy applications. In addition to integrating private and cloud data centers, you'll discover how to establish a solid foundation for a transition from batch to stream processing.
Ganesh Prabhu, Alex Rivlin, and Vivek Agate share an approach that enabled a small team at FireEye to migrate 20 TB of RDBMS data comprised of 250+ tables and nearly 2,000 partitions to Hadoop and an adaptive platform that allows migration of a rapidly changing dataset to Hive. Along the way, they explore some of the challenges typical for a company implementing Hadoop.
Marcel Kornacker and Mostafa Mokhtar help simplify the process of making good SQL-on-Hadoop decisions and cover top performance optimizations for Apache Impala (incubating), from schema design and memory optimization to query tuning.
Most internet companies record a constant stream of logs as a user interacts with their application. Depending on the complexity of the application, the logs can be extremely difficult to decipher. Dorna Bandari presents a novel NLP-based method for clustering user sessions in consumer internet applications, which has proved to be extremely effective in both driving strategy and personalization.
Real-world data is incomplete and imperfect. The right way to handle it is with Bayesian inference. Michael Williams  demonstrates how probabilistic programming languages hide the gory details of this elegant but potentially tricky approach, making a powerful statistical method easy and enabling rapid iteration and new kinds of data-driven products.
How do we know that an advertisement or promotion truly drives incremental revenue? Michelangelo D'Agostino and Bill Lattner share their experience developing machine-learning techniques for predicting treatment responsiveness from randomized controlled experiments and explore the use of these persuasion models at scale in politics, social good, and marketing.
Supporting multiple locales involves the maintenance and generation of localized strings. Michelle Casbon explains how machine learning and natural language processing are applied to the underserved domain of localization using primarily open source tools, including Scala, Apache Spark, Apache Cassandra, and Apache Kafka.
The promise of the automated statistician is as old as statistics itself. Eduardo Arino de la Rubia explores the tools created by the open source community to free data scientists from tedium, enabling them to work on the high-value aspects of insight creation. Along the way, Eduardo compares open source tools such as TPOT and auto-sklearn and discusses their place in the DS workflow.
Eric Richardson explains how ACS used Hadoop, HBase, Spark, Kafka, and Solr to create a hybrid cloud enterprise data hub that scales without drama and drives adoption by ease of use, covering the architecture, technologies used, the challenges faced and defeated, and problems yet to solve.
It's well known that data analysts spend 80% of their time preparing data and only 20% analyzing. In order to change that ratio, organizations must build tools specifically designed for working with ad hoc (semistructured) data. Sean Kandel and Karthik Sethuraman explore a new technique leveraging machine learning to discover and profile the inherent structure in ad hoc datasets.
Pinterest built a flexible, graph-based system for making recommendations to users in real time. The system uses random walks on a user-and-object graph in order to make personalized recommendations to 100+ million Pinterest users out of a catalog of over a billion items. Jure Leskovec explains how Pinterest built its modern recommendation engine and the lessons learned along the way.
When there is a strong signal in a large dataset, many machine-learning algorithms will find it. On the other hand, when the effect is weak and the data is large, there are many ways to discover an effect that is in fact nothing more than noise. Robert Grossman shares best practices so that you will not be accused of p-hacking.
Ted Dunning offers an overview of tensor computingcovering, in practical terms, the high-level principles behind tensor computing systemsand explains how it can be put to good use in a variety of settings beyond training deep neural networks (the most common use case).
In the machine-learning pipeline, feature engineering takes up the majority amount of time yet is seldom discussed. Alice Zheng leads a tour of popular feature engineering methods for text, logs, and images, giving you an intuitive and actionable understanding of tricks of the trade.
Anima Anandkumar demonstrates how to use preconfigured Deep Learning AMIs and CloudFormation templates on AWS to help speed up deep learning development and shares use cases in computer vision and natural language processing.
Despite widespread adoption, machine-learning models remain mostly black boxes, making it very difficult to understand the reasons behind a prediction. Such understanding is fundamentally important to assess trust in a model before we take actions based on a prediction or choose to deploy a new ML service.
With over 75 billion pins, the Pinterest content corpus is one of the largest human-curated collection of ideas. Grace Huang walks you through the lifecycle of a piece of content in Pinterest, a portfolio of metrics developed to monitor the health of the content corpus, and the story of creating a cross-functional initiative to preserve a healthy, sustainable content ecosystem.
Gleicon Moraes and Arthur Grava share war stories about developing and deploying a cloud-based large-scale recommender system for a top-three Brazilian ecommerce company. The system, which uses Cassandra and graph traversal, led to a more than 15% increase in sales.
Many iterative machine-learning algorithms can only operate efficiently when a large matrix of training data fits in the main memory. Frederick Reiss and Arvind Surve offer an overview of compressed linear algebra, a technique for compressing training data and performing key operations in the compressed domain that lets you build models over big data with small machines.
David Talby and Claudiu Branzan offer a live demo of an end-to-end system that makes nontrivial clinical inferences from free-text patient records. Infrastructure components include Kafka, Spark Streaming, Spark, and Elasticsearch; data science components include spaCy, custom annotators, curated taxonomies, machine-learned dynamic ontologies, and real-time inferencing.
Come learn the basics of stream processing via a guided walkthrough of the most sophisticated and portable stream processing model on the planetApache Beam (incubating). Tyler Akidau and Frances Perry cover the basics of robust stream processing with the option to execute exercises on top of the runner of your choiceFlink, Spark, or Google Cloud Dataflow.
Ian Wrigley demonstrates how Kafka Connect and Kafka Streams can be used together to build real-world, real-time streaming data pipelines. Using Kafka Connect, you'll ingest data from a relational database into Kafka topics as the data is being generated and then process and enrich the data in real time using Kafka Streams before writing it out for further analysis.
Qubole started out by offering Hadoop as a service in AWS. Over time, it extended its big data capabilities beyond Hadoop and its cloud infrastructure support beyond AWS. Sriram Ganesan and Prakhar Jain explain how and why Qubole built Cloudman, a simple, cloud-agnostic, multipurpose provisioning tool that can be extended for further engines and further cloud support.
Cloud infrastructure, with a scalable data store and elastic compute, is particularly well suited for large-scale data engineering workloads. Andrei Savu and Jennifer Wu explore the latest cloud technologies and outline cost, security, and ease-of-use considerations for data engineers.
Shubham Tagra offers an introduction to RubiX, a lightweight, cross-engine caching solution that works well with optimized columnar formats by caching only the required amount of data. RubiX can be used with any data analytics engine that reads data from remote sources via the Hadoop FileSystem interface without any changes to the source code of those engines.
Big data needs governance, not just for compliance but also for data scientists. Governance empowers data scientists to find, trust, and use data on their own, yet it can be overwhelming to know where to startespecially if your big data environment spans beyond your enterprise to the cloud. Mark Donsky shares a step-by-step approach to kick-start your big data governance initiatives.
Big data applications in the cloud are becoming more about the global distribution and access of data than about easier deployments. Dale Kim shares insights on architecting big data applications for the cloud, using an example reference application his team built and published as context for describing several key requirements for cloud-based environments.
Rajat Monga offers an overview of TensorFlow progress and adoption in 2016 before looking ahead to the areas of importance in the futureperformance, usability, and ubiquityand the efforts TensorFlow is making in those areas.
Joseph Bradley and Tim Hunter share best practices for building deep learning pipelines with Apache Spark, covering cluster setup, data ingest, tuning clusters, and monitoring jobsall demonstrated using Googles TensorFlow library.
Self-service data science is easier said than delivered, especially on Apache Hadoop. Most organizations struggle to balance the diverging needs of the data scientist, data engineer, operator, and architect. Matt Brandwein and Tristan Zajonc cover the underlying root causes of these challenges and introduce new capabilities being developed to make self-service data science a reality.
Thanks to frameworks such as Spark's GraphX and GraphFrames, graph-based techniques are increasingly applicable to anomaly, outlier, and event detection in time series. Jeffrey Yau offers an overview of applying graph-based techniques in fraud detection, IoT processing, and financial data and outlines the benefits of graphs relative to other techniques.
Data Refuge, a nationwide volunteer effort led by librarians, scientists and coders, is currently working to discover and back up research data at risk of disappearing. Max has been working with this effort to uncover hundreds of federal data servers containing petabytes of publicly funded research data, and is making a plan to keep it online and useful to researchers in the future.
Henry Robinson and Alex Gutow explain how to best take advantage of the flexibility and cost-effectiveness of the cloud with your BI and SQL analytic workloads using Apache Hadoop and Apache Impala (incubating) to provide the same great functionality, partner ecosystem, and flexibility of on-premises deployments.
Divide and recombine techniques provide scalable methods for exploration and visualization of otherwise intractable datasets. Stephen Elston and Ryan Hafen lead a series of hands-on exercises to help you develop skills in exploration and visualization of large, complex datasets using R, Hadoop, and Spark.
Visualizations are a key part of conveying any dataset. D3 is the most popular, easiest, and most extensible way to get your data online in an interactive way. Brian Suda outlines best practices for good data visualizations and explains how you can build them using D3.
Although deep learning has proved to be very powerful, few results are reported on its application to business-focused problems. Feng Zhu and Val Fontama explore how Microsoft built a deep learning-based churn predictive model and demonstrate how to explain the predictions using LIMEa novel algorithm published in KDD 2016to make the black box models more transparent and accessible.
Over the last few years, convolutional neural networks (CNN) have risen in popularity, especially in computer vision. Anirudh Koul explains how to bring the power of deep learning to memory- and power-constrained devices like smartphones and drones.
James Bradbury offers an overview of PyTorch, a brand-new deep learning framework from developers at Facebook AI Research that's intended to be faster, easier, and more flexible than alternatives like TensorFlow. James makes the case for PyTorch, focusing on the library's advantages for natural language processing and reinforcement learning.
Deep learning has revolutionized AI and natural language processing in particular. Delip Rao walks you through some of the recent breakthroughs in NLP and the deep learning technologies powering them and discusses some of the challenges in building and deploying such systems.
While attention and memory have become important components in many state-of-the-art deep learning architectures, it's not always obvious where they may be most useful. Even more challenging, such models can be very computationally intensive for production. Stephen Merity discusses the most recent techniques, what tasks they show the most promise in, and when they make sense in production systems.
In a data-driven organization, vice presidents, directors, and managers play a crucial role as translators between senior leadership and data science teams. They dont need to be full-fledged data scientists, but they do need data science "street smarts in order to succeed in this critical task. Mehmet Irmak Sirer outlines the skills they need and gives practical ways to improve them.
The goal of RCSA's Scialog conferences is to foster collaboration between scientists with different specialties and approaches, and, working with Datascope, the company has been doing so in a quantitative way for the last six years. Brian Lange discusses how Datasope and RCSA arrived at the problem, the design choices made in the survey and optimization, and how the results were visualized.
Gillian Docherty shares her experience leading The Data Lab, an innovation center focused on helping organizations drive economic and social benefit through data science and analytics. Along the way, Gillian discusses some of the projects her teams have supported, from multinationals to startups, and explains how they leverage academic capability to help drive innovation from data.
Data scientists blend the skills of statisticians, software engineers, and domain experts to create new roles. Data science isn't merely an amalgam of disciplines but rather a gestalt which synthesizes the ethos of various fields. This merits new thinking when it comes to organization. Eric Colson explores some noveland often unintuitiveways to unleash the value of your data science team.
Programmable enterprises are developing their businesses around cloud computing, big data, and the internet of things. Robert Cohen explores how infrastructure changes will alter corporate use of software, skilled employees, and strategies, the business and economic impacts of these changes, and the broader impacts of these shifts on our economy and society.
Estimating the growth rate of tumors is a very important but very expensive and time-consuming part of diagnosing and treating breast cancer. Michael Dusenberry and Frederick Reiss describe how to use deep learning with Apache Spark and Apache SystemML to automate this critical image classification task.
What are the essential components of a data platform? John Akred and Stephen O'Sullivan explain how the various parts of the Hadoop, Spark, and big data ecosystems fit together in production to create a data platform supporting batch, interactive, and real-time analytical workloads.
James Malone explores using managed Spark and Hadoop solutions in public clouds alongside cloud products for storage, analysis, and message queues to meet enterprise requirements via the Spark and Hadoop ecosystem.
Leading companies are integrating operations and analytics to make real-time adjustments to improve revenues, reduce costs, and mitigate risks. There are many aspects to digital transformation, but the timely delivery of actionable data is both a key enabler and an obstacle. Jack Norris explores how companies from TransUnion to Uber use event-driven processing to transform their businesses.
Data science is a rewarding career. It's also really hardnot just the technical work itself but also "how to do the work well" in an organization. Yael Garten explores what data scientists do, how they fit into the broader company organization, and how they can excel at their trade and shares the hard and soft skills required, tips and tricks for success, and challenges to watch out for.
Vishal Bamba and Rocky Tiwari offer an overview of Transamerica's Customer 360 platform and the work done afterward to utilize this technology, including graph databases and machine learning to help create targeted segments for products and campaigns.
Bitvore Corps Bitvore for Munis personalized news surveillance system is rapidly becoming a must-have for all major fixed-income securities analysts, investors, and brokers working in the three-trillion-dollar municipal bond market in the USA. Alan Chaney explains how Bitvore delivers the few important and relevant articles out of thousands each day, saving users many hours daily.
What if companies could predict what products people will buy, how much they will buy, and when? It would be a game changerand its already possible with the power of predictive intelligence. Amanda Kahlow explores how BlueJeans Network was able to leverage predictive analytics to uncover buyers earlier, convert them at a 20x higher rate, and build a $33M pipeline.
With more than 91M customers, Verizon produces oceans of data. The challenge this onslaught presents isnt one of storageits one of speed. The solution? Harnessing the power of GPUs to access insights in less than a millisecond. Todd Mostak and Abdul Subhan explain how Verizon solved its data challenge by implementing GPU-tuned analytics and visualization.
Many hospitals combine early warning systems with rapid response teams (RRT) to detect patient decline and respond with elevated care.Predictive models can minimize RRT events by identifying at-risk patients, but modeling is difficult because events are rare and features are varied.Emily Spahn explores the creation of one such patient-risk model and shares lessons learned along the way.
Sikorsky collects data onboard thousands of helicopters deployed worldwide that is used for fleet management services, engineering analyses, and business intelligence. Mike Koelemay offers an overview of the data platform that Sikorsky has built to manage the ingestion, processing, and serving of this data so that it can be used to rapidly generate information to drive decision making.
Over the course of just six years, Pinterest has helped over 100 million pinners discover and collect over 75+ billion ideas to plan their everyday lives. Romit Jadhwani walks you through the different phases of this hypergrowth journey and explores the focuses, thought processes, and decisions of Pinterests data team as they scaled and enabled this growth.
Mahesh Goud shares success stories using Ticketmaster's large-scale contextual bandit platform for SEM, which determines the optimal keyword bids under evolving keyword contexts to meet different business requirements, and explores Ticketmaster's streaming pipeline, consisting of Storm, Kafka, HBase, the ELK Stack, and Spring Boot.
Chandan Joarder shares a guide to building real-time dashboards in-house using tools such as Kafka, web frameworks, and an in-memory database, utilizing JavaScript and Scala. Along the way, Chandan also discusses the architectural principles used in these dashboards to provide up-to-the-hour business performance metrics and alerts.
In collaboration with the Gray Area Foundation for the Arts and Metis Data Science, Rumman Chowdhury created an interactive data art installation with the purpose of educating San Franciscans about their own city. Rumman discusses the challenges of using historical, predigital-era data with D3 and R to craft a compelling and educational story residing at the intersection of art and technology.
Joe Hellerstein, Giorgio Caviglia, and Alon Bartur share their design philosophy for users and their experience designing UIs, illustrating their design principles with core elements from Trifacta, including the founding technology of predictive interaction, recent innovations like transform builder, and other developments in their core transformation experience.
In a panel discussion, top-tier VCs look over the horizon and consider the big trends in big data, explaining what they think the field will look like a few years (or more) down the road.
Sean Kandel and Wei Zheng offer an overview of an entirely new approach to visualizing metadata and data lineage, demonstrating automated methods for detecting, visualizing, and interacting with potential anomalies in reporting pipelines. Join in to learn whats required to efficiently apply these techniques to large-scale data.
With the exploding growth of video and audio content online, there's an increasing need for indexable and searchable audio. Matar Haller demonstrates how to automatically identify who is speaking when in a recorded conversation using machine learning applied to a corpus of audio recordings. Matar shares how she approached the problem, the algorithms used, and steps taken to validate the results.
From personalized newsfeeds to curated playlists, users want tailored experiences when they interact with their devices. Ricky Hennessy and Charlie Burgoyne explain how frogs interdisciplinary teams of designers, technologists, and data scientists create data-driven, personalized, and adaptive user experiences.
Mark Grover and Jonathan Seidman, the authors of Hadoop Application Architectures, share considerations and recommendations for the architecture and design of applications using Hadoop. Come with questions about your use case and its big data architecture or just listen in on the conversation.
Join Confluent system architect Gwen Shapira to discuss Apache Kafka and its use cases, data streaming platforms, and microservices.
John Akred, Julie Steele, Stephen O'Sullivan, and Scott Kurth field a wide range of detailed questions about developing a modern data strategy, architecting a data platform, and best practices for CDO and its evolving role. Even if you dont have a specific question, join in to hear what others are asking.
The real power and value proposition of Apache Spark is in building a unified use case that combines ETL, batch analytics, real-time stream analysis, machine learning, graph processing, and visualizations. Brian Clapper employs hands-on exercises using various Wikipedia datasets to illustrate the variety of ideal programming paradigms Spark makes possible.
Satya Ramaswamy and Sunil Karkeraof offer an overview of the recent technical advances that have made the current AI revolution possible, convincingly answering the "why now?" question.
This Executive Briefing is a part of the Strata Business Summit. Details to come
Jerry Overton provides an executive's guide to understanding advanced analytics in the cloudoffering a comprehensive survey of cloud technologies, patterns of cloud-based architectures, and patterns of enterprise cloud adoption, describing paths to achieving a cognitive enterprise, and outlining the realistic next steps for executives.
Big data promises enormous benefits for companies, and new innovations in this space only mean more data collection is required. Having a solid understanding of legal obligations will help you avoid the legal snafus that can come with collecting big data. Alysa Hutnik and Crystal Skelton outline legal best practices and practical tips to avoid becoming a big data dont.
The IoT is driven by outcomes delivered by applications, but to gain operational efficiency, many organizations are looking toward a horizontal platform for delivering and supporting a number of applications. Teresa Tung explores how to choose and implement a platformand deal with the fact that the platform is horizontal and application outcomes are vertical.
This Executive Briefing is a part of the Strata Business Summit. Details to come.
Vartika Singh, Jayant Shekhar, and Jeffrey Shmain walk you through various approaches available via the machine-learning algorithms available in Spark Framework (and more) to understand and decipher meaningful patterns in real-world data in order to derive value.
Big data and data science have great potential for accelerating business, but how do you reconcile the business opportunity with the sea of possible technologies? Data should serve the strategic imperatives of a businessthose aspirations that will define an organizations future vision. Scott Kurth and Edd Wilder-James explain how to create a modern data strategy that powers data-driven business.
Mark Burnette outlines five keys to success with data lakes and explores several real-world data lake implementations that are changing the world.
Companies store tons of data in Hadoop in hopes of turning the data into actionable insights, but maximizing the value of this resource with artificial intelligence and machine learning eludes most organizations. Greg Michaelson defines analytic trends around Hadoop, separates fact from hype, and sets out a roadmap for fully optimizing the value of the data stored in Hadoop.
Peak Games, a leading online and mobile company, unites 30 million monthly unique players with free, culturally relevant, community-driven games. Serdar Sahin shares the company's journey evaluating MPP columnar databases against Hadoop to find the right data infrastructure to enable the company to handle the unpredictable popularity of newly launched games.
Siva Raghupathy explores the concepts behind and benefits of serverless architectures for big data, looking at design patterns to ingest, store, process, and visualize your data. Along the way, Siva explains when and how you can use serverless technologies to streamline data processing and shares a reference architecture using a combination of cloud and open source technologies.
Wee Hyong Tok and Danielle Dean explain how the global, trusted, and hybrid Microsoft platform can enable you to do intelligence at scale, describing real-life applications where big data, the cloud, and AI are making a difference and how this is accelerating the digital transformation for these organizations at a lighting pace.
Machine-learning algorithms can improve predictions and optimize business operations across industry verticals, but building and scoring models still presents a significant computational challenge requiring massive training data and complex pipelines. Crystal Valentine outlines the benefits of implementing a microservices-based architecture to support a machine-learning model-scoring workflow.
Vahid Fereydouny and Anjaneya Chagam share the results of running Hadoop workloads on a standard all-flash vSAN cluster, unleashing the simplicity and power of big data in a hyperconverged environment.
A recent study suggests that 44 % of businesses are unsure what to do about big data. Erin Banks explains how big data analytics can help transform your business and ensure your data provides the greatest value to you, covering best business practices to help you achieve insights from your analytics, extract value from your data, and drive business change.
Join Googles product lead for Cloud Intelligence in a discussion of machine learning and predictive analytics. Rob will discuss how you can leverage the power of ML whether you have a machine learning team of your own or if you just want to use ML as a service.
Continuous queries on streaming data play a vital role in fast data applications, providing always up-to-date results based on the most recent data. Ethan Zhang offers an overview of VoltDB, a NewSQL distributed database that supports continuous queries three orders of magnitude faster with materialized views, highlighting a transparent, automatic, and incremental-view maintenance approach.
Beachbody, the maker of P90X and CIZE, needed to enable its central analytics team with Agile access to its corporate data lake. Eric Anderson explains how, in less than six months, the IT team successfully ingested all data into Amazon S3 using Talend Data Integration with self-service access via a variety of analysis tools, helping the company make business decisions faster.
Large-scale machine learning is a big challenge in industry due to the huge computing resources required and the difficulty of parameter tuning. Xiatian Zhang offers an overview of Fregata, TalkingData's open source machine-learning library based on Spark, which provides a lightweight, fast, memory-efficient, and parameter-free solution for large-scale machine learning.
Teradata joined the Presto community in 2015 and is now a leading contributor to this open source SQL engine, originally created by Facebook. Join Kamil Bajda-Pawlikowski to learn about Presto, Teradata's recent enhancements in query performance, security integrations, and ANSI SQL coverage, and its roadmap for 2017 and beyond.
With Huawei's big data cloud ecosystem, you can define and setup your data pipelines quickly and easily, whether youre looking for batch processing or stream analytics. Luhui Hu shares best practices for designing a big data pipeline in the cloud and explains how to implement serverless big data solutions and intelligent data clouds.
Jagane Sundar shares a strongly consistent replication service for replicating between cloud object stores, HDFS, NFS, and other S3- and Hadoop-compatible filesystems.
Thousands of companies have made their initial investments into next-generation data lake architecture, and they are on the verge of generating quality business returns. Chandhu Yalla and Neshad Bardoliwalla explain how enterprises have unlocked tangible value from their data lakes with adaptive information management and how their organizations are providing self-service to business units.
Sasi Kuppannagari explores the innovative sports analytics solutions Intel is creating, such as using computer vision and big data analytics for athlete performance optimization.
Hadoop and Spark provide scale and flexibility at a low cost compared to data warehouses, but the messy and diverse nature of big data results in undesirable complexities and inefficiencies. We will delve into standardization, automation, and deep integration aspects of the technologies that will allow users to focus on application logic and insights rather than infrastructure and integration.
Michael Abbott shares trends Kleiner Perkins Caufield & Byers is seeing in the area of transportation and logistics from an investments perspective and offers direct insights from companies in the sector, looking at how these firms deal with unique data processing challenges.
Rodrigo Fontecilla explains how many of the largest airlines use different classes of machine-learning algorithms to create robust and reusable predictive models to provide a holistic view of operations and provide business value.
Ryan Baumann explains how Mapbox Cities helps transform transportation and safety using open data, spatial analysis, and Mapbox tools.
The simultaneous localization and mapping (SLAM) problem is the cutting edge of robotics for autonomous vehicles and a key challenge in both industry and research. Jay White Bear shares a new integrated framework that demonstrates a constrained SLAM using online algorithms to navigate and map in real time using the Turtlebot II.
Andre Luckow shares best practices for developing and deploying deep learning solutions in the automotive industry and explores different deep learning frameworks, including TensorFlow, Caffe, and Torch, and deep neural network architectures, evaluating their trade-offs in terms of classification performance, training, and inference time.
Connected vehicle applications require massive amounts of data to be transported and analyzedoften in real timein order to support high-frequency decision-making requirements. Crystal Valentine outlines the five foundational characteristics of a next-generation big data platform that can support innovative connected vehicle and mobility services applications.
James Burkhart explains how Uber supports millions of analytical queries daily across real-time data with Apollo.
Stuck with manual, siloed, inflexible, laborious practices for big data projects? Successful teams use machine-learning-based approaches to power self-service preparation, enterprise-wide data catalogs, and real-time stream processing with role-specific tools. Murthy Mathiprakasam explains how using Informatica atop Hadoop, Spark, and Spark Streaming maximizes teamwork, trust, and timeliness.
Justin Murray outlines the benefits of virtualizing Hadoop and Spark, covering the main architectural approaches at a technical level and demonstrating how the core Hadoop architecture maps into virtual machines and how those relate to physical servers. You'll gain a set of design approaches and best practices to make your application infrastructure fit well with the virtualization layer.
Evangelos Simoudis explores how data generated in and around increasingly autonomous vehicles and by on-demand mobility services will enable the development of new transportation experiences and solutions for a diverse set of industries and government types.
Transport for London (TfL) and its partners have been working together on broader integration projects focused on getting the most efficient use out of road networks and public transport. Roland Major explains how TfL brings together a wide range of data from multiple disconnected systems for operational purposes while also making more of them open and available, all in real time.
Data is transforming global trade. We look at three frictions in logistics and container shipping: price opacity, inefficient markets, and unstructured data. Using examples from historical trade and our work at Haven, we identify the important ways in which data will change how we price and exchange goods worldwide.
Lloyd Palum explores the importance of identifying the target business value in an IIoT applicationa prerequisite to justifying a return on technology investmentand explains how to deliver that value using the concept of a digital twin.
The aviation industry is facing a huge pressure in costs as well as a profound disruption in marketing and service. With ticket revenues dropping, increasing customer loyalty is key. Andreas Ribbrock explains how Lufthansa German Airlines uses data science and data-driven decision making to create the next level of digital customer experience along the full customer journey.
Darren Chinen demonstrates how to use a Lambda architecture to provide real-time views into big data by combining batch and stream processing, leveraging BMCs Control-M as a critical component of both batch processing and ecosystem management.
The massive shift of data to the cloud is exacerbating data preparation and transport complexities that slow data analytics to a crawl. Bill Dentinger explains how the deployment of FPGA/x86-based heterogeneous compute architectures by cloud vendors is giving all organizations the opportunity to speed their data analytics to unprecedented levels.
Andrei Savu, Vinithra Varadharajan, Matthew Jacobs, and Jennifer Wu explore best practices for Hadoop deployments in the public cloud and provide detailed guidance for deploying, configuring, and managing Hive, Spark, and Impala in the public cloud.
Michael Yoder, Ben Spivey, Mark Donsky, and Mubashir Kazia walk you through securing a Hadoop cluster. Youll start with a cluster with no security and then add security features related to authentication, authorization, encryption of data at rest, encryption of data in transit, and complete data governance.
Apache Spark has become the go-to system for servicing ad hoc queries, but the Catalyst optimizer still lacks many of the pushdown optimizations necessary to take advantage of native database features. Jason Slepicka explains how DataScience replaced Catalyst with Apache Calcite to achieve performance improvements of two orders of magnitude when querying SQL and NoSQL databases with Spark.
Ben Sharma shares real-world lessons and best practices for building a scalable data architecture.
Ken Tsai and Michael Eacrett explore critical components of enterprise production environments that support day-to-day business processes while ensuring security, governance, and operational administration and share best practices to ensure business value.
Big data is moving from science projects to mainstream, mission-critical deployments. Drawing on his interactions and conversations with business and IT leaders across the world, Scott Gnau outlines adoption trends and popular use cases.
Reflecting that old horror movie gimmick "the call that comes from inside the house," an increasing number of data breaches are carried out by insiders. Charlotte Crain shares a unique, hybrid approach to insider threat deterrence that combines traditional detection methods and investigative methodologies with behavioral analysis to enable complete, continuous monitoring of activity.
An experiment at Pinterest revealed somewhat shocking results. When nine data scientists and ML engineers were asked the same constrained question, they gave nine spectacularly different answers. The implications for business are astronomical. June Andrews and Frances Haugen explore the aspects of analysis that cause differences in conclusions and offer some solutions.
Modern data applications combine functions from many libraries and frameworks and cannot achieve peak hardware performance due to data movement across functions. Shoumik Palkar offers an overview of Weld, an optimizing runtime that enables optimizations across disjoint libraries, and explains how to integrate it into frameworks such as Spark SQL for performance gains with no changes to user code.
Apps have so many moving parts that a simple change to one element can cause havoc somewhere else. The resulting issues annoy users and cause revenue leaks. Ira Cohen outlines ways to use anomaly detection to monitor all areas of an app, from the code to the user behavior to partner integrations and more, to fully optimize your mobile app.
Data analysts, data scientists, and data engineers are already working on teams delivering insight and analysis, but how do you get the team to support experimentation and insight delivery without ending up in an IT versus data engineer versus data scientist war? Christopher Bergh and Gil Benghiat present the seven shocking steps to get these groups of people working together.
This one-day hands-on class introduces you to Apache Spark 2.0 core concepts with a focus on Spark's machine-learning library, using text mining on real-world data as the primary end-to-end use case.
Program chairs Roger Magoulas, Doug Cutting, and Alistair Croll welcome you to the first day of keynotes.
Program chairs Roger Magoulas, Doug Cutting, and Alistair Croll welcome you to the second day of keynotes.
Data is powering a machine learning renaissance.  Understanding our data helps us save lives, secure our personal and business information, and engage our customers with better relevance.  However, without big data and a platform to manage big data, machine learning and artificial intelligence just dont work.
The internet of things is turning the internet upside down, and the effects are causing all kinds of problems. We have to answer questions about how to have data where we want it and computation where we need itand we have to coordinate and control all of this while maintaining visibility and security. Ted Dunning shares solutions for this problem from across multiple industries and businesses.
Listen in on Phil Keslin, the founder and CTO of Niantic, Inc. (creator of Pokmon Go), in conversation with Salesforce's Beau Cronin.
Daphne Koller explains how Coursera is using large-scale data processing and machine learning in online education. Building on Coursera's wealth of online learning data, Daphne discusses the role of automation in scaling access to education that is personalized and efficient at connecting people with skills and knowledge throughout their lives.
Eric Frenkiel explains how to use real-time data as a vehicle for operationalizing machine-learning models by leveraging MemSQL, exploring advanced tools, including TensorFlow, Apache Spark, and Apache Kafka, and compelling use cases demonstrating the power of machine learning to effect positive change.
What happens when machines understand sports? As Rajiv Maheswaran demonstrates, everything changes, from how coaches coach and how players play to how storytellers tells stories and how fans experience the game.
Data helps us understand our market in new and novel ways.  In today's world, sifting through the noise in modern journalism means navigating enormous amounts of data, news, and tweets.  Thomson Reuters is leveraging big data and machine learning to chase down leads, verify sources, determine what's newsworthy.
Lets stop talking about bad robots and start talking about what makes a robot good. A good or ethical robot must be carefully designed. Andra Keay outlines five principles of good robot design and discusses the implications of implicit bias in our robots.
Keynote with Michael I. Jordan
It is no surprise that reducing operational IT expenditures and increasing software capabilities is a top priority for large enterprises. Given its advantages, open source software has proliferated across the globe. Ron Bodkin explains how Teradata drives open source adoption inside enterprises using open source data management and AI techniques leveraged across the analytical ecosystem.
Data to the rescue. Desi Matel-Anderson offers an immersive deep dive into the world of the Field Innovation Team, who routinely find themselves on the frontier of disasters working closely with data to save lives, at times while risking their own.
Vijay Narayanan takes you on an inspiring journey exploring how the cloud, data, and artificial intelligence are powering and accelerating the genomic revolutionsaving and changing lives in the process.
Keynote with Maya Shankar
Cathy O'Neil exposes the mathematical models that shape our future, both as individuals and as a society. These weapons of math destruction score teachers and students, sort rsums, grant (or deny) loans, evaluate workers, target voters, set parole, and monitor our health.
Artificial Intelligence (AI) will accelerate both cancer research and the development of autonomous vehicles. But the ultimate potential of AI is realized through societal benefits and positive impact on our world.  Collaboration between industry, government and academia are required to drive this societal innovation and deliver the scale and promise of AI to everyone.
Bruce Martin walks you through applying data science methods to real-world challenges in different industries, offering preparation for data scientist roles in the field. Join in to learn how Spark and Hadoop enable data scientists to help companies reduce costs, increase profits, improve products, retain customers, and identify new opportunities.
To handle real-time big data, you need to solve two difficult problems: how do you ingest that much data and how will you process that much data? Jesse Anderson explores the latest real-time frameworksboth open source and managed cloud servicesdiscusses the leading cloud providers, and explains how to choose the right one for your company.
Robert Schroll demonstrates TensorFlow's capabilities through its Python interface and explores TFLearn, a high-level deep learning library built on TensorFlow.  Join in to learn how to use TFLearn and TensorFlow to build machine-learning models on real-world data.
