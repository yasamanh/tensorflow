{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "matplotlib.rcParams['savefig.dpi'] = 144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import expectexception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = None\n",
    "\n",
    "def reset_tf():\n",
    "    global sess\n",
    "    if sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()\n",
    "    sess = tf.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization Schemes in TensorFlow\n",
    "\n",
    "Most machine learning algorithms are based on some optimization scheme.  There is a loss function to be minimized, and the parameters of the model are adjusted to decrease the loss.  Most optimization schemes operate iteratively.  They can't find the optimum value right away, but they can improve the target value step by step.\n",
    "\n",
    "In order to start the next step at the right place, state needs to be stored.  We have already seen two operations that create tensors: constants and placeholders.  Both can be used to enter arbitrary values into a computation through the feed dictionary, but neither offers the ability to hold state.\n",
    "\n",
    "Consider this (silly) way to calculate $2^{10}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_tf()\n",
    "\n",
    "n = tf.placeholder(np.int64)\n",
    "n_val = 1\n",
    "for i in xrange(10):\n",
    "    n_val = sess.run(2 * n, feed_dict={n: n_val})\n",
    "\n",
    "print n_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the placeholder doesn't store any state, we had to store the current value of *n* separately, being sure to feed it in correctly at each step.  This is extra work to do, and is obviously inviting errors.  There's a better way to do things.\n",
    "\n",
    "## Variables\n",
    "\n",
    "In addition to constants and placeholders, TensorFlow provides **variables**.  These tensors hold values much like others, but these values can be updated during a computation.  We will use variables again and again to hold parameters of models that need to be updated during training.\n",
    "\n",
    "Variables are created with an initial value.  Like other tensors, they can be given a name and a dtype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "var = tf.Variable(3, name=\"my_var\", dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, if you try to run a variable, you'll run into a problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%expect_exception tf.errors.FailedPreconditionError\n",
    "\n",
    "sess.run(var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables must be initialized before they are run.  The simplest way to do that is with `global_variables_initializer()`.  This TensorFlow operation will initialize all variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "sess.run(var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many other ways to [initialize variables](https://www.tensorflow.org/programmers_guide/variables#initialization).  Values can be stored on disk and restored later.  You can also initialize only the subset of variables that concern you.  We will use the global initializer so much that we'll make a simple function to run it in our global session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reset_vars():\n",
    "    sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables don't have to be initialized with a known value. In some cases, we'll want to initialize values randomly.  Here we create a random tensor of shape `[2, 2]` with values drawn from a normal distribution with $\\sigma = 1$ and $\\mu=2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rand_var = tf.Variable(tf.random_normal([2, 2], stddev=1, mean=2))\n",
    "reset_vars()\n",
    "print sess.run(rand_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables can be updated with the *assign* operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "change_var = var.assign(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this is an operation.  Nothing changes until the assignment is actually run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print sess.run(var)\n",
    "print sess.run(change_var)\n",
    "print sess.run(var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More typically, the value of a variable will be updated based on current values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "double_var = var.assign(var * 2)\n",
    "print sess.run(double_var)\n",
    "print sess.run(double_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of each variable exists only within a given session.  If you start with another session, variables will have a separate value in that one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess2 = tf.Session()\n",
    "sess2.run(tf.global_variables_initializer())\n",
    "print \"New session:\", sess2.run(var)\n",
    "print \"Old session:\", sess.run(var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise:\n",
    "\n",
    "Reimplement the (silly) algorithm from above to calculate $2^{10}$ using a variable instead of a placeholder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent\n",
    "\n",
    "One of the simplest optimization schemes is the **gradient descent** algorithm.  For illustration, we'll consider the case where we are optimizing a loss function of a single weight variable $L(W)$.  We want to find $W^*$ such that $L(W^*)$ is a minimum, or equivalently, so $dL(W^*)/dW = 0$.  We don't know the form of $L$, but we can calculate its value and its derivatives at some point $W$.  Thus, we can Taylor expand about $W$ to find\n",
    "\n",
    "$$ \\frac{dL(W^*)}{dW} = 0 = \\frac{dL(W)}{dW} + \\left(W^* - W\\right) \\frac{d^2 L(W)}{dW^2} + \\ldots $$\n",
    "\n",
    "Cutting this off after the second derivative, we can approximate the minimum as\n",
    "\n",
    "$$ W^* = W - \\left. \\frac{dL}{dW} \\middle/ \\frac{d^2 L}{dW^2} \\right. $$\n",
    "\n",
    "For gradient descent, we'll assume that we can calculate (or at least approximate) the first derivative, but not the second derivative.  (In the multi-dimensional case, the first derivative becomes the gradient vector and the second derivative becomes the **hessian** matrix, which requires a bit more work to compute.)  Instead, we make use only of the knowledge that near a minimum the second derivative will be positive.  Thus, in gradient descent, the parameters are updated according to\n",
    "\n",
    "$$ W^* = W - \\eta \\frac{dL}{dW} $$\n",
    "\n",
    "where $\\eta$ is a constant called the **learning rate**, a hyperparameter.\n",
    "\n",
    "Gradient descent is a rather straightforward algorithm, but we still probably don't want to implement it ourselves if a case with many parameters.  Luckily, we don't have to: TensorFlow has a number of optimizers built in.  We will demonstrate how to use gradient descent to optimize a linear model on the Boston housing price data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "with open(\"small_data/housing.pkl\", \"r\") as fin:\n",
    "    housing = dill.load(fin)\n",
    "\n",
    "for train, test in ShuffleSplit(1, 0.2, random_state=42).split(housing.data):\n",
    "    X_train = housing.data[train]\n",
    "    y_train = housing.target[train]\n",
    "    X_test = housing.data[test]\n",
    "    y_test = housing.target[test]\n",
    "\n",
    "print X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We're going to shift and scale the data, so that all columns are centered about zero and have a standard deviation of 1.  This will help a single learning rate be appropriate for all parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_mean = X_train.mean(axis=0)\n",
    "X_std = X_train.std(axis=0)\n",
    "\n",
    "Xs_train = (X_train - X_mean) / X_std\n",
    "Xs_test = (X_test - X_mean) / X_std\n",
    "\n",
    "print \"Mean:\", Xs_test.mean()\n",
    "print \"Std: \", Xs_test.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will make placholders for the feature matrix and label vector.  The first dimension will have size `None`, so that we can feed in an arbitrary number of observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_tf()\n",
    "\n",
    "x = tf.placeholder(shape=[None, 13], dtype=tf.float32, name='x')\n",
    "y_label = tf.placeholder(shape=[None, 1], dtype=tf.float32, name='y_label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our linear model is $y = W\\cdot x + b$.  Both the weight vector and bias term will be updated by each step of the gradient descent optimizer, so they need to be variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.zeros((13, 1)), name=\"weight\")\n",
    "b = tf.Variable(tf.zeros(1), name=\"bias\")\n",
    "\n",
    "y = tf.matmul(x, W) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll choose to minimize the mean squared error as our loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.square(y - y_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we create an optimizer, we need to specify a learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.1\n",
    "optimizer = tf.train.GradientDescentOptimizer(eta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimizer has a `minimize` method.  This method takes a tensor as an argument and returns an operation that will update all the parameters that contribute to this tensor's value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is possible through two pieces of cleverness.\n",
    "\n",
    "First, by examining the computation graph, the optimizer can find all variables involved in the calculation.  All variables created with the keyword option `trainable=True`, the default, are considered to be parameters that need to be optimized.\n",
    "\n",
    "Second, through a technique known as [automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation), TensorFlow can exactly calculate numeric values for all derivatives.  This lets the optimizer calculate the gradient of the loss with respect to those parameters and update their values appropriately.\n",
    "\n",
    "All that's left to do is run the `train` operation several times.  We will feed in the full training set each time.  For evaluation purposes, we'll keep track of both the in- and out-of-sample error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_sample_error = []\n",
    "out_of_sample_error = []\n",
    "\n",
    "reset_vars()\n",
    "for _ in xrange(20):\n",
    "    sess.run(train, feed_dict={x: Xs_train, y_label: y_train.reshape(-1, 1)})\n",
    "    in_sample_error.append(sess.run(loss, feed_dict={x: Xs_train, y_label: y_train.reshape(-1, 1)}))\n",
    "    out_of_sample_error.append(sess.run(loss, feed_dict={x: Xs_test, y_label: y_test.reshape(-1, 1)}))\n",
    "\n",
    "plt.plot(np.array([in_sample_error, out_of_sample_error]).T, '.-')\n",
    "plt.legend(['In-sample error', 'Out-of-sample error'])\n",
    "plt.xlabel('step')\n",
    "plt.ylabel('loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We picked the learning rate more or less at random.  Let's see what happens with different learning rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "etas = [0.05, 0.1, 0.15, 0.162, 0.171]\n",
    "errors = []\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "for e in etas:\n",
    "    reset_vars()\n",
    "    optimizer = tf.train.GradientDescentOptimizer(e)  # learning rate\n",
    "    train = optimizer.minimize(loss)\n",
    "    errors.append([sess.run(loss, feed_dict={x: Xs_train,\n",
    "                                             y_label: y_train.reshape(-1, 1)})])\n",
    "    for i in xrange(20):\n",
    "        sess.run(train, feed_dict={x: Xs_train,\n",
    "                                   y_label: y_train.reshape(-1,1)})\n",
    "        errors[-1].append(sess.run(loss, feed_dict={x: Xs_train,\n",
    "                                                    y_label: y_train.reshape(-1, 1)}))\n",
    "\n",
    "plt.plot(np.array(errors).T, '.-')\n",
    "plt.legend(etas, title=r'$\\eta$')\n",
    "plt.xlabel('step')\n",
    "plt.ylabel('loss');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the optimal learning rate is around 0.15.  Lower learning rates reach the same solution, but they take longer to do so.  The gradient descent algorithm is taking steps that are too small, causing it to take more steps than strictly necessary.\n",
    "\n",
    "For larger learning rates, the convergence also slows down.  In this case, the steps are too large.  As each step takes it too far in a direction, it has to waste some of the next step recovering.  As the learning rate goes up, these oversteps become larger and larger, until the algorithm finds itself ending up further away from the optimum each time than it started.  The process no longer converges.\n",
    "\n",
    "> The learning rate may be chosen by trial and error, but it is usually best to choose it by monitoring learning curves that plot the objective function as a function of time.  This is more of an art than a science, and most guidance on this subject should be regarded with some skepticism.\n",
    "\n",
    "> &mdash;Ian Goodfellow, Yoshua Bengio, and Aaron Courville, [Deep Learning](http://www.deeplearningbook.org/)\n",
    "\n",
    "If you get confused by the behavior of your learning curves, don't despair.  You are in [good company](http://lossfunctions.tumblr.com/).\n",
    "\n",
    "## Stochastic gradient descent\n",
    "\n",
    "We used the full training set in calculating the loss function above.  With only 404 observations, this is easy enough.  But when dealing with millions of observations (or more), this will become infeasible. It's also unnecessary.  Most observations in a large data set will have many neighbors that impart almost the same information.  We don't need all of them in our learning process.\n",
    "\n",
    "If we take this reasoning to the extreme, we can train based on the error of a single observation at a time.  We want to train with the \"most unexpected\" datum each time.  In lieu of trying to figure this out, we'll feed the data randomly, so this process is known as **stochastic gradient descent**.  Each individual step will be noisy, since it contains only a small amount of information.  This means that you will need more steps and a smaller learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "etas = [0.001, 0.005, 0.02]\n",
    "errors = []\n",
    "\n",
    "for e in etas:\n",
    "    reset_vars()\n",
    "    optimizer = tf.train.GradientDescentOptimizer(e)\n",
    "    train = optimizer.minimize(loss)\n",
    "    errors.append([sess.run(loss, feed_dict={x: Xs_train,\n",
    "                                                y_label: y_train.reshape(-1, 1)})])\n",
    "    for i in xrange(1000):\n",
    "        j = np.random.randint(len(y_train))\n",
    "        sess.run(train, feed_dict={x: Xs_train[j:j+1,:],\n",
    "                                   y_label: [[y_train[j]]]})\n",
    "        errors[-1].append(sess.run(loss, feed_dict={x: Xs_train,\n",
    "                                                    y_label: y_train.reshape(-1, 1)}))\n",
    "\n",
    "plt.plot(np.array(errors).T, '-')\n",
    "plt.legend(etas, title=r'$\\eta$')\n",
    "plt.xlabel('step')\n",
    "plt.ylabel('loss');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In addition to generally being faster than batch processing, stochastic gradient descent is often more robust.  The noise inherent in this process is often enough to push us out of local minima of the loss function.  (A downside of this noise it that it makes it harder for the model to finally converge at the end.)  Additionally, stochastic gradient descent supports **online learning**.  We can update an existing model with new data just by running some descent steps on it.\n",
    "\n",
    "## Minibatch processing\n",
    "\n",
    "**Minibatch processing** attempts to combine the benefits of batch and stochastic gradient descent by training on a \"minibatch\", a subset of the whole data, at a time.  This increases the accuracy of the gradient calculation, reducing the noise in each step, while still avoiding much of the duplication in the whole set.  Minibatches can often be about as fast as individual steps&mdash;the parallelism available in modern CPUs and especially GPUs is wasted if we are calculating a single row at a time.\n",
    "\n",
    "A batch size of 100 is usually enough to smooth out the noise.  In our case, the full data is not so much larger than the batch size, so there is little difference in performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "etas = [0.05, 0.1, 0.15]\n",
    "errors = []\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "for e in etas:\n",
    "    reset_vars()\n",
    "    optimizer = tf.train.GradientDescentOptimizer(e)\n",
    "    train = optimizer.minimize(loss)\n",
    "    errors.append([sess.run(loss, feed_dict={x: Xs_train,\n",
    "                                                y_label: y_train.reshape(-1, 1)})])\n",
    "    for i in xrange(20):\n",
    "        j = np.random.choice(len(y_train), BATCH_SIZE, replace=False)\n",
    "        sess.run(train, feed_dict={x: Xs_train[j, :],\n",
    "                                   y_label: y_train[j].reshape(-1,1)})\n",
    "        errors[-1].append(sess.run(loss, feed_dict={x: Xs_train,\n",
    "                                                    y_label: y_train.reshape(-1, 1)}))\n",
    "\n",
    "plt.plot(np.array(errors).T, '.-')\n",
    "plt.legend(etas, title=r'$\\eta$')\n",
    "plt.xlabel('step')\n",
    "plt.ylabel('loss');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Recall that logistic regression is a linear model used for two-class classification.  Our classes will be labelled 0 and 1, and our model predicts the probability of an observation belonging to class 1 as\n",
    "\n",
    "$$ p = f\\left( X \\cdot W + b \\right) $$\n",
    "\n",
    "where $f$ represents the **logistic** (or **sigmoid**) function\n",
    "\n",
    "$$ f(x) = \\frac{1}{1 + e^{-x}} $$\n",
    "\n",
    "which as a range of $[0, 1]$.\n",
    "\n",
    "Logistic regression is designed to minimize the cross-entropy:\n",
    "\n",
    "$$ L = -\\frac{1}{n}\\sum_{j=1}^{n} y_j \\log p_j + (1 - y_j) \\log (1-p_j) $$\n",
    "\n",
    "This expression is more complicated than that of the mean squared error for linear regression, but the same techniques are still available.  We can still take the derivative of this loss function with respect to $W$ and $b$ to work out the update rules in a gradient descent algorithm.  It's even easier than you might think, because TensorFlow provides both the logistic function (as `tf.nn.sigmoid()`) and a entropy calculated from logistic values (as `tf.nn.sigmoid_cross_entropy_with_logits()`).\n",
    "\n",
    "For an example, we'll classify census tracts on whether their average house price is above 20, in whatever units we happen to be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "yc_train = np.float32(y_train > 20)\n",
    "yc_test = np.float32(y_test > 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The setup is nearly identical to the case of linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "reset_tf()\n",
    "\n",
    "W = tf.Variable(tf.zeros((13, 1)), name=\"weight\")\n",
    "b = tf.Variable(tf.zeros(1), name=\"bias\")\n",
    "\n",
    "x = tf.placeholder(shape=[None, 13], dtype=tf.float32, name='x')\n",
    "y_label = tf.placeholder(shape=[None, 1], dtype=tf.float32, name='y_label')\n",
    "\n",
    "y = tf.matmul(x, W) + b\n",
    "loss = tf.reduce_mean(\n",
    "    tf.nn.sigmoid_cross_entropy_with_logits(logits=y, labels=y_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Here, we make a prediction by calling the logistic function on our linear model, and then comparing that to a threshold of 0.5.  The accuracy is measured by figuring if the predictions are equal to the ground truth labels, casting the result to a float, and taking the average.  Most of us find the accuracy easier to understand than the entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "predicted = tf.cast(tf.nn.sigmoid(y) > 0.5, np.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, y_label), np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "reset_vars()\n",
    "\n",
    "for i in xrange(50):\n",
    "    j = np.random.choice(len(y_train), BATCH_SIZE, replace=False)\n",
    "    sess.run(train, feed_dict={x: Xs_train[j,:], \n",
    "                               y_label: yc_train[j].reshape(-1,1)})\n",
    "    if i % 5 == 0:\n",
    "        print sess.run([loss, accuracy], feed_dict={x: Xs_test, \n",
    "                                                    y_label: yc_test.reshape(-1, 1)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Our model is able to predict 88% of the test data correctly.  Note that in some steps, the entropy improves while the accuracy remains constant.  By measuring our confidence in an answer, the entropy allows improvements even when they don't change any prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Softmax classification\n",
    "\n",
    "Recall that any binary classifier can be bootstrapped to multi-class classification with the one-versus-all approach.  If there are $C$ classes, then $C$ different binary classifiers will be built, each classifier $k$ telling class $k$ from not-$k$.\n",
    "\n",
    "When these scheme is applied to logistic regression, the softmax function is the result.  Let $\\pi_k$ be the logit of the probability from the $k^\\mathrm{th}$ classifier.  We can represent this in matrix form as\n",
    "\n",
    "$$ \\pi_k^{(j)} = \\sum_i X_{ji} W_{ik} + b_k $$\n",
    "\n",
    "Note that there are no $C$ times as many weights and bias terms.  From these, we can calculate a set of $C$ probabilities\n",
    "\n",
    "$$ p_k = \\frac{e^{-\\pi_k}}{\\sum_\\ell e^{-\\pi_\\ell}} $$\n",
    "\n",
    "Because of the chosen normalization, we know that $\\sum_k p_k = 1$.  Thus, we can associate $p_k$ with the probability of an observation being in class $k$.\n",
    "\n",
    "The cross-entropy needs to be expanded to contain a term for each class:\n",
    "\n",
    "$$ L = -\\frac{1}{n}\\sum_{j=1}^{n} \\sum_{k=1}^C y_k^{(j)} \\log p_k^{(j)} $$\n",
    "\n",
    "Here $y_k^{(j)}$ is an indicator variable which is one if the $j^{th}$ observation is in class $k$, and zero otherwise.  In other words, we are one-hot encoding the labels.\n",
    "\n",
    "We're going to switch datasets and use the famous [MNIST database](https://en.wikipedia.org/wiki/MNIST_database) of handwritten digits.  Each example is a $28\\times28$ pixel grayscale image.  We'll read them in along with one-hot encoded labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets('/tmp/data', one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The images come in as 1-D vectors.  We need to reshape them to view the images.  To get the label, we just look for the index of maximum value of the label vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with sns.axes_style(\"ticks\"):\n",
    "    plt.imshow(mnist.train.images[0].reshape((28, 28)),\n",
    "               cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "print \"Label:\", mnist.train.labels[0].argmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The linear model is very similar to before, but it now has an output of dimension 10, instead of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "reset_tf()\n",
    "\n",
    "N_PIXELS= 28 * 28\n",
    "BATCH_SIZE = 100\n",
    "LEARNING_RATE = 0.5\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, N_PIXELS], name=\"pixels\")\n",
    "y_labels = tf.placeholder(tf.float32, [None, 10], name=\"labels\")\n",
    "\n",
    "W = tf.Variable(tf.zeros([N_PIXELS, 10]), name=\"weights\")\n",
    "b = tf.Variable(tf.zeros([10]), name=\"biases\")\n",
    "y = tf.matmul(x, W) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "TensorFlow has a built-in softmax entropy function for us to use.  Note that the softmax is monotonic, so we don't need to calculate the probabilities to tell which class we're predicting.  We can just find the largest component of the result of our linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y, \n",
    "                                                              labels=y_labels))\n",
    "train = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(y, 1),\n",
    "                                           tf.argmax(y_labels, 1)),\n",
    "                                  tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "If we run the optimizer, we can pretty easily manage about 92% accuracy.  This is not bad, given the simplicity of the model.  Random guessing would have only given us 10% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "reset_vars()\n",
    "\n",
    "for i in range(1000):\n",
    "    batch_x, batch_y = mnist.train.next_batch(BATCH_SIZE)\n",
    "    sess.run(train,\n",
    "             feed_dict={x: batch_x, y_labels: batch_y})\n",
    "    if i % 100 == 0:\n",
    "        print sess.run([loss, accuracy],\n",
    "                       feed_dict={x: mnist.test.images, y_labels: mnist.test.labels})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Exercises\n",
    "\n",
    "1. Adjust the learning rate and the batch size.  Can you get better performance?\n",
    "\n",
    "2. The weight matrix $W$ contains information about how much each pixel contributes to each class.  Plot these contributions for several of the numbers.  Can you understand what the model was looking for in each case?\n",
    "\n",
    "3. The [Iris data set](https://en.wikipedia.org/wiki/Iris_flower_data_set) is famous set of measurements of four features from three different species of *Iris*.  Build a classifer to descriminate between the three species based on these four features.  The data are provided by Scikit-Learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "print \"Features:\", iris.data.shape\n",
    "print \"Labels:\", iris.target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "*Hint:* The labels are just integers specifying the class.  You will probably need to convert them to a one-hot encoded value; [`tf.one_hot()`](https://www.tensorflow.org/versions/master/api_docs/python/tf/one_hot) might be useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright &copy; 2017 The Data Incubator.  All rights reserved.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
